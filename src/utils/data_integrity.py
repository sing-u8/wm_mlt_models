"""
ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ ëª¨ë“ˆ

íŒŒì´í”„ë¼ì¸ì˜ ê° ë‹¨ê³„ì—ì„œ ë°ì´í„° í’ˆì§ˆê³¼ ë¬´ê²°ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.
"""

import numpy as np
import pandas as pd
from typing import Dict, Any, List, Tuple, Optional
from dataclasses import dataclass
from ..utils.logger import LoggerMixin


@dataclass
class DataIntegrityReport:
    """ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ ë³´ê³ ì„œ"""
    step_name: str
    passed: bool
    total_checks: int
    passed_checks: int
    failed_checks: int
    warnings: List[str]
    errors: List[str]
    details: Dict[str, Any]
    
    @property
    def success_rate(self) -> float:
        """ì„±ê³µë¥  ë°˜í™˜"""
        return self.passed_checks / self.total_checks if self.total_checks > 0 else 0.0


class DataIntegrityChecker(LoggerMixin):
    """
    ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ê¸°
    
    íŒŒì´í”„ë¼ì¸ì˜ ê° ë‹¨ê³„ì—ì„œ ë°ì´í„° í’ˆì§ˆì„ ê²€ì¦í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, config=None):
        """
        ë¬´ê²°ì„± ê²€ì‚¬ê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        Parameters:
        -----------
        config : Config, optional
            êµ¬ì„± ê°ì²´
        """
        self.config = config
        self.check_results = []
        
    def check_audio_features(self, X: np.ndarray, y: np.ndarray, 
                           step_name: str = "feature_extraction") -> DataIntegrityReport:
        """
        ì˜¤ë””ì˜¤ íŠ¹ì§• ë°ì´í„°ì˜ ë¬´ê²°ì„±ì„ ê²€ì‚¬í•©ë‹ˆë‹¤.
        
        Parameters:
        -----------
        X : np.ndarray
            íŠ¹ì§• ë°ì´í„°
        y : np.ndarray
            ë ˆì´ë¸” ë°ì´í„°
        step_name : str
            ê²€ì‚¬ ë‹¨ê³„ ì´ë¦„
            
        Returns:
        --------
        DataIntegrityReport
            ë¬´ê²°ì„± ê²€ì‚¬ ë³´ê³ ì„œ
        """
        self.logger.info(f"=== {step_name} ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ ì‹œì‘ ===")
        
        warnings = []
        errors = []
        checks_passed = 0
        total_checks = 0
        details = {}
        
        try:
            # 1. ê¸°ë³¸ í˜•íƒœ ê²€ì‚¬
            total_checks += 1
            if X.shape[0] == len(y):
                checks_passed += 1
                self.logger.info(f"âœ… íŠ¹ì§•-ë ˆì´ë¸” ê¸¸ì´ ì¼ì¹˜: {X.shape[0]} ìƒ˜í”Œ")
            else:
                errors.append(f"íŠ¹ì§•-ë ˆì´ë¸” ê¸¸ì´ ë¶ˆì¼ì¹˜: {X.shape[0]} vs {len(y)}")
                self.logger.error(f"âŒ íŠ¹ì§•-ë ˆì´ë¸” ê¸¸ì´ ë¶ˆì¼ì¹˜: {X.shape[0]} vs {len(y)}")
            
            # 2. íŠ¹ì§• ë²¡í„° ì°¨ì› ê²€ì‚¬
            total_checks += 1
            expected_features = 30  # design.md ëª…ì„¸
            if len(X.shape) == 2 and X.shape[1] == expected_features:
                checks_passed += 1
                self.logger.info(f"âœ… íŠ¹ì§• ë²¡í„° ì°¨ì› ì˜¬ë°”ë¦„: {X.shape[1]}")
            else:
                errors.append(f"íŠ¹ì§• ë²¡í„° ì°¨ì› ì˜¤ë¥˜: {X.shape[1] if len(X.shape) > 1 else 'N/A'} != {expected_features}")
                self.logger.error(f"âŒ íŠ¹ì§• ë²¡í„° ì°¨ì› ì˜¤ë¥˜: {X.shape}")
            
            # 3. ë°ì´í„° íƒ€ì… ê²€ì‚¬
            total_checks += 1
            if X.dtype in [np.float32, np.float64]:
                checks_passed += 1
                self.logger.info(f"âœ… íŠ¹ì§• ë°ì´í„° íƒ€ì… ì˜¬ë°”ë¦„: {X.dtype}")
            else:
                warnings.append(f"íŠ¹ì§• ë°ì´í„° íƒ€ì… ì£¼ì˜: {X.dtype} (float ê¶Œì¥)")
                self.logger.warning(f"âš ï¸ íŠ¹ì§• ë°ì´í„° íƒ€ì…: {X.dtype}")
            
            # 4. NaN/Infinity ê²€ì‚¬
            total_checks += 1
            nan_count = np.sum(np.isnan(X))
            inf_count = np.sum(np.isinf(X))
            
            if nan_count == 0 and inf_count == 0:
                checks_passed += 1
                self.logger.info("âœ… NaN/Infinity ì—†ìŒ")
            else:
                if nan_count > 0:
                    errors.append(f"NaN ê°’ ë°œê²¬: {nan_count}ê°œ")
                if inf_count > 0:
                    errors.append(f"Infinity ê°’ ë°œê²¬: {inf_count}ê°œ")
                self.logger.error(f"âŒ NaN: {nan_count}, Infinity: {inf_count}")
            
            # 5. íŠ¹ì§• ê°’ ë²”ìœ„ ê²€ì‚¬
            total_checks += 1
            feature_min = np.min(X)
            feature_max = np.max(X)
            feature_mean = np.mean(X)
            feature_std = np.std(X)
            
            # ì¼ë°˜ì ì¸ ì˜¤ë””ì˜¤ íŠ¹ì§• ë²”ìœ„ í™•ì¸ (ëŒ€ëµì )
            if -100 <= feature_min <= 100 and -100 <= feature_max <= 100:
                checks_passed += 1
                self.logger.info(f"âœ… íŠ¹ì§• ê°’ ë²”ìœ„ ì •ìƒ: [{feature_min:.3f}, {feature_max:.3f}]")
            else:
                warnings.append(f"íŠ¹ì§• ê°’ ë²”ìœ„ ì£¼ì˜: [{feature_min:.3f}, {feature_max:.3f}]")
                self.logger.warning(f"âš ï¸ íŠ¹ì§• ê°’ ë²”ìœ„: [{feature_min:.3f}, {feature_max:.3f}]")
            
            details['feature_statistics'] = {
                'min': float(feature_min),
                'max': float(feature_max),
                'mean': float(feature_mean),
                'std': float(feature_std),
                'shape': X.shape
            }
            
            # 6. ë ˆì´ë¸” ê²€ì‚¬
            total_checks += 1
            unique_labels = np.unique(y)
            expected_classes = len(self.config.class_names) if self.config else 3
            
            if (len(unique_labels) <= expected_classes and 
                np.all(unique_labels >= 0) and 
                np.all(unique_labels < expected_classes)):
                checks_passed += 1
                self.logger.info(f"âœ… ë ˆì´ë¸” ë²”ìœ„ ì •ìƒ: {unique_labels}")
            else:
                errors.append(f"ë ˆì´ë¸” ë²”ìœ„ ì˜¤ë¥˜: {unique_labels} (ê¸°ëŒ€: 0-{expected_classes-1})")
                self.logger.error(f"âŒ ë ˆì´ë¸” ë²”ìœ„ ì˜¤ë¥˜: {unique_labels}")
            
            # 7. í´ë˜ìŠ¤ ë¶„í¬ ê²€ì‚¬
            total_checks += 1
            unique, counts = np.unique(y, return_counts=True)
            min_samples = np.min(counts)
            max_samples = np.max(counts)
            imbalance_ratio = max_samples / min_samples if min_samples > 0 else float('inf')
            
            if imbalance_ratio <= 10:  # 10:1 ë¹„ìœ¨ ì´ë‚´
                checks_passed += 1
                self.logger.info(f"âœ… í´ë˜ìŠ¤ ë¶„í¬ ê· í˜•: ë¹„ìœ¨ {imbalance_ratio:.2f}")
            else:
                warnings.append(f"í´ë˜ìŠ¤ ë¶ˆê· í˜• ì£¼ì˜: ë¹„ìœ¨ {imbalance_ratio:.2f}")
                self.logger.warning(f"âš ï¸ í´ë˜ìŠ¤ ë¶ˆê· í˜•: ë¹„ìœ¨ {imbalance_ratio:.2f}")
            
            details['label_distribution'] = {
                'unique_labels': unique_labels.tolist(),
                'class_counts': dict(zip(unique.tolist(), counts.tolist())),
                'imbalance_ratio': float(imbalance_ratio)
            }
            
            # 8. ìƒ˜í”Œ ìˆ˜ ê²€ì‚¬
            total_checks += 1
            n_samples = X.shape[0]
            min_samples_threshold = 10  # ìµœì†Œ ìƒ˜í”Œ ìˆ˜
            
            if n_samples >= min_samples_threshold:
                checks_passed += 1
                self.logger.info(f"âœ… ì¶©ë¶„í•œ ìƒ˜í”Œ ìˆ˜: {n_samples}")
            else:
                warnings.append(f"ìƒ˜í”Œ ìˆ˜ ë¶€ì¡±: {n_samples} < {min_samples_threshold}")
                self.logger.warning(f"âš ï¸ ìƒ˜í”Œ ìˆ˜ ë¶€ì¡±: {n_samples}")
            
            details['sample_info'] = {
                'total_samples': int(n_samples),
                'meets_minimum': n_samples >= min_samples_threshold
            }
            
        except Exception as e:
            errors.append(f"ë¬´ê²°ì„± ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            self.logger.error(f"âŒ ë¬´ê²°ì„± ê²€ì‚¬ ì˜¤ë¥˜: {e}")
        
        # ë³´ê³ ì„œ ìƒì„±
        passed = len(errors) == 0
        failed_checks = total_checks - checks_passed
        
        report = DataIntegrityReport(
            step_name=step_name,
            passed=passed,
            total_checks=total_checks,
            passed_checks=checks_passed,
            failed_checks=failed_checks,
            warnings=warnings,
            errors=errors,
            details=details
        )
        
        # ê²°ê³¼ ë¡œê¹…
        self.logger.info(f"ğŸ“Š {step_name} ë¬´ê²°ì„± ê²€ì‚¬ ì™„ë£Œ:")
        self.logger.info(f"  ì „ì²´ ê²€ì‚¬: {total_checks}ê°œ")
        self.logger.info(f"  í†µê³¼: {checks_passed}ê°œ")
        self.logger.info(f"  ì‹¤íŒ¨: {failed_checks}ê°œ")
        self.logger.info(f"  ê²½ê³ : {len(warnings)}ê°œ")
        self.logger.info(f"  ì˜¤ë¥˜: {len(errors)}ê°œ")
        self.logger.info(f"  ì„±ê³µë¥ : {report.success_rate:.1%}")
        
        if passed:
            self.logger.info(f"âœ… {step_name} ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ í†µê³¼")
        else:
            self.logger.error(f"âŒ {step_name} ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ ì‹¤íŒ¨")
            for error in errors:
                self.logger.error(f"  - {error}")
        
        if warnings:
            self.logger.warning(f"âš ï¸ {len(warnings)}ê°œ ê²½ê³ ì‚¬í•­:")
            for warning in warnings:
                self.logger.warning(f"  - {warning}")
        
        self.check_results.append(report)
        
        self.logger.info(f"=== {step_name} ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ ì™„ë£Œ ===\n")
        
        return report
    
    def check_model_outputs(self, predictions: np.ndarray, probabilities: np.ndarray = None,
                          true_labels: np.ndarray = None, 
                          step_name: str = "model_prediction") -> DataIntegrityReport:
        """
        ëª¨ë¸ ì¶œë ¥ì˜ ë¬´ê²°ì„±ì„ ê²€ì‚¬í•©ë‹ˆë‹¤.
        
        Parameters:
        -----------
        predictions : np.ndarray
            ì˜ˆì¸¡ ê²°ê³¼
        probabilities : np.ndarray, optional
            ì˜ˆì¸¡ í™•ë¥ 
        true_labels : np.ndarray, optional
            ì‹¤ì œ ë ˆì´ë¸”
        step_name : str
            ê²€ì‚¬ ë‹¨ê³„ ì´ë¦„
            
        Returns:
        --------
        DataIntegrityReport
            ë¬´ê²°ì„± ê²€ì‚¬ ë³´ê³ ì„œ
        """
        self.logger.info(f"=== {step_name} ëª¨ë¸ ì¶œë ¥ ë¬´ê²°ì„± ê²€ì‚¬ ì‹œì‘ ===")
        
        warnings = []
        errors = []
        checks_passed = 0
        total_checks = 0
        details = {}
        
        try:
            # 1. ì˜ˆì¸¡ ê²°ê³¼ ê¸°ë³¸ ê²€ì‚¬
            total_checks += 1
            if len(predictions) > 0:
                checks_passed += 1
                self.logger.info(f"âœ… ì˜ˆì¸¡ ê²°ê³¼ ì¡´ì¬: {len(predictions)}ê°œ")
            else:
                errors.append("ì˜ˆì¸¡ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
                self.logger.error("âŒ ì˜ˆì¸¡ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
            
            # 2. ì˜ˆì¸¡ ê°’ ë²”ìœ„ ê²€ì‚¬
            total_checks += 1
            expected_classes = len(self.config.class_names) if self.config else 3
            unique_preds = np.unique(predictions)
            
            if (np.all(unique_preds >= 0) and 
                np.all(unique_preds < expected_classes)):
                checks_passed += 1
                self.logger.info(f"âœ… ì˜ˆì¸¡ ë²”ìœ„ ì •ìƒ: {unique_preds}")
            else:
                errors.append(f"ì˜ˆì¸¡ ë²”ìœ„ ì˜¤ë¥˜: {unique_preds} (ê¸°ëŒ€: 0-{expected_classes-1})")
                self.logger.error(f"âŒ ì˜ˆì¸¡ ë²”ìœ„ ì˜¤ë¥˜: {unique_preds}")
            
            details['prediction_info'] = {
                'total_predictions': len(predictions),
                'unique_predictions': unique_preds.tolist(),
                'prediction_distribution': dict(zip(*np.unique(predictions, return_counts=True)))
            }
            
            # 3. í™•ë¥  ê²€ì‚¬ (ì œê³µëœ ê²½ìš°)
            if probabilities is not None:
                total_checks += 1
                if probabilities.shape[0] == len(predictions):
                    checks_passed += 1
                    self.logger.info("âœ… í™•ë¥ -ì˜ˆì¸¡ ê¸¸ì´ ì¼ì¹˜")
                else:
                    errors.append(f"í™•ë¥ -ì˜ˆì¸¡ ê¸¸ì´ ë¶ˆì¼ì¹˜: {probabilities.shape[0]} vs {len(predictions)}")
                    self.logger.error(f"âŒ í™•ë¥ -ì˜ˆì¸¡ ê¸¸ì´ ë¶ˆì¼ì¹˜")
                
                total_checks += 1
                prob_sums = np.sum(probabilities, axis=1)
                if np.allclose(prob_sums, 1.0, atol=1e-6):
                    checks_passed += 1
                    self.logger.info("âœ… í™•ë¥  í•©ê³„ ì •ìƒ (â‰ˆ1.0)")
                else:
                    warnings.append(f"í™•ë¥  í•©ê³„ ì£¼ì˜: í‰ê·  {np.mean(prob_sums):.4f}")
                    self.logger.warning(f"âš ï¸ í™•ë¥  í•©ê³„: í‰ê·  {np.mean(prob_sums):.4f}")
                
                total_checks += 1
                if np.all(probabilities >= 0) and np.all(probabilities <= 1):
                    checks_passed += 1
                    self.logger.info("âœ… í™•ë¥  ë²”ìœ„ ì •ìƒ [0, 1]")
                else:
                    errors.append("í™•ë¥  ë²”ìœ„ ì˜¤ë¥˜: [0, 1] ë²—ì–´ë‚¨")
                    self.logger.error("âŒ í™•ë¥  ë²”ìœ„ ì˜¤ë¥˜")
                
                details['probability_info'] = {
                    'shape': probabilities.shape,
                    'mean_confidence': float(np.mean(np.max(probabilities, axis=1))),
                    'min_probability': float(np.min(probabilities)),
                    'max_probability': float(np.max(probabilities))
                }
            
            # 4. ì‹¤ì œ ë ˆì´ë¸”ê³¼ ë¹„êµ (ì œê³µëœ ê²½ìš°)
            if true_labels is not None:
                total_checks += 1
                if len(true_labels) == len(predictions):
                    checks_passed += 1
                    accuracy = np.mean(true_labels == predictions)
                    self.logger.info(f"âœ… ì˜ˆì¸¡ ì •í™•ë„: {accuracy:.3f}")
                    
                    if accuracy < 0.1:  # 10% ë¯¸ë§Œì´ë©´ ê²½ê³ 
                        warnings.append(f"ë‚®ì€ ì •í™•ë„: {accuracy:.3f}")
                        self.logger.warning(f"âš ï¸ ë‚®ì€ ì •í™•ë„: {accuracy:.3f}")
                    
                    details['accuracy_info'] = {
                        'accuracy': float(accuracy),
                        'correct_predictions': int(np.sum(true_labels == predictions)),
                        'total_predictions': len(predictions)
                    }
                else:
                    errors.append(f"ì‹¤ì œ ë ˆì´ë¸”-ì˜ˆì¸¡ ê¸¸ì´ ë¶ˆì¼ì¹˜: {len(true_labels)} vs {len(predictions)}")
                    self.logger.error(f"âŒ ì‹¤ì œ ë ˆì´ë¸”-ì˜ˆì¸¡ ê¸¸ì´ ë¶ˆì¼ì¹˜")
        
        except Exception as e:
            errors.append(f"ëª¨ë¸ ì¶œë ¥ ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            self.logger.error(f"âŒ ëª¨ë¸ ì¶œë ¥ ê²€ì‚¬ ì˜¤ë¥˜: {e}")
        
        # ë³´ê³ ì„œ ìƒì„±
        passed = len(errors) == 0
        failed_checks = total_checks - checks_passed
        
        report = DataIntegrityReport(
            step_name=step_name,
            passed=passed,
            total_checks=total_checks,
            passed_checks=checks_passed,
            failed_checks=failed_checks,
            warnings=warnings,
            errors=errors,
            details=details
        )
        
        # ê²°ê³¼ ë¡œê¹…
        self.logger.info(f"ğŸ“Š {step_name} ì¶œë ¥ ë¬´ê²°ì„± ê²€ì‚¬ ì™„ë£Œ:")
        self.logger.info(f"  ì „ì²´ ê²€ì‚¬: {total_checks}ê°œ")
        self.logger.info(f"  í†µê³¼: {checks_passed}ê°œ")
        self.logger.info(f"  ì‹¤íŒ¨: {failed_checks}ê°œ")
        self.logger.info(f"  ì„±ê³µë¥ : {report.success_rate:.1%}")
        
        if passed:
            self.logger.info(f"âœ… {step_name} ëª¨ë¸ ì¶œë ¥ ë¬´ê²°ì„± ê²€ì‚¬ í†µê³¼")
        else:
            self.logger.error(f"âŒ {step_name} ëª¨ë¸ ì¶œë ¥ ë¬´ê²°ì„± ê²€ì‚¬ ì‹¤íŒ¨")
        
        self.check_results.append(report)
        
        self.logger.info(f"=== {step_name} ëª¨ë¸ ì¶œë ¥ ë¬´ê²°ì„± ê²€ì‚¬ ì™„ë£Œ ===\n")
        
        return report
    
    def check_pipeline_consistency(self, train_data: Tuple[np.ndarray, np.ndarray],
                                 val_data: Tuple[np.ndarray, np.ndarray],
                                 test_data: Tuple[np.ndarray, np.ndarray],
                                 step_name: str = "pipeline_consistency") -> DataIntegrityReport:
        """
        íŒŒì´í”„ë¼ì¸ ì „ì²´ì˜ ë°ì´í„° ì¼ê´€ì„±ì„ ê²€ì‚¬í•©ë‹ˆë‹¤.
        
        Parameters:
        -----------
        train_data : Tuple
            (í›ˆë ¨ íŠ¹ì§•, í›ˆë ¨ ë ˆì´ë¸”)
        val_data : Tuple
            (ê²€ì¦ íŠ¹ì§•, ê²€ì¦ ë ˆì´ë¸”)
        test_data : Tuple
            (í…ŒìŠ¤íŠ¸ íŠ¹ì§•, í…ŒìŠ¤íŠ¸ ë ˆì´ë¸”)
        step_name : str
            ê²€ì‚¬ ë‹¨ê³„ ì´ë¦„
            
        Returns:
        --------
        DataIntegrityReport
            ë¬´ê²°ì„± ê²€ì‚¬ ë³´ê³ ì„œ
        """
        self.logger.info(f"=== {step_name} íŒŒì´í”„ë¼ì¸ ì¼ê´€ì„± ê²€ì‚¬ ì‹œì‘ ===")
        
        warnings = []
        errors = []
        checks_passed = 0
        total_checks = 0
        details = {}
        
        try:
            X_train, y_train = train_data
            X_val, y_val = val_data
            X_test, y_test = test_data
            
            # 1. íŠ¹ì§• ì°¨ì› ì¼ê´€ì„± ê²€ì‚¬
            total_checks += 1
            feature_dims = [X_train.shape[1], X_val.shape[1], X_test.shape[1]]
            if len(set(feature_dims)) == 1:
                checks_passed += 1
                self.logger.info(f"âœ… íŠ¹ì§• ì°¨ì› ì¼ê´€ì„±: {feature_dims[0]}")
            else:
                errors.append(f"íŠ¹ì§• ì°¨ì› ë¶ˆì¼ì¹˜: train={feature_dims[0]}, val={feature_dims[1]}, test={feature_dims[2]}")
                self.logger.error(f"âŒ íŠ¹ì§• ì°¨ì› ë¶ˆì¼ì¹˜: {feature_dims}")
            
            # 2. ë°ì´í„° íƒ€ì… ì¼ê´€ì„± ê²€ì‚¬
            total_checks += 1
            dtypes = [X_train.dtype, X_val.dtype, X_test.dtype]
            if len(set(dtypes)) == 1:
                checks_passed += 1
                self.logger.info(f"âœ… ë°ì´í„° íƒ€ì… ì¼ê´€ì„±: {dtypes[0]}")
            else:
                warnings.append(f"ë°ì´í„° íƒ€ì… ë¶ˆì¼ì¹˜: {dtypes}")
                self.logger.warning(f"âš ï¸ ë°ì´í„° íƒ€ì… ë¶ˆì¼ì¹˜: {dtypes}")
            
            # 3. í´ë˜ìŠ¤ ë ˆì´ë¸” ì¼ê´€ì„± ê²€ì‚¬  
            total_checks += 1
            train_classes = set(np.unique(y_train))
            val_classes = set(np.unique(y_val))
            test_classes = set(np.unique(y_test))
            all_classes = train_classes | val_classes | test_classes
            
            if train_classes == val_classes == test_classes:
                checks_passed += 1
                self.logger.info(f"âœ… í´ë˜ìŠ¤ ë ˆì´ë¸” ì¼ê´€ì„±: {sorted(all_classes)}")
            else:
                warnings.append(f"í´ë˜ìŠ¤ ë¶„í¬ ì°¨ì´: train={sorted(train_classes)}, val={sorted(val_classes)}, test={sorted(test_classes)}")
                self.logger.warning(f"âš ï¸ í´ë˜ìŠ¤ ë¶„í¬ ì°¨ì´ ë°œê²¬")
            
            # 4. ë°ì´í„° ë¶„í•  ë¹„ìœ¨ ê²€ì‚¬
            total_checks += 1
            total_samples = len(X_train) + len(X_val) + len(X_test)
            train_ratio = len(X_train) / total_samples
            val_ratio = len(X_val) / total_samples
            test_ratio = len(X_test) / total_samples
            
            # ì¼ë°˜ì ì¸ ë¶„í•  ë¹„ìœ¨ (70-15-15 ë˜ëŠ” 80-10-10)
            if (0.6 <= train_ratio <= 0.9 and 
                0.05 <= val_ratio <= 0.3 and 
                0.05 <= test_ratio <= 0.3):
                checks_passed += 1
                self.logger.info(f"âœ… ë°ì´í„° ë¶„í•  ë¹„ìœ¨ ì ì ˆ: {train_ratio:.2f}/{val_ratio:.2f}/{test_ratio:.2f}")
            else:
                warnings.append(f"ë°ì´í„° ë¶„í•  ë¹„ìœ¨ ì£¼ì˜: {train_ratio:.2f}/{val_ratio:.2f}/{test_ratio:.2f}")
                self.logger.warning(f"âš ï¸ ë°ì´í„° ë¶„í•  ë¹„ìœ¨: {train_ratio:.2f}/{val_ratio:.2f}/{test_ratio:.2f}")
            
            # 5. ë°ì´í„° ëˆ„ì¶œ ê²€ì‚¬ (ê°„ë‹¨í•œ ì¤‘ë³µ ê²€ì‚¬)
            total_checks += 1
            # í›ˆë ¨-ê²€ì¦ ì¤‘ë³µ í™•ì¸ (ê·¼ì‚¬ì¹˜, ì‹¤ì œë¡œëŠ” í•´ì‹œë‚˜ ë” ì •êµí•œ ë°©ë²• í•„ìš”)
            if len(X_train) > 0 and len(X_val) > 0:
                # ê°„ë‹¨í•œ í‰ê· ê°’ ë¹„êµë¡œ ëŒ€ëµì  ì¤‘ë³µ í™•ì¸
                train_means = np.mean(X_train, axis=1)
                val_means = np.mean(X_val, axis=1)
                
                duplicates = 0
                for train_mean in train_means[:min(100, len(train_means))]:  # ì²˜ìŒ 100ê°œë§Œ í™•ì¸
                    if np.any(np.abs(val_means - train_mean) < 1e-6):
                        duplicates += 1
                
                if duplicates == 0:
                    checks_passed += 1
                    self.logger.info("âœ… í›ˆë ¨-ê²€ì¦ ë°ì´í„° ëˆ„ì¶œ ì—†ìŒ")
                else:
                    warnings.append(f"ì ì¬ì  ë°ì´í„° ëˆ„ì¶œ: {duplicates}ê°œ ìœ ì‚¬ ìƒ˜í”Œ")
                    self.logger.warning(f"âš ï¸ ì ì¬ì  ë°ì´í„° ëˆ„ì¶œ: {duplicates}ê°œ")
            else:
                checks_passed += 1  # ë¹ˆ ë°ì´í„°ì…‹ì€ ëˆ„ì¶œ ì—†ìŒìœ¼ë¡œ ê°„ì£¼
            
            details['consistency_info'] = {
                'feature_dimensions': feature_dims,
                'data_types': [str(dt) for dt in dtypes],
                'class_distribution': {
                    'train': sorted(train_classes),
                    'val': sorted(val_classes),
                    'test': sorted(test_classes)
                },
                'split_ratios': {
                    'train': float(train_ratio),
                    'val': float(val_ratio),
                    'test': float(test_ratio)
                },
                'sample_counts': {
                    'train': len(X_train),
                    'val': len(X_val),
                    'test': len(X_test),
                    'total': total_samples
                }
            }
            
        except Exception as e:
            errors.append(f"íŒŒì´í”„ë¼ì¸ ì¼ê´€ì„± ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            self.logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì¼ê´€ì„± ê²€ì‚¬ ì˜¤ë¥˜: {e}")
        
        # ë³´ê³ ì„œ ìƒì„±
        passed = len(errors) == 0
        failed_checks = total_checks - checks_passed
        
        report = DataIntegrityReport(
            step_name=step_name,
            passed=passed,
            total_checks=total_checks,
            passed_checks=checks_passed,
            failed_checks=failed_checks,
            warnings=warnings,
            errors=errors,
            details=details
        )
        
        # ê²°ê³¼ ë¡œê¹…
        self.logger.info(f"ğŸ“Š {step_name} ì¼ê´€ì„± ê²€ì‚¬ ì™„ë£Œ:")
        self.logger.info(f"  ì „ì²´ ê²€ì‚¬: {total_checks}ê°œ")
        self.logger.info(f"  í†µê³¼: {checks_passed}ê°œ")
        self.logger.info(f"  ì‹¤íŒ¨: {failed_checks}ê°œ")
        self.logger.info(f"  ì„±ê³µë¥ : {report.success_rate:.1%}")
        
        if passed:
            self.logger.info(f"âœ… {step_name} íŒŒì´í”„ë¼ì¸ ì¼ê´€ì„± ê²€ì‚¬ í†µê³¼")
        else:
            self.logger.error(f"âŒ {step_name} íŒŒì´í”„ë¼ì¸ ì¼ê´€ì„± ê²€ì‚¬ ì‹¤íŒ¨")
        
        self.check_results.append(report)
        
        self.logger.info(f"=== {step_name} íŒŒì´í”„ë¼ì¸ ì¼ê´€ì„± ê²€ì‚¬ ì™„ë£Œ ===\n")
        
        return report
    
    def get_summary_report(self) -> Dict[str, Any]:
        """
        ì „ì²´ ë¬´ê²°ì„± ê²€ì‚¬ ìš”ì•½ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Returns:
        --------
        Dict[str, Any]
            ìš”ì•½ ë³´ê³ ì„œ
        """
        if not self.check_results:
            return {'message': 'ì‹¤í–‰ëœ ë¬´ê²°ì„± ê²€ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.'}
        
        total_checks = sum(r.total_checks for r in self.check_results)
        total_passed = sum(r.passed_checks for r in self.check_results)
        total_failed = total_checks - total_passed
        
        total_warnings = sum(len(r.warnings) for r in self.check_results)
        total_errors = sum(len(r.errors) for r in self.check_results)
        
        overall_success_rate = total_passed / total_checks if total_checks > 0 else 0
        
        step_summaries = []
        for report in self.check_results:
            step_summaries.append({
                'step_name': report.step_name,
                'passed': report.passed,
                'success_rate': report.success_rate,
                'checks': report.total_checks,
                'warnings': len(report.warnings),
                'errors': len(report.errors)
            })
        
        summary = {
            'overall_statistics': {
                'total_checks_run': len(self.check_results),
                'total_individual_checks': total_checks,
                'total_passed_checks': total_passed,
                'total_failed_checks': total_failed,
                'overall_success_rate': overall_success_rate,
                'total_warnings': total_warnings,
                'total_errors': total_errors
            },
            'step_summaries': step_summaries,
            'overall_status': 'PASSED' if total_errors == 0 else 'FAILED'
        }
        
        return summary