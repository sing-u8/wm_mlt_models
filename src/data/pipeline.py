"""
ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê´€ë¦¬ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ ì ì ˆí•œ ìˆœì„œë¥¼ í†µí•´ ë°ì´í„° íë¦„ì„ ê´€ë¦¬í•˜ê³  
ë°ì´í„° ëˆ„ì¶œì„ ë°©ì§€í•˜ëŠ” í¬ê´„ì ì¸ íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
"""

import os
from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional, Any
from pathlib import Path
import numpy as np
from collections import defaultdict

from ..utils.logger import LoggerMixin
from ..audio.feature_extraction import extract_features, FeatureVector
from .augmentation import BatchAugmentor, AugmentationResult
from config import DEFAULT_CONFIG


@dataclass
class AudioFile:
    """
    ì˜¤ë””ì˜¤ íŒŒì¼ ë©”íƒ€ë°ì´í„°ë¥¼ ê´€ë¦¬í•˜ëŠ” ë°ì´í„° í´ë˜ìŠ¤.
    """
    file_path: str
    class_name: str
    split: str  # 'train', 'validation', 'test'
    is_augmented: bool = False
    original_file: Optional[str] = None  # ì¦ê°•ëœ ê²½ìš° ì›ë³¸ íŒŒì¼ ê²½ë¡œ
    noise_type: Optional[str] = None     # ì‚¬ìš©ëœ ì†ŒìŒ íƒ€ì…
    snr_level: Optional[float] = None    # ì‚¬ìš©ëœ SNR ë ˆë²¨
    file_size: Optional[int] = None      # íŒŒì¼ í¬ê¸° (bytes)
    duration: Optional[float] = None     # ì˜¤ë””ì˜¤ ê¸¸ì´ (seconds)
    
    def __post_init__(self):
        """íŒŒì¼ ë©”íƒ€ë°ì´í„° ìë™ ì„¤ì •"""
        if os.path.exists(self.file_path):
            self.file_size = os.path.getsize(self.file_path)
        
        # ì¦ê°• íŒŒì¼ì—ì„œ ì •ë³´ ì¶”ì¶œ
        if self.is_augmented and "_noise_" in self.file_path:
            filename = Path(self.file_path).stem
            parts = filename.split("_")
            
            # SNR ì •ë³´ ì¶”ì¶œ (ì˜ˆ: snr+5dB)
            for part in parts:
                if part.startswith("snr"):
                    try:
                        snr_str = part.replace("snr", "").replace("dB", "")
                        self.snr_level = float(snr_str)
                    except ValueError:
                        pass


@dataclass
class DatasetSplit:
    """
    ë°ì´í„°ì…‹ ë¶„í•  ì •ë³´ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤.
    """
    train_files: Dict[str, List[AudioFile]]
    validation_files: Dict[str, List[AudioFile]]
    test_files: Dict[str, List[AudioFile]]
    noise_files: List[str]
    total_train: int = 0
    total_validation: int = 0
    total_test: int = 0
    
    def __post_init__(self):
        """ì „ì²´ íŒŒì¼ ìˆ˜ ê³„ì‚°"""
        self.total_train = sum(len(files) for files in self.train_files.values())
        self.total_validation = sum(len(files) for files in self.validation_files.values())
        self.total_test = sum(len(files) for files in self.test_files.values())


class DataPipeline(LoggerMixin):
    """
    ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê´€ë¦¬ í´ë˜ìŠ¤.
    
    design.mdì— ëª…ì‹œëœ ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•˜ê³ 
    ë°ì´í„° ëˆ„ì¶œ ë°©ì§€ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, config=None):
        """
        ë°ì´í„° ë””ë ‰í† ë¦¬ë¡œ íŒŒì´í”„ë¼ì¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        Parameters:
        -----------
        config : Config, optional
            êµ¬ì„± ê°ì²´. Noneì´ë©´ ê¸°ë³¸ êµ¬ì„±ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """
        self.config = config or DEFAULT_CONFIG
        
        # ê²½ë¡œ ì„¤ì •
        self.raw_data_dir = self.config.raw_data_dir
        self.noise_dir = self.config.noise_dir
        self.processed_dir = self.config.processed_dir
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.batch_augmentor = BatchAugmentor(config)
        
        # ìƒíƒœ ê´€ë¦¬
        self._dataset_split = None
        self._augmentation_results = {}
        
        self.logger.info(f"DataPipeline ì´ˆê¸°í™”ë¨")
        self.logger.info(f"  ì›ë³¸ ë°ì´í„°: {self.raw_data_dir}")
        self.logger.info(f"  ì†ŒìŒ ë°ì´í„°: {self.noise_dir}")
        self.logger.info(f"  ì²˜ë¦¬ëœ ë°ì´í„°: {self.processed_dir}")
    
    def _load_class_files(self, data_dir: str, split_name: str) -> Dict[str, List[AudioFile]]:
        """
        ì§€ì •ëœ ë””ë ‰í† ë¦¬ì—ì„œ í´ë˜ìŠ¤ë³„ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        
        Parameters:
        -----------
        data_dir : str
            ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        split_name : str
            ë¶„í•  ì´ë¦„ ('train', 'validation', 'test')
            
        Returns:
        --------
        Dict[str, List[AudioFile]]
            í´ë˜ìŠ¤ë³„ ì˜¤ë””ì˜¤ íŒŒì¼ ë”•ì…”ë„ˆë¦¬
        """
        class_files = {}
        
        if not os.path.exists(data_dir):
            self.logger.warning(f"ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {data_dir}")
            return class_files
        
        for class_name in self.config.class_names:
            class_dir = os.path.join(data_dir, class_name)
            audio_files = []
            
            if os.path.exists(class_dir):
                for file_name in os.listdir(class_dir):
                    if file_name.lower().endswith('.wav'):
                        file_path = os.path.join(class_dir, file_name)
                        
                        audio_file = AudioFile(
                            file_path=file_path,
                            class_name=class_name,
                            split=split_name,
                            is_augmented=False
                        )
                        audio_files.append(audio_file)
            
            class_files[class_name] = audio_files
            self.logger.debug(f"{split_name} {class_name}: {len(audio_files)}ê°œ íŒŒì¼")
        
        return class_files
    
    def load_train_data(self) -> Dict[str, List[AudioFile]]:
        """
        data/raw/train/ ë””ë ‰í† ë¦¬ì—ì„œ í´ë˜ìŠ¤ë³„ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        
        Returns:
        --------
        Dict[str, List[AudioFile]]
            í´ë˜ìŠ¤ë³„ í›ˆë ¨ ì˜¤ë””ì˜¤ íŒŒì¼ ë”•ì…”ë„ˆë¦¬
        """
        train_dir = os.path.join(self.raw_data_dir, "train")
        train_files = self._load_class_files(train_dir, "train")
        
        total_files = sum(len(files) for files in train_files.values())
        self.logger.info(f"í›ˆë ¨ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {total_files}ê°œ íŒŒì¼")
        
        return train_files
    
    def load_validation_data(self) -> Dict[str, List[AudioFile]]:
        """
        data/raw/validation/ ë””ë ‰í† ë¦¬ì—ì„œ í´ë˜ìŠ¤ë³„ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        
        Returns:
        --------
        Dict[str, List[AudioFile]]
            í´ë˜ìŠ¤ë³„ ê²€ì¦ ì˜¤ë””ì˜¤ íŒŒì¼ ë”•ì…”ë„ˆë¦¬
        """
        validation_dir = os.path.join(self.raw_data_dir, "validation")
        validation_files = self._load_class_files(validation_dir, "validation")
        
        total_files = sum(len(files) for files in validation_files.values())
        self.logger.info(f"ê²€ì¦ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {total_files}ê°œ íŒŒì¼")
        
        return validation_files
    
    def load_test_data(self) -> Dict[str, List[AudioFile]]:
        """
        data/raw/test/ ë””ë ‰í† ë¦¬ì—ì„œ í´ë˜ìŠ¤ë³„ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        
        Returns:
        --------
        Dict[str, List[AudioFile]]
            í´ë˜ìŠ¤ë³„ í…ŒìŠ¤íŠ¸ ì˜¤ë””ì˜¤ íŒŒì¼ ë”•ì…”ë„ˆë¦¬
        """
        test_dir = os.path.join(self.raw_data_dir, "test")
        test_files = self._load_class_files(test_dir, "test")
        
        total_files = sum(len(files) for files in test_files.values())
        self.logger.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {total_files}ê°œ íŒŒì¼")
        
        return test_files
    
    def load_noise_files(self) -> List[str]:
        """
        data/noise/ ë””ë ‰í† ë¦¬ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì†ŒìŒ íŒŒì¼ì„ ì¬ê·€ì ìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.
        
        Returns:
        --------
        List[str]
            ì†ŒìŒ íŒŒì¼ ê²½ë¡œ ëª©ë¡
        """
        noise_files = self.config.get_all_noise_files()
        
        if noise_files:
            self.logger.info(f"ì†ŒìŒ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(noise_files)}ê°œ íŒŒì¼")
            
            # ì†ŒìŒ íƒ€ì…ë³„ í†µê³„
            noise_stats = defaultdict(int)
            for noise_file in noise_files:
                noise_type = self.batch_augmentor._extract_noise_type(noise_file)
                noise_stats[noise_type] += 1
            
            for noise_type, count in noise_stats.items():
                self.logger.info(f"  {noise_type}: {count}ê°œ íŒŒì¼")
        else:
            self.logger.warning("ì‚¬ìš© ê°€ëŠ¥í•œ ì†ŒìŒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        return noise_files
    
    def load_all_data(self) -> DatasetSplit:
        """
        ëª¨ë“  ë°ì´í„° ë¶„í• ì„ ë¡œë“œí•˜ê³  DatasetSplit ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
        --------
        DatasetSplit
            ì „ì²´ ë°ì´í„°ì…‹ ë¶„í•  ì •ë³´
        """
        self.logger.info("=== ì „ì²´ ë°ì´í„° ë¡œë”© ì‹œì‘ ===")
        
        # ê° ë¶„í•  ë°ì´í„° ë¡œë“œ
        train_files = self.load_train_data()
        validation_files = self.load_validation_data()
        test_files = self.load_test_data()
        noise_files = self.load_noise_files()
        
        # DatasetSplit ìƒì„±
        dataset_split = DatasetSplit(
            train_files=train_files,
            validation_files=validation_files,
            test_files=test_files,
            noise_files=noise_files
        )
        
        self._dataset_split = dataset_split
        
        self.logger.info("=== ì „ì²´ ë°ì´í„° ë¡œë”© ì™„ë£Œ ===")
        self.logger.info(f"  í›ˆë ¨: {dataset_split.total_train}ê°œ íŒŒì¼")
        self.logger.info(f"  ê²€ì¦: {dataset_split.total_validation}ê°œ íŒŒì¼")
        self.logger.info(f"  í…ŒìŠ¤íŠ¸: {dataset_split.total_test}ê°œ íŒŒì¼")
        self.logger.info(f"  ì†ŒìŒ: {len(dataset_split.noise_files)}ê°œ íŒŒì¼")
        
        return dataset_split
    
    def augment_training_data(self, noise_files: List[str] = None, 
                            force_augmentation: bool = False) -> Dict[str, List[AudioFile]]:
        """
        í›ˆë ¨ ì„¸íŠ¸ì—ë§Œ ì¦ê°•ì„ ì ìš©í•©ë‹ˆë‹¤. 
        ì†ŒìŒ íŒŒì¼ì´ ì—†ìœ¼ë©´ ì›ë³¸ ë°ì´í„°ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        
        Parameters:
        -----------
        noise_files : List[str], optional
            ì‚¬ìš©í•  ì†ŒìŒ íŒŒì¼ ëª©ë¡. Noneì´ë©´ ìë™ìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.
        force_augmentation : bool
            ì†ŒìŒ íŒŒì¼ì´ ë¶€ì¡±í•´ë„ ê°•ì œë¡œ ì¦ê°•ì„ ìˆ˜í–‰í• ì§€ ì—¬ë¶€
            
        Returns:
        --------
        Dict[str, List[AudioFile]]
            ì¦ê°•ëœ(ë˜ëŠ” ì›ë³¸) í›ˆë ¨ ë°ì´í„°
        """
        self.logger.info("=== í›ˆë ¨ ë°ì´í„° ì¦ê°• ì‹œì‘ ===")
        
        # í›ˆë ¨ ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìœ¼ë©´ ë¡œë“œ
        if self._dataset_split is None:
            self.load_all_data()
        
        train_files = self._dataset_split.train_files
        
        # ì†ŒìŒ íŒŒì¼ ì¤€ë¹„
        if noise_files is None:
            noise_files = self._dataset_split.noise_files
        
        # ì†ŒìŒ íŒŒì¼ ê²€ì¦
        if not noise_files:
            self.logger.warning("ì†ŒìŒ íŒŒì¼ì´ ì—†ì–´ ì›ë³¸ í›ˆë ¨ ë°ì´í„°ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return train_files
        
        if len(noise_files) < self.config.min_noise_files and not force_augmentation:
            self.logger.warning(f"ì†ŒìŒ íŒŒì¼ ë¶€ì¡± ({len(noise_files)}ê°œ < {self.config.min_noise_files}ê°œ). "
                              f"ì›ë³¸ í›ˆë ¨ ë°ì´í„°ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return train_files
        
        # ì¦ê°• ì¶œë ¥ ë””ë ‰í† ë¦¬ ì¤€ë¹„
        augmented_base_dir = os.path.join(self.processed_dir, "augmented")
        os.makedirs(augmented_base_dir, exist_ok=True)
        
        # í´ë˜ìŠ¤ë³„ ì¦ê°• ìˆ˜í–‰
        augmented_train_files = {}
        self._augmentation_results = {}
        
        for class_name, class_files in train_files.items():
            if not class_files:
                self.logger.warning(f"í´ë˜ìŠ¤ {class_name}ì— í›ˆë ¨ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                augmented_train_files[class_name] = []
                continue
            
            self.logger.info(f"í´ë˜ìŠ¤ {class_name} ì¦ê°• ì‹œì‘ ({len(class_files)}ê°œ ì›ë³¸ íŒŒì¼)")
            
            # í´ë˜ìŠ¤ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            class_dir = os.path.join(self.raw_data_dir, "train", class_name)
            
            # ë°°ì¹˜ ì¦ê°• ìˆ˜í–‰
            try:
                augmentation_result = self.batch_augmentor.augment_class_directory(
                    class_dir, class_name, noise_files, augmented_base_dir
                )
                
                self._augmentation_results[class_name] = augmentation_result
                
                # ì¦ê°•ëœ íŒŒì¼ë“¤ì„ AudioFile ê°ì²´ë¡œ ë³€í™˜
                augmented_audio_files = []
                
                # ì›ë³¸ íŒŒì¼ë“¤ ì¶”ê°€
                augmented_audio_files.extend(class_files)
                
                # ì¦ê°•ëœ íŒŒì¼ë“¤ ì¶”ê°€
                for augmented_path in augmentation_result.augmented_files:
                    augmented_file = AudioFile(
                        file_path=augmented_path,
                        class_name=class_name,
                        split="train",
                        is_augmented=True,
                        original_file=class_dir  # ì›ë³¸ í´ë˜ìŠ¤ ë””ë ‰í† ë¦¬
                    )
                    augmented_audio_files.append(augmented_file)
                
                augmented_train_files[class_name] = augmented_audio_files
                
                self.logger.info(f"í´ë˜ìŠ¤ {class_name} ì¦ê°• ì™„ë£Œ: "
                               f"{len(class_files)}ê°œ ì›ë³¸ + {augmentation_result.total_created}ê°œ ì¦ê°• "
                               f"= {len(augmented_audio_files)}ê°œ ì´ íŒŒì¼")
                
            except Exception as e:
                self.logger.error(f"í´ë˜ìŠ¤ {class_name} ì¦ê°• ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨ ì‹œ ì›ë³¸ íŒŒì¼ë§Œ ì‚¬ìš©
                augmented_train_files[class_name] = class_files
        
        # ì „ì²´ í†µê³„
        total_original = sum(len(files) for files in train_files.values())
        total_augmented = sum(len(files) for files in augmented_train_files.values())
        
        self.logger.info("=== í›ˆë ¨ ë°ì´í„° ì¦ê°• ì™„ë£Œ ===")
        self.logger.info(f"  ì›ë³¸: {total_original}ê°œ íŒŒì¼")
        self.logger.info(f"  ì´ íŒŒì¼: {total_augmented}ê°œ íŒŒì¼")
        self.logger.info(f"  ì¦ê°• ë¹„ìœ¨: {(total_augmented / total_original):.1f}x")
        
        return augmented_train_files
    
    def validate_data_integrity(self) -> bool:
        """
        ë°ì´í„° ë¬´ê²°ì„±ì„ ê²€ì¦í•˜ê³  ë°ì´í„° ëˆ„ì¶œì´ ì—†ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        
        Returns:
        --------
        bool
            ë°ì´í„° ë¬´ê²°ì„±ì´ ìœ ì§€ë˜ë©´ True
        """
        self.logger.info("=== ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ ì‹œì‘ ===")
        
        if self._dataset_split is None:
            self.logger.error("ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        integrity_issues = []
        
        # 1. ê° ë¶„í• ì— ëª¨ë“  í´ë˜ìŠ¤ê°€ ìˆëŠ”ì§€ í™•ì¸
        for split_name, split_files in [
            ("train", self._dataset_split.train_files),
            ("validation", self._dataset_split.validation_files),
            ("test", self._dataset_split.test_files)
        ]:
            missing_classes = []
            for class_name in self.config.class_names:
                if class_name not in split_files or not split_files[class_name]:
                    missing_classes.append(class_name)
            
            if missing_classes:
                integrity_issues.append(f"{split_name} ë¶„í• ì—ì„œ ëˆ„ë½ëœ í´ë˜ìŠ¤: {missing_classes}")
        
        # 2. íŒŒì¼ ì¡´ì¬ í™•ì¸
        missing_files = []
        for split_name, split_files in [
            ("train", self._dataset_split.train_files),
            ("validation", self._dataset_split.validation_files),
            ("test", self._dataset_split.test_files)
        ]:
            for class_name, files in split_files.items():
                for audio_file in files:
                    if not os.path.exists(audio_file.file_path):
                        missing_files.append(f"{split_name}/{class_name}/{os.path.basename(audio_file.file_path)}")
        
        if missing_files:
            integrity_issues.append(f"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” íŒŒì¼ë“¤: {missing_files[:5]}{'... ë“±' if len(missing_files) > 5 else ''}")
        
        # 3. ë°ì´í„° ëˆ„ì¶œ ê²€ì‚¬: ê°™ì€ íŒŒì¼ì´ ì—¬ëŸ¬ ë¶„í• ì— ìˆëŠ”ì§€ í™•ì¸
        all_file_paths = {}
        for split_name, split_files in [
            ("train", self._dataset_split.train_files),
            ("validation", self._dataset_split.validation_files),
            ("test", self._dataset_split.test_files)
        ]:
            for class_name, files in split_files.items():
                for audio_file in files:
                    if not audio_file.is_augmented:  # ì›ë³¸ íŒŒì¼ë§Œ ê²€ì‚¬
                        file_key = os.path.basename(audio_file.file_path)
                        if file_key in all_file_paths:
                            integrity_issues.append(f"ë°ì´í„° ëˆ„ì¶œ ê°ì§€: {file_key}ê°€ {all_file_paths[file_key]}ì™€ {split_name}ì— ë™ì‹œ ì¡´ì¬")
                        else:
                            all_file_paths[file_key] = split_name
        
        # 4. ì¦ê°• ë°ì´í„° ê²€ì¦: ì¦ê°•ì´ í›ˆë ¨ ì„¸íŠ¸ì—ë§Œ ì ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸
        for split_name, split_files in [
            ("validation", self._dataset_split.validation_files),
            ("test", self._dataset_split.test_files)
        ]:
            for class_name, files in split_files.items():
                augmented_in_split = [f for f in files if f.is_augmented]
                if augmented_in_split:
                    integrity_issues.append(f"{split_name} ë¶„í• ì— ì¦ê°• ë°ì´í„° ë°œê²¬: {len(augmented_in_split)}ê°œ íŒŒì¼")
        
        # ê²°ê³¼ ë³´ê³ 
        if integrity_issues:
            self.logger.error("ë°ì´í„° ë¬´ê²°ì„± ë¬¸ì œ ë°œê²¬:")
            for issue in integrity_issues:
                self.logger.error(f"  - {issue}")
            return False
        else:
            self.logger.info("âœ… ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ í†µê³¼")
            self.logger.info("  - ëª¨ë“  í´ë˜ìŠ¤ê°€ ê° ë¶„í• ì— ì¡´ì¬")
            self.logger.info("  - ëª¨ë“  íŒŒì¼ì´ ì¡´ì¬í•¨")
            self.logger.info("  - ë°ì´í„° ëˆ„ì¶œ ì—†ìŒ")
            self.logger.info("  - ì¦ê°•ì´ í›ˆë ¨ ì„¸íŠ¸ì—ë§Œ ì ìš©ë¨")
            return True
    
    def extract_all_features(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, 
                                         np.ndarray, np.ndarray, np.ndarray]:
        """
        ëª¨ë“  ë°ì´í„°ì…‹ì—ì„œ íŠ¹ì§•ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Returns:
        --------
        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]
            (X_train, y_train, X_val, y_val, X_test, y_test)
        """
        self.logger.info("=== ì „ì²´ íŠ¹ì§• ì¶”ì¶œ ì‹œì‘ ===")
        
        if self._dataset_split is None:
            raise ValueError("ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_all_data()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        # í´ë˜ìŠ¤ ë¼ë²¨ ë§¤í•‘
        class_to_label = {class_name: idx for idx, class_name in enumerate(self.config.class_names)}
        
        # ê° ë¶„í• ë³„ íŠ¹ì§• ì¶”ì¶œ
        def extract_split_features(split_files: Dict[str, List[AudioFile]], split_name: str):
            features = []
            labels = []
            
            total_files = sum(len(files) for files in split_files.values())
            processed = 0
            
            for class_name, audio_files in split_files.items():
                class_label = class_to_label[class_name]
                
                for audio_file in audio_files:
                    try:
                        # íŠ¹ì§• ì¶”ì¶œ
                        feature_vector = extract_features(audio_file.file_path, self.config)
                        
                        if feature_vector is not None:
                            features.append(feature_vector.to_array())
                            labels.append(class_label)
                        else:
                            self.logger.warning(f"íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {audio_file.file_path}")
                    
                    except Exception as e:
                        self.logger.error(f"íŠ¹ì§• ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ {audio_file.file_path}: {e}")
                        continue
                    
                    processed += 1
                    if processed % 100 == 0:
                        self.logger.info(f"{split_name} íŠ¹ì§• ì¶”ì¶œ ì§„í–‰: {processed}/{total_files}")
            
            if features:
                X = np.array(features)
                y = np.array(labels)
                self.logger.info(f"{split_name} íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ: {X.shape}")
                return X, y
            else:
                self.logger.error(f"{split_name} íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: ìœ íš¨í•œ íŠ¹ì§•ì´ ì—†ìŒ")
                return np.array([]), np.array([])
        
        # í›ˆë ¨ ë°ì´í„°ëŠ” ì¦ê°•ëœ ë°ì´í„° ì‚¬ìš© (ì´ë¯¸ augment_training_data í˜¸ì¶œí–ˆë‹¤ê³  ê°€ì •)
        if hasattr(self, '_augmented_train_files'):
            train_files = self._augmented_train_files
        else:
            train_files = self._dataset_split.train_files
            self.logger.warning("ì¦ê°•ëœ í›ˆë ¨ ë°ì´í„°ê°€ ì—†ì–´ ì›ë³¸ ë°ì´í„° ì‚¬ìš©")
        
        # ê° ë¶„í• ë³„ íŠ¹ì§• ì¶”ì¶œ
        X_train, y_train = extract_split_features(train_files, "í›ˆë ¨")
        X_val, y_val = extract_split_features(self._dataset_split.validation_files, "ê²€ì¦")
        X_test, y_test = extract_split_features(self._dataset_split.test_files, "í…ŒìŠ¤íŠ¸")
        
        self.logger.info("=== ì „ì²´ íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ ===")
        self.logger.info(f"  í›ˆë ¨: {X_train.shape}")
        self.logger.info(f"  ê²€ì¦: {X_val.shape}")
        self.logger.info(f"  í…ŒìŠ¤íŠ¸: {X_test.shape}")
        
        return X_train, y_train, X_val, y_val, X_test, y_test
    
    def run_complete_pipeline(self, skip_augmentation: bool = False) -> Tuple[np.ndarray, np.ndarray, 
                                                                             np.ndarray, np.ndarray, 
                                                                             np.ndarray, np.ndarray]:
        """
        ì™„ì „í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        
        design.mdì— ëª…ì‹œëœ ìˆœì„œë¥¼ ë”°ë¦…ë‹ˆë‹¤:
        1. ë°ì´í„° ë¡œë”© â†’ 2. ì†ŒìŒ íŒŒì¼ ê²€ìƒ‰ â†’ 3. í›ˆë ¨ ì¦ê°• â†’ 4. íŠ¹ì§• ì¶”ì¶œ
        
        Parameters:
        -----------
        skip_augmentation : bool
            ì¦ê°•ì„ ê±´ë„ˆë›¸ì§€ ì—¬ë¶€
            
        Returns:
        --------
        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]
            (X_train, y_train, X_val, y_val, X_test, y_test)
        """
        self.logger.info("ğŸ‰ ì™„ì „í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘ ğŸ‰")
        
        # 1. ë°ì´í„° ë¡œë”©
        dataset_split = self.load_all_data()
        
        # 2. ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
        if not self.validate_data_integrity():
            raise ValueError("ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨")
        
        # 3. í›ˆë ¨ ì¦ê°• (ì˜µì…˜)
        if not skip_augmentation:
            augmented_train = self.augment_training_data()
            self._augmented_train_files = augmented_train
        else:
            self.logger.info("ì¦ê°• ê±´ë„ˆëœ€")
            self._augmented_train_files = dataset_split.train_files
        
        # 4. íŠ¹ì§• ì¶”ì¶œ
        features = self.extract_all_features()
        
        self.logger.info("ğŸ‰ ì™„ì „í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ ğŸ‰")
        
        return features
    
    def get_pipeline_summary(self) -> Dict[str, Any]:
        """
        íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ìš”ì•½ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
        --------
        Dict[str, Any]
            íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ìš”ì•½ ì •ë³´
        """
        if self._dataset_split is None:
            return {"status": "not_loaded"}
        
        summary = {
            "status": "loaded",
            "data_splits": {
                "train": self._dataset_split.total_train,
                "validation": self._dataset_split.total_validation,
                "test": self._dataset_split.total_test
            },
            "noise_files": len(self._dataset_split.noise_files),
            "augmentation_results": {}
        }
        
        # ì¦ê°• ê²°ê³¼ í¬í•¨
        if self._augmentation_results:
            for class_name, result in self._augmentation_results.items():
                summary["augmentation_results"][class_name] = {
                    "original_files": len(self._dataset_split.train_files.get(class_name, [])),
                    "augmented_files": result.total_created,
                    "noise_types_used": result.noise_types_used,
                    "snr_levels_used": result.snr_levels_used
                }
        
        return summary