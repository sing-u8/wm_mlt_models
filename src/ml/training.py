"""
ë¨¸ì‹ ëŸ¬ë‹ í›ˆë ¨ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ scikit-learnì„ ì‚¬ìš©í•˜ì—¬ SVMê³¼ Random Forest ë¶„ë¥˜ ëª¨ë¸ì„ 
í›ˆë ¨í•˜ê³  í‰ê°€í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import os
import pickle
import json
from dataclasses import dataclass, asdict
from datetime import datetime
from typing import Dict, Any, Optional, Tuple, List
import numpy as np
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import classification_report, confusion_matrix
import joblib

from ..utils.logger import LoggerMixin
from config import DEFAULT_CONFIG


@dataclass
class ModelConfig:
    """
    ëª¨ë¸ êµ¬ì„±ì„ ë‹´ëŠ” ë°ì´í„° í´ëž˜ìŠ¤.
    """
    model_type: str
    model_params: Dict[str, Any]
    param_grid: Dict[str, List[Any]]
    cv_folds: int = 5
    scoring: str = 'f1_macro'
    random_state: int = 42


@dataclass
class TrainingResult:
    """
    í›ˆë ¨ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ëž˜ìŠ¤.
    """
    model_type: str
    best_params: Dict[str, Any]
    best_score: float
    cv_scores: List[float]
    training_time: float
    n_features: int
    n_samples: int
    feature_importance: Optional[List[float]] = None  # Random Forestë§Œ


@dataclass
class ModelArtifact:
    """
    ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ë©”íƒ€ë°ì´í„°ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ëž˜ìŠ¤.
    """
    model_type: str
    model_path: str
    config_path: str
    created_at: str
    training_result: TrainingResult
    feature_extraction_config: Dict[str, Any]
    class_names: List[str]
    feature_names: List[str]
    model_version: str = "1.0"


class ModelTrainer(LoggerMixin):
    """
    ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ì„ ìœ„í•œ í´ëž˜ìŠ¤.
    
    design.mdì— ëª…ì‹œëœ ì¸í„°íŽ˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, config=None):
        """
        ëª¨ë¸ êµ¬ì„±ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        Parameters:
        -----------
        config : Config, optional
            êµ¬ì„± ê°ì²´. Noneì´ë©´ ê¸°ë³¸ êµ¬ì„±ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """
        self.config = config or DEFAULT_CONFIG
        
        # ëª¨ë¸ êµ¬ì„± ì •ì˜
        self.models = self._initialize_model_configs()
        
        # í›ˆë ¨ëœ ëª¨ë¸ ì €ìž¥
        self.trained_models = {}
        self.training_results = {}
        self.model_artifacts = {}
        
        self.logger.info(f"ModelTrainer ì´ˆê¸°í™”ë¨")
        self.logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {list(self.models.keys())}")
    
    def _initialize_model_configs(self) -> Dict[str, ModelConfig]:
        """
        design.md ëª…ì„¸ì— ë”°ë¥¸ ëª¨ë¸ êµ¬ì„±ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        Returns:
        --------
        Dict[str, ModelConfig]
            ëª¨ë¸ë³„ êµ¬ì„± ë”•ì…”ë„ˆë¦¬
        """
        model_configs = {}
        
        # SVM êµ¬ì„± (design.md ëª…ì„¸)
        svm_config = ModelConfig(
            model_type="svm",
            model_params={
                "kernel": "rbf",
                "random_state": 42,
                "probability": True  # predict_probaë¥¼ ìœ„í•´ í•„ìš”
            },
            param_grid={
                "C": [0.1, 1, 10, 100],
                "gamma": ["scale", "auto", 0.001, 0.01]
            }
        )
        model_configs["svm"] = svm_config
        
        # Random Forest êµ¬ì„± (design.md ëª…ì„¸)
        rf_config = ModelConfig(
            model_type="random_forest",
            model_params={
                "random_state": 42,
                "n_jobs": -1  # ë³‘ë ¬ ì²˜ë¦¬
            },
            param_grid={
                "n_estimators": [50, 100, 200],
                "max_depth": [None, 10, 20],
                "min_samples_split": [2, 5, 10]
            }
        )
        model_configs["random_forest"] = rf_config
        
        return model_configs
    
    def _create_model_instance(self, model_type: str, params: Dict[str, Any]):
        """
        ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Parameters:
        -----------
        model_type : str
            ëª¨ë¸ íƒ€ìž… ("svm" ë˜ëŠ” "random_forest")
        params : Dict[str, Any]
            ëª¨ë¸ íŒŒë¼ë¯¸í„°
            
        Returns:
        --------
        sklearn model instance
            ìƒì„±ëœ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
        """
        if model_type == "svm":
            return SVC(**params)
        elif model_type == "random_forest":
            return RandomForestClassifier(**params)
        else:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ìž…: {model_type}")
    
    def train_single_model(self, model_type: str, X_train: np.ndarray, 
                          y_train: np.ndarray, cv_folds: int = 5) -> TrainingResult:
        """
        ë‹¨ì¼ ëª¨ë¸ì„ êµì°¨ ê²€ì¦ìœ¼ë¡œ í›ˆë ¨í•©ë‹ˆë‹¤.
        
        Parameters:
        -----------
        model_type : str
            í›ˆë ¨í•  ëª¨ë¸ íƒ€ìž…
        X_train : np.ndarray
            í›ˆë ¨ íŠ¹ì§• ë°ì´í„°
        y_train : np.ndarray
            í›ˆë ¨ ë¼ë²¨ ë°ì´í„°
        cv_folds : int
            êµì°¨ ê²€ì¦ í´ë“œ ìˆ˜
            
        Returns:
        --------
        TrainingResult
            í›ˆë ¨ ê²°ê³¼
        """
        if model_type not in self.models:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ìž…: {model_type}")
        
        model_config = self.models[model_type]
        
        self.logger.info(f"=== {model_type.upper()} ëª¨ë¸ í›ˆë ¨ ì‹œìž‘ ===")
        self.logger.info(f"ë°ì´í„° í˜•íƒœ: {X_train.shape}")
        self.logger.info(f"í´ëž˜ìŠ¤ ë¶„í¬: {np.bincount(y_train)}")
        self.logger.info(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ: {model_config.param_grid}")
        
        start_time = datetime.now()
        
        # ê¸°ë³¸ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        base_model = self._create_model_instance(model_type, model_config.model_params)
        
        # ê³„ì¸µí™”ëœ K-Fold êµì°¨ ê²€ì¦ ì„¤ì •
        stratified_kfold = StratifiedKFold(
            n_splits=cv_folds, 
            shuffle=True, 
            random_state=42
        )
        
        # GridSearchCV ì„¤ì •
        grid_search = GridSearchCV(
            estimator=base_model,
            param_grid=model_config.param_grid,
            scoring=model_config.scoring,
            cv=stratified_kfold,
            n_jobs=-1,
            verbose=1,
            return_train_score=True
        )
        
        try:
            # ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹¤í–‰
            self.logger.info("GridSearchCV ì‹¤í–‰ ì¤‘...")
            grid_search.fit(X_train, y_train)
            
            training_time = (datetime.now() - start_time).total_seconds()
            
            # ê²°ê³¼ ìˆ˜ì§‘
            best_model = grid_search.best_estimator_
            cv_scores = grid_search.cv_results_['mean_test_score']
            
            # íŠ¹ì§• ì¤‘ìš”ë„ ì¶”ì¶œ (Random Forestë§Œ)
            feature_importance = None
            if model_type == "random_forest" and hasattr(best_model, 'feature_importances_'):
                feature_importance = best_model.feature_importances_.tolist()
            
            # í›ˆë ¨ ê²°ê³¼ ìƒì„±
            training_result = TrainingResult(
                model_type=model_type,
                best_params=grid_search.best_params_,
                best_score=grid_search.best_score_,
                cv_scores=cv_scores.tolist(),
                training_time=training_time,
                n_features=X_train.shape[1],
                n_samples=X_train.shape[0],
                feature_importance=feature_importance
            )
            
            # ëª¨ë¸ ì €ìž¥
            self.trained_models[model_type] = best_model
            self.training_results[model_type] = training_result
            
            self.logger.info(f"=== {model_type.upper()} ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ ===")
            self.logger.info(f"ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
            self.logger.info(f"ìµœì  CV ì ìˆ˜: {grid_search.best_score_:.4f}")
            self.logger.info(f"í›ˆë ¨ ì‹œê°„: {training_time:.2f}ì´ˆ")
            
            return training_result
            
        except Exception as e:
            self.logger.error(f"{model_type} ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {e}")
            raise
    
    def train_with_cv(self, X_train: np.ndarray, y_train: np.ndarray, 
                      cv_folds: int = 5) -> Dict[str, TrainingResult]:
        """
        êµì°¨ ê²€ì¦ìœ¼ë¡œ ëª¨ë“  ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤.
        
        design.mdì— ëª…ì‹œëœ ì¸í„°íŽ˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
        
        Parameters:
        -----------
        X_train : np.ndarray
            í›ˆë ¨ íŠ¹ì§• ë°ì´í„°
        y_train : np.ndarray  
            í›ˆë ¨ ë¼ë²¨ ë°ì´í„°
        cv_folds : int
            êµì°¨ ê²€ì¦ í´ë“œ ìˆ˜
            
        Returns:
        --------
        Dict[str, TrainingResult]
            ëª¨ë¸ë³„ í›ˆë ¨ ê²°ê³¼
        """
        self.logger.info("ðŸ¤– ëª¨ë“  ëª¨ë¸ êµì°¨ ê²€ì¦ í›ˆë ¨ ì‹œìž‘ ðŸ¤–")
        self.logger.info(f"ì´ ë°ì´í„°: {X_train.shape[0]}ê°œ ìƒ˜í”Œ, {X_train.shape[1]}ê°œ íŠ¹ì§•")
        self.logger.info(f"êµì°¨ ê²€ì¦: {cv_folds}-fold")
        
        all_results = {}
        
        # ê° ëª¨ë¸ë³„ë¡œ í›ˆë ¨ ìˆ˜í–‰
        for model_type in self.models.keys():
            try:
                result = self.train_single_model(model_type, X_train, y_train, cv_folds)
                all_results[model_type] = result
                
            except Exception as e:
                self.logger.error(f"ëª¨ë¸ {model_type} í›ˆë ¨ ì‹¤íŒ¨: {e}")
                continue
        
        # ê²°ê³¼ ìš”ì•½
        self.logger.info("ðŸ¤– ëª¨ë“  ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ ðŸ¤–")
        self.logger.info("ëª¨ë¸ë³„ ìµœê³  CV ì ìˆ˜:")
        
        for model_type, result in all_results.items():
            self.logger.info(f"  {model_type}: {result.best_score:.4f} "
                           f"(í›ˆë ¨ ì‹œê°„: {result.training_time:.1f}ì´ˆ)")
        
        return all_results
    
    def evaluate_single_model(self, model_type: str, X_test: np.ndarray, 
                             y_test: np.ndarray) -> Dict[str, float]:
        """
        ë‹¨ì¼ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ í‰ê°€í•©ë‹ˆë‹¤.
        
        Parameters:
        -----------
        model_type : str
            í‰ê°€í•  ëª¨ë¸ íƒ€ìž…
        X_test : np.ndarray
            í…ŒìŠ¤íŠ¸ íŠ¹ì§• ë°ì´í„°
        y_test : np.ndarray
            í…ŒìŠ¤íŠ¸ ë¼ë²¨ ë°ì´í„°
            
        Returns:
        --------
        Dict[str, float]
            í‰ê°€ ë©”íŠ¸ë¦­ë“¤
        """
        if model_type not in self.trained_models:
            raise ValueError(f"ëª¨ë¸ {model_type}ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        model = self.trained_models[model_type]
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        y_pred = model.predict(X_test)
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision_macro': precision_score(y_test, y_pred, average='macro', zero_division=0),
            'recall_macro': recall_score(y_test, y_pred, average='macro', zero_division=0),
            'f1_macro': f1_score(y_test, y_pred, average='macro', zero_division=0),
            'precision_weighted': precision_score(y_test, y_pred, average='weighted', zero_division=0),
            'recall_weighted': recall_score(y_test, y_pred, average='weighted', zero_division=0),
            'f1_weighted': f1_score(y_test, y_pred, average='weighted', zero_division=0)
        }
        
        # í´ëž˜ìŠ¤ë³„ ë©”íŠ¸ë¦­
        class_report = classification_report(y_test, y_pred, 
                                           target_names=self.config.class_names,
                                           output_dict=True, zero_division=0)
        
        # í´ëž˜ìŠ¤ë³„ ë©”íŠ¸ë¦­ì„ ì „ì²´ ë©”íŠ¸ë¦­ì— ì¶”ê°€
        for class_name in self.config.class_names:
            if class_name in class_report:
                metrics[f'{class_name}_precision'] = class_report[class_name]['precision']
                metrics[f'{class_name}_recall'] = class_report[class_name]['recall']
                metrics[f'{class_name}_f1'] = class_report[class_name]['f1-score']
        
        self.logger.info(f"{model_type} í‰ê°€ ì™„ë£Œ:")
        self.logger.info(f"  ì •í™•ë„: {metrics['accuracy']:.4f}")
        self.logger.info(f"  F1 (macro): {metrics['f1_macro']:.4f}")
        self.logger.info(f"  F1 (weighted): {metrics['f1_weighted']:.4f}")
        
        return metrics
    
    def evaluate_final(self, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, Dict[str, float]]:
        """
        í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ í›ˆë ¨ëœ ëª¨ë“  ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤.
        
        design.mdì— ëª…ì‹œëœ ì¸í„°íŽ˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
        
        Parameters:
        -----------
        X_test : np.ndarray
            í…ŒìŠ¤íŠ¸ íŠ¹ì§• ë°ì´í„°
        y_test : np.ndarray
            í…ŒìŠ¤íŠ¸ ë¼ë²¨ ë°ì´í„°
            
        Returns:
        --------
        Dict[str, Dict[str, float]]
            ëª¨ë¸ë³„ í‰ê°€ ë©”íŠ¸ë¦­ë“¤
        """
        self.logger.info("ðŸ“Š ìµœì¢… ëª¨ë¸ í‰ê°€ ì‹œìž‘ ðŸ“Š")
        self.logger.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape[0]}ê°œ ìƒ˜í”Œ, {X_test.shape[1]}ê°œ íŠ¹ì§•")
        
        all_metrics = {}
        
        for model_type in self.trained_models.keys():
            try:
                metrics = self.evaluate_single_model(model_type, X_test, y_test)
                all_metrics[model_type] = metrics
                
            except Exception as e:
                self.logger.error(f"ëª¨ë¸ {model_type} í‰ê°€ ì‹¤íŒ¨: {e}")
                continue
        
        # ëª¨ë¸ ë¹„êµ (requirements.md ìš”êµ¬ì‚¬í•­ 3.5)
        if len(all_metrics) >= 2:
            self.logger.info("ðŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ðŸ“Š")
            
            comparison_metrics = ['accuracy', 'f1_macro', 'f1_weighted']
            for metric in comparison_metrics:
                self.logger.info(f"{metric.upper()}:")
                for model_type, metrics in all_metrics.items():
                    self.logger.info(f"  {model_type}: {metrics[metric]:.4f}")
        
        self.logger.info("ðŸ“Š ìµœì¢… ëª¨ë¸ í‰ê°€ ì™„ë£Œ ðŸ“Š")
        
        return all_metrics
    
    def save_models(self, output_dir: str = None) -> Dict[str, str]:
        """
        í›ˆë ¨ëœ ëª¨ë¸ì„ pickle í˜•ì‹ìœ¼ë¡œ ì €ìž¥í•©ë‹ˆë‹¤.
        
        design.mdì— ëª…ì‹œëœ ì¸í„°íŽ˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
        
        Parameters:
        -----------
        output_dir : str, optional
            ì¶œë ¥ ë””ë ‰í† ë¦¬. Noneì´ë©´ ê¸°ë³¸ ëª¨ë¸ ë””ë ‰í† ë¦¬ ì‚¬ìš©
            
        Returns:
        --------
        Dict[str, str]
            ëª¨ë¸ë³„ ì €ìž¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        if output_dir is None:
            output_dir = os.path.join(self.config.model_output_dir, "pickle")
        
        os.makedirs(output_dir, exist_ok=True)
        
        self.logger.info(f"ðŸ’¾ ëª¨ë¸ ì €ìž¥ ì‹œìž‘: {output_dir}")
        
        saved_paths = {}
        
        for model_type, model in self.trained_models.items():
            try:
                # ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
                model_filename = f"{model_type}_model.pkl"
                model_path = os.path.join(output_dir, model_filename)
                
                # ëª¨ë¸ ì €ìž¥
                joblib.dump(model, model_path)
                saved_paths[model_type] = model_path
                
                # ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ìƒì„± (requirements.md ìš”êµ¬ì‚¬í•­ 7.2)
                artifact = ModelArtifact(
                    model_type=model_type,
                    model_path=model_path,
                    config_path=os.path.join(output_dir, f"{model_type}_config.json"),
                    created_at=datetime.now().isoformat(),
                    training_result=self.training_results[model_type],
                    feature_extraction_config={
                        'sample_rate': self.config.sample_rate,
                        'hop_length': self.config.hop_length,
                        'n_mfcc': self.config.n_mfcc,
                        'n_chroma': self.config.n_chroma
                    },
                    class_names=self.config.class_names,
                    feature_names=[f'feature_{i}' for i in range(30)]  # 30ì°¨ì› íŠ¹ì§•
                )
                
                # ì•„í‹°íŒ©íŠ¸ ë©”íƒ€ë°ì´í„° ì €ìž¥
                with open(artifact.config_path, 'w', encoding='utf-8') as f:
                    json.dump(asdict(artifact), f, indent=2, ensure_ascii=False)
                
                self.model_artifacts[model_type] = artifact
                
                self.logger.info(f"  {model_type}: ì €ìž¥ ì™„ë£Œ ({os.path.basename(model_path)})")
                
            except Exception as e:
                self.logger.error(f"ëª¨ë¸ {model_type} ì €ìž¥ ì‹¤íŒ¨: {e}")
                continue
        
        self.logger.info(f"ðŸ’¾ ëª¨ë¸ ì €ìž¥ ì™„ë£Œ: {len(saved_paths)}ê°œ ëª¨ë¸")
        
        return saved_paths
    
    def load_model(self, model_path: str) -> Any:
        """
        ì €ìž¥ëœ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        
        Parameters:
        -----------
        model_path : str
            ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
            
        Returns:
        --------
        Any
            ë¡œë“œëœ ëª¨ë¸ ê°ì²´
        """
        try:
            model = joblib.load(model_path)
            self.logger.info(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
            return model
            
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_path}: {e}")
            raise
    
    def get_feature_importance(self, model_type: str) -> Optional[List[Tuple[str, float]]]:
        """
        ëª¨ë¸ì˜ íŠ¹ì§• ì¤‘ìš”ë„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤ (Random Forestë§Œ ì§€ì›).
        
        Parameters:
        -----------
        model_type : str
            ëª¨ë¸ íƒ€ìž…
            
        Returns:
        --------
        Optional[List[Tuple[str, float]]]
            (íŠ¹ì§•ëª…, ì¤‘ìš”ë„) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        if model_type not in self.training_results:
            return None
        
        result = self.training_results[model_type]
        if result.feature_importance is None:
            return None
        
        # íŠ¹ì§•ëª…ê³¼ ì¤‘ìš”ë„ ë§¤í•‘
        feature_names = [f'feature_{i}' for i in range(len(result.feature_importance))]
        importance_pairs = list(zip(feature_names, result.feature_importance))
        
        # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        importance_pairs.sort(key=lambda x: x[1], reverse=True)
        
        return importance_pairs
    
    def get_training_summary(self) -> Dict[str, Any]:
        """
        í›ˆë ¨ ìš”ì•½ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
        --------
        Dict[str, Any]
            í›ˆë ¨ ìš”ì•½ ì •ë³´
        """
        summary = {
            'total_models': len(self.trained_models),
            'trained_models': list(self.trained_models.keys()),
            'training_results': {}
        }
        
        for model_type, result in self.training_results.items():
            summary['training_results'][model_type] = {
                'best_score': result.best_score,
                'best_params': result.best_params,
                'training_time': result.training_time,
                'n_features': result.n_features,
                'n_samples': result.n_samples
            }
        
        return summary