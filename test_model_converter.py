#!/usr/bin/env python3
"""
ëª¨ë¸ ë³€í™˜ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ModelConverter í´ë˜ìŠ¤ ë° Core ML ë³€í™˜ ê¸°ëŠ¥ ê²€ì¦
"""

import sys
import os
import tempfile
import shutil
import numpy as np
from pathlib import Path
import json
sys.path.append('.')

from src.ml.model_converter import ModelConverter, ConversionResult, CoreMLModelInfo
from src.ml.training import ModelTrainer
from src.utils.logger import setup_logger
from config import DEFAULT_CONFIG

try:
    import coremltools as ct
    COREML_AVAILABLE = True
except ImportError:
    COREML_AVAILABLE = False

def create_test_model_and_data():
    """í…ŒìŠ¤íŠ¸ìš© ëª¨ë¸ê³¼ ë°ì´í„° ìƒì„±"""
    np.random.seed(42)
    
    # 30ì°¨ì› íŠ¹ì§• ë²¡í„° (design.md ëª…ì„¸)
    n_samples = 150
    n_features = 30
    n_classes = 3
    
    # ê° í´ë˜ìŠ¤ë³„ë¡œ 50ê°œ ìƒ˜í”Œ ìƒì„±
    X = []
    y = []
    
    for class_idx in range(n_classes):
        # í´ë˜ìŠ¤ë³„ë¡œ ë‹¤ë¥¸ íŠ¹ì„±ì„ ê°€ì§„ ë°ì´í„° ìƒì„±
        class_center = np.random.randn(n_features) * 2
        class_samples = np.random.randn(50, n_features) + class_center
        
        X.append(class_samples)
        y.extend([class_idx] * 50)
    
    X = np.vstack(X)
    y = np.array(y)
    
    # ë°ì´í„° ì…”í”Œ
    indices = np.random.permutation(len(X))
    X = X[indices]
    y = y[indices]
    
    # train/test ë¶„í• 
    train_size = int(0.8 * len(X))
    
    X_train = X[:train_size]
    y_train = y[:train_size]
    X_test = X[train_size:]
    y_test = y[train_size:]
    
    # ê°„ë‹¨í•œ ëª¨ë¸ í›ˆë ¨
    trainer = ModelTrainer()
    
    # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì¶•ì†Œ
    trainer.models["svm"].param_grid = {"C": [1], "gamma": ["scale"]}
    trainer.models["random_forest"].param_grid = {"n_estimators": [50], "max_depth": [10]}
    
    # ëª¨ë¸ í›ˆë ¨
    all_results = trainer.train_with_cv(X_train, y_train, cv_folds=3)
    
    # í›ˆë ¨ëœ ëª¨ë¸ ë°˜í™˜
    models = {
        "svm": trainer.trained_models["svm"],
        "random_forest": trainer.trained_models["random_forest"]
    }
    
    return models, X_test, y_test

def test_model_converter_initialization():
    """ModelConverter ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
    
    logger = setup_logger("converter_init_test", "INFO")
    logger.info("=== ModelConverter ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    try:
        # ê¸°ë³¸ êµ¬ì„±ìœ¼ë¡œ ì´ˆê¸°í™”
        converter = ModelConverter()
        
        # êµ¬ì„± í™•ì¸
        if converter.config is not None:
            logger.info("âœ… êµ¬ì„± ê°ì²´ ë¡œë“œë¨")
        else:
            logger.error("âŒ êµ¬ì„± ê°ì²´ ë¡œë“œ ì‹¤íŒ¨")
            return False
        
        # Core ML ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸
        logger.info(f"Core ML ì§€ì›: {COREML_AVAILABLE}")
        
        # ë³€í™˜ ê²°ê³¼ ì €ì¥ì†Œ ì´ˆê¸°í™” í™•ì¸
        if hasattr(converter, 'conversion_results'):
            logger.info("âœ… ë³€í™˜ ê²°ê³¼ ì €ì¥ì†Œ ì´ˆê¸°í™”ë¨")
        else:
            logger.error("âŒ ë³€í™˜ ê²°ê³¼ ì €ì¥ì†Œ ì´ˆê¸°í™” ì‹¤íŒ¨")
            return False
        
        logger.info("âœ… ModelConverter ì´ˆê¸°í™” ì„±ê³µ")
        return True
        
    except Exception as e:
        logger.error(f"ModelConverter ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False
    
    finally:
        logger.info("=== ModelConverter ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===\n")

def test_pickle_model_saving():
    """Pickle ëª¨ë¸ ì €ì¥ í…ŒìŠ¤íŠ¸"""
    
    logger = setup_logger("pickle_save_test", "INFO")
    logger.info("=== Pickle ëª¨ë¸ ì €ì¥ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    try:
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë° ëª¨ë¸ ìƒì„±
        models, X_test, y_test = create_test_model_and_data()
        logger.info(f"í…ŒìŠ¤íŠ¸ ëª¨ë¸ ìƒì„± ì™„ë£Œ")
        
        # ModelConverter ì´ˆê¸°í™”
        converter = ModelConverter()
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ì— ëª¨ë¸ ì €ì¥
        with tempfile.TemporaryDirectory() as temp_dir:
            logger.info(f"ì„ì‹œ ë””ë ‰í† ë¦¬ì— ëª¨ë¸ ì €ì¥: {temp_dir}")
            
            # SVM ëª¨ë¸ ì €ì¥
            model_metadata = {
                'model_type': 'svm',
                'feature_count': 30,
                'class_names': DEFAULT_CONFIG.class_names
            }
            
            svm_path = converter.save_pickle_model(
                models["svm"], "svm", model_metadata, temp_dir
            )
            
            # ì €ì¥ëœ íŒŒì¼ í™•ì¸
            if not os.path.exists(svm_path):
                logger.error(f"âŒ SVM ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {svm_path}")
                return False
            
            file_size = os.path.getsize(svm_path)
            if file_size < 100:  # 100 bytes ë¯¸ë§Œì´ë©´ ì˜¤ë¥˜ë¡œ ê°„ì£¼
                logger.error(f"âŒ SVM ëª¨ë¸ íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìŒ: {file_size} bytes")
                return False
            
            logger.info(f"âœ… SVM ëª¨ë¸ ì €ì¥ ì„±ê³µ: {file_size} bytes")
            
            # ë©”íƒ€ë°ì´í„° íŒŒì¼ í™•ì¸
            metadata_path = os.path.join(temp_dir, "svm_metadata.json")
            if os.path.exists(metadata_path):
                with open(metadata_path, 'r') as f:
                    saved_metadata = json.load(f)
                
                if 'model_type' in saved_metadata and saved_metadata['model_type'] == 'svm':
                    logger.info("âœ… ë©”íƒ€ë°ì´í„° ì €ì¥ ë° ë¡œë“œ ì„±ê³µ")
                else:
                    logger.error("âŒ ë©”íƒ€ë°ì´í„° ë‚´ìš© ì˜¤ë¥˜")
                    return False
            
            # ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸
            loaded_model = converter.load_pickle_model(svm_path)
            
            # ë¡œë”©ëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
            predictions = loaded_model.predict(X_test[:5])  # ì²˜ìŒ 5ê°œ ìƒ˜í”Œë§Œ
            
            if len(predictions) == 5:
                logger.info("âœ… ëª¨ë¸ ë¡œë”© ë° ì˜ˆì¸¡ ì„±ê³µ")
            else:
                logger.error("âŒ ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨")
                return False
        
        logger.info("âœ… Pickle ëª¨ë¸ ì €ì¥ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        return True
        
    except Exception as e:
        logger.error(f"Pickle ëª¨ë¸ ì €ì¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        logger.info("=== Pickle ëª¨ë¸ ì €ì¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===\n")

def test_coreml_conversion():
    """Core ML ë³€í™˜ í…ŒìŠ¤íŠ¸"""
    
    logger = setup_logger("coreml_conversion_test", "INFO")
    logger.info("=== Core ML ë³€í™˜ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    if not COREML_AVAILABLE:
        logger.warning("âš ï¸ coremltoolsê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ Core ML í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return True  # ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²ƒì€ ì˜¤ë¥˜ê°€ ì•„ë‹˜
    
    try:
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë° ëª¨ë¸ ìƒì„±
        models, X_test, y_test = create_test_model_and_data()
        
        # ModelConverter ì´ˆê¸°í™”
        converter = ModelConverter()
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ì— ë³€í™˜
        with tempfile.TemporaryDirectory() as temp_dir:
            logger.info(f"ì„ì‹œ ë””ë ‰í† ë¦¬ì— Core ML ëª¨ë¸ ì €ì¥: {temp_dir}")
            
            # Random Forest ëª¨ë¸ ë³€í™˜ (SVMë³´ë‹¤ ì•ˆì •ì )
            input_description = {
                'name': 'audio_features',
                'shape': (30,),
                'description': 'Audio feature vector extracted from watermelon sound'
            }
            
            coreml_path = converter.convert_to_coreml(
                models["random_forest"], "random_forest", input_description, temp_dir
            )
            
            # ë³€í™˜ëœ íŒŒì¼ í™•ì¸
            if not os.path.exists(coreml_path):
                logger.error(f"âŒ Core ML ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {coreml_path}")
                return False
            
            file_size = os.path.getsize(coreml_path)
            if file_size < 1000:  # 1KB ë¯¸ë§Œì´ë©´ ì˜¤ë¥˜ë¡œ ê°„ì£¼
                logger.error(f"âŒ Core ML ëª¨ë¸ íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìŒ: {file_size} bytes")
                return False
            
            logger.info(f"âœ… Core ML ë³€í™˜ ì„±ê³µ: {file_size} bytes")
            
            # ë©”íƒ€ë°ì´í„° íŒŒì¼ í™•ì¸
            metadata_path = os.path.join(temp_dir, "random_forest_coreml_info.json")
            if os.path.exists(metadata_path):
                with open(metadata_path, 'r') as f:
                    coreml_info = json.load(f)
                
                required_keys = ['model_path', 'input_name', 'input_shape', 'class_labels']
                for key in required_keys:
                    if key not in coreml_info:
                        logger.error(f"âŒ Core ML ë©”íƒ€ë°ì´í„°ì—ì„œ {key} ëˆ„ë½")
                        return False
                
                logger.info("âœ… Core ML ë©”íƒ€ë°ì´í„° ì €ì¥ ì„±ê³µ")
            
            # Core ML ëª¨ë¸ ë¡œë”© ë° ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
            try:
                coreml_model = ct.models.MLModel(coreml_path)
                
                # ë‹¨ì¼ ìƒ˜í”Œë¡œ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
                test_sample = X_test[0:1].astype(np.float32)
                input_dict = {'audio_features': test_sample}
                
                result = coreml_model.predict(input_dict)
                
                if 'watermelon_class' in result:
                    logger.info(f"âœ… Core ML ëª¨ë¸ ì˜ˆì¸¡ ì„±ê³µ: {result['watermelon_class']}")
                else:
                    logger.error("âŒ Core ML ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ì— í´ë˜ìŠ¤ ì •ë³´ ì—†ìŒ")
                    return False
                
            except Exception as e:
                logger.error(f"âŒ Core ML ëª¨ë¸ ë¡œë”©/ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                return False
        
        logger.info("âœ… Core ML ë³€í™˜ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        return True
        
    except Exception as e:
        logger.error(f"Core ML ë³€í™˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        logger.info("=== Core ML ë³€í™˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===\n")

def test_model_validation():
    """ëª¨ë¸ ë³€í™˜ ê²€ì¦ í…ŒìŠ¤íŠ¸"""
    
    logger = setup_logger("model_validation_test", "INFO")
    logger.info("=== ëª¨ë¸ ë³€í™˜ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    if not COREML_AVAILABLE:
        logger.warning("âš ï¸ coremltoolsê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ê²€ì¦ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return True
    
    try:
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë° ëª¨ë¸ ìƒì„±
        models, X_test, y_test = create_test_model_and_data()
        
        # ModelConverter ì´ˆê¸°í™”
        converter = ModelConverter()
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ì— ë³€í™˜ ë° ê²€ì¦
        with tempfile.TemporaryDirectory() as temp_dir:
            logger.info(f"ì„ì‹œ ë””ë ‰í† ë¦¬ì—ì„œ ë³€í™˜ ë° ê²€ì¦: {temp_dir}")
            
            # Random Forest ëª¨ë¸ ë³€í™˜
            coreml_path = converter.convert_to_coreml(
                models["random_forest"], "random_forest", output_dir=temp_dir
            )
            
            # ê²€ì¦ ìˆ˜í–‰
            validation_result = converter.validate_model_conversion(
                models["random_forest"], coreml_path, test_samples=10
            )
            
            # ê²€ì¦ ê²°ê³¼ í™•ì¸
            if 'error' in validation_result:
                logger.error(f"âŒ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {validation_result['error']}")
                return False
            
            required_keys = ['test_samples', 'class_prediction_accuracy', 'predictions_match']
            for key in required_keys:
                if key not in validation_result:
                    logger.error(f"âŒ ê²€ì¦ ê²°ê³¼ì—ì„œ {key} ëˆ„ë½")
                    return False
            
            accuracy = validation_result['class_prediction_accuracy']
            logger.info(f"âœ… ê²€ì¦ ì™„ë£Œ:")
            logger.info(f"  í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {validation_result['test_samples']}ê°œ")
            logger.info(f"  í´ë˜ìŠ¤ ì˜ˆì¸¡ ì •í™•ë„: {accuracy:.1%}")
            logger.info(f"  ëª¨ë“  ì˜ˆì¸¡ ì¼ì¹˜: {validation_result['predictions_match']}")
            
            # í™•ë¥  ê²€ì¦ ì •ë³´ (ìˆëŠ” ê²½ìš°)
            if 'probability_validation' in validation_result:
                logger.info(f"  í™•ë¥  ì˜ˆì¸¡ ìµœëŒ€ ì°¨ì´: {validation_result['max_probability_difference']:.6f}")
                logger.info(f"  í™•ë¥  ì˜ˆì¸¡ í‰ê·  ì°¨ì´: {validation_result['mean_probability_difference']:.6f}")
            
            # ê²€ì¦ í†µê³¼ ê¸°ì¤€ (80% ì´ìƒ ì¼ì¹˜)
            if accuracy >= 0.8:
                logger.info("âœ… ê²€ì¦ í†µê³¼ (80% ì´ìƒ ì¼ì¹˜)")
            else:
                logger.warning(f"âš ï¸ ê²€ì¦ ì£¼ì˜ (ì •í™•ë„ {accuracy:.1%} < 80%)")
        
        logger.info("âœ… ëª¨ë¸ ë³€í™˜ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        return True
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ ë³€í™˜ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        logger.info("=== ëª¨ë¸ ë³€í™˜ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===\n")

def test_conversion_with_validation():
    """ê²€ì¦ì„ í¬í•¨í•œ ì „ì²´ ë³€í™˜ í”„ë¡œì„¸ìŠ¤ í…ŒìŠ¤íŠ¸"""
    
    logger = setup_logger("full_conversion_test", "INFO")
    logger.info("=== ê²€ì¦ í¬í•¨ ì „ì²´ ë³€í™˜ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    if not COREML_AVAILABLE:
        logger.warning("âš ï¸ coremltoolsê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì „ì²´ ë³€í™˜ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return True
    
    try:
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë° ëª¨ë¸ ìƒì„±
        models, X_test, y_test = create_test_model_and_data()
        
        # ModelConverter ì´ˆê¸°í™”
        converter = ModelConverter()
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ì—ì„œ ì „ì²´ ë³€í™˜ í”„ë¡œì„¸ìŠ¤
        with tempfile.TemporaryDirectory() as temp_dir:
            logger.info(f"ì„ì‹œ ë””ë ‰í† ë¦¬ì—ì„œ ì „ì²´ ë³€í™˜ í”„ë¡œì„¸ìŠ¤: {temp_dir}")
            
            # ê²€ì¦ì„ í¬í•¨í•œ ë³€í™˜
            conversion_result = converter.convert_model_with_validation(
                models["random_forest"], "random_forest", 
                output_dir=temp_dir, validate=True
            )
            
            # ë³€í™˜ ê²°ê³¼ ê²€ì¦
            if not isinstance(conversion_result, ConversionResult):
                logger.error("âŒ ë³€í™˜ ê²°ê³¼ í˜•ì‹ ì˜¤ë¥˜")
                return False
            
            # í•„ìˆ˜ ì†ì„± í™•ì¸
            required_attributes = [
                'model_name', 'original_format', 'target_format',
                'converted_path', 'conversion_time', 'validation_passed'
            ]
            
            for attr in required_attributes:
                if not hasattr(conversion_result, attr):
                    logger.error(f"âŒ ë³€í™˜ ê²°ê³¼ì—ì„œ {attr} ëˆ„ë½")
                    return False
            
            logger.info(f"âœ… ì „ì²´ ë³€í™˜ ì™„ë£Œ:")
            logger.info(f"  ëª¨ë¸ëª…: {conversion_result.model_name}")
            logger.info(f"  ë³€í™˜ í˜•ì‹: {conversion_result.original_format} â†’ {conversion_result.target_format}")
            logger.info(f"  ë³€í™˜ ì‹œê°„: {conversion_result.conversion_time:.3f}ì´ˆ")
            logger.info(f"  íŒŒì¼ í¬ê¸°: {conversion_result.file_size_bytes:,} bytes")
            logger.info(f"  ê²€ì¦ í†µê³¼: {conversion_result.validation_passed}")
            
            # ë³€í™˜ëœ íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not os.path.exists(conversion_result.converted_path):
                logger.error(f"âŒ ë³€í™˜ëœ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {conversion_result.converted_path}")
                return False
            
            # ê²€ì¦ ì„¸ë¶€ ì •ë³´ í™•ì¸
            if 'validation_details' in conversion_result.__dict__:
                validation_details = conversion_result.validation_details
                if 'class_prediction_accuracy' in validation_details:
                    logger.info(f"  ê²€ì¦ ì •í™•ë„: {validation_details['class_prediction_accuracy']:.1%}")
        
        logger.info("âœ… ê²€ì¦ í¬í•¨ ì „ì²´ ë³€í™˜ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        return True
        
    except Exception as e:
        logger.error(f"ê²€ì¦ í¬í•¨ ì „ì²´ ë³€í™˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        logger.info("=== ê²€ì¦ í¬í•¨ ì „ì²´ ë³€í™˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===\n")

def test_metadata_creation():
    """ë©”íƒ€ë°ì´í„° ìƒì„± í…ŒìŠ¤íŠ¸"""
    
    logger = setup_logger("metadata_test", "INFO")
    logger.info("=== ë©”íƒ€ë°ì´í„° ìƒì„± í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    try:
        # ModelConverter ì´ˆê¸°í™”
        converter = ModelConverter()
        
        # í…ŒìŠ¤íŠ¸ ëª¨ë¸ ì •ë³´
        model_info = {
            'model_type': 'random_forest',
            'n_estimators': 50,
            'max_depth': 10
        }
        
        training_info = {
            'training_samples': 120,
            'validation_samples': 30,
            'cv_folds': 5,
            'best_score': 0.85
        }
        
        performance_metrics = {
            'accuracy': 0.87,
            'f1_macro': 0.85,
            'precision_macro': 0.86,
            'recall_macro': 0.84
        }
        
        # ë©”íƒ€ë°ì´í„° ìƒì„±
        metadata = converter.create_model_metadata(
            model_info, training_info, performance_metrics
        )
        
        # ë©”íƒ€ë°ì´í„° ê²€ì¦
        required_sections = ['model_info', 'feature_extraction', 'class_information']
        for section in required_sections:
            if section not in metadata:
                logger.error(f"âŒ ë©”íƒ€ë°ì´í„°ì—ì„œ {section} ì„¹ì…˜ ëˆ„ë½")
                return False
        
        # íŠ¹ì§• ì¶”ì¶œ ì •ë³´ í™•ì¸
        feature_extraction = metadata['feature_extraction']
        if feature_extraction['feature_vector_size'] != 30:
            logger.error(f"âŒ íŠ¹ì§• ë²¡í„° í¬ê¸° ì˜¤ë¥˜: {feature_extraction['feature_vector_size']}")
            return False
        
        # í´ë˜ìŠ¤ ì •ë³´ í™•ì¸
        class_info = metadata['class_information']
        if len(class_info['class_names']) != 3:
            logger.error(f"âŒ í´ë˜ìŠ¤ ìˆ˜ ì˜¤ë¥˜: {len(class_info['class_names'])}")
            return False
        
        logger.info(f"âœ… ë©”íƒ€ë°ì´í„° ìƒì„± ì„±ê³µ:")
        logger.info(f"  ëª¨ë¸ íƒ€ì…: {metadata['model_info']['model_type']}")
        logger.info(f"  íŠ¹ì§• ë²¡í„° í¬ê¸°: {metadata['feature_extraction']['feature_vector_size']}")
        logger.info(f"  í´ë˜ìŠ¤ ìˆ˜: {metadata['class_information']['n_classes']}")
        logger.info(f"  ë²„ì „: {metadata['version']}")
        
        # ì„ íƒì  ì •ë³´ í™•ì¸
        if 'training_info' in metadata:
            logger.info(f"  í›ˆë ¨ ìƒ˜í”Œ: {metadata['training_info']['training_samples']}")
        
        if 'performance_metrics' in metadata:
            logger.info(f"  ì •í™•ë„: {metadata['performance_metrics']['accuracy']:.3f}")
        
        logger.info("âœ… ë©”íƒ€ë°ì´í„° ìƒì„± í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        return True
        
    except Exception as e:
        logger.error(f"ë©”íƒ€ë°ì´í„° ìƒì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        logger.info("=== ë©”íƒ€ë°ì´í„° ìƒì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===\n")

def test_conversion_summary():
    """ë³€í™˜ ìš”ì•½ ì •ë³´ í…ŒìŠ¤íŠ¸"""
    
    logger = setup_logger("conversion_summary_test", "INFO")
    logger.info("=== ë³€í™˜ ìš”ì•½ ì •ë³´ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    if not COREML_AVAILABLE:
        logger.warning("âš ï¸ coremltoolsê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ë³€í™˜ ìš”ì•½ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return True
    
    try:
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë° ëª¨ë¸ ìƒì„±
        models, X_test, y_test = create_test_model_and_data()
        
        # ModelConverter ì´ˆê¸°í™”
        converter = ModelConverter()
        
        # ë¹ˆ ìƒíƒœì—ì„œ ìš”ì•½ í™•ì¸
        empty_summary = converter.get_conversion_summary()
        if empty_summary['total_conversions'] != 0:
            logger.error("âŒ ë¹ˆ ìƒíƒœ ìš”ì•½ ì˜¤ë¥˜")
            return False
        
        logger.info("âœ… ë¹ˆ ìƒíƒœ ìš”ì•½ ì •ë³´ ì˜¬ë°”ë¦„")
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ì—ì„œ ë³€í™˜ ìˆ˜í–‰
        with tempfile.TemporaryDirectory() as temp_dir:
            # ë‘ ëª¨ë¸ ë³€í™˜
            for model_name, model in models.items():
                try:
                    conversion_result = converter.convert_model_with_validation(
                        model, model_name, output_dir=temp_dir, validate=True
                    )
                    logger.info(f"âœ… {model_name} ë³€í™˜ ì™„ë£Œ")
                except Exception as e:
                    logger.warning(f"âš ï¸ {model_name} ë³€í™˜ ì‹¤íŒ¨: {e}")
                    continue
        
        # ë³€í™˜ ìš”ì•½ ìƒì„±
        summary = converter.get_conversion_summary()
        
        # ìš”ì•½ ì •ë³´ ê²€ì¦
        required_keys = [
            'total_conversions', 'successful_conversions', 'success_rate',
            'total_conversion_time', 'average_conversion_time',
            'total_file_size_bytes', 'average_file_size_bytes'
        ]
        
        for key in required_keys:
            if key not in summary:
                logger.error(f"âŒ ë³€í™˜ ìš”ì•½ì—ì„œ {key} ëˆ„ë½")
                return False
        
        logger.info(f"âœ… ë³€í™˜ ìš”ì•½ ì •ë³´:")
        logger.info(f"  ì´ ë³€í™˜ ìˆ˜: {summary['total_conversions']}")
        logger.info(f"  ì„±ê³µ ë³€í™˜ ìˆ˜: {summary['successful_conversions']}")
        logger.info(f"  ì„±ê³µë¥ : {summary['success_rate']:.1%}")
        logger.info(f"  ì´ ë³€í™˜ ì‹œê°„: {summary['total_conversion_time']:.3f}ì´ˆ")
        
        if summary['total_conversions'] > 0:
            logger.info(f"  í‰ê·  ë³€í™˜ ì‹œê°„: {summary['average_conversion_time']:.3f}ì´ˆ")
            logger.info(f"  ì´ íŒŒì¼ í¬ê¸°: {summary['total_file_size_bytes']:,} bytes")
            logger.info(f"  í‰ê·  íŒŒì¼ í¬ê¸°: {summary['average_file_size_bytes']:,.0f} bytes")
        
        logger.info("âœ… ë³€í™˜ ìš”ì•½ ì •ë³´ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        return True
        
    except Exception as e:
        logger.error(f"ë³€í™˜ ìš”ì•½ ì •ë³´ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        logger.info("=== ë³€í™˜ ìš”ì•½ ì •ë³´ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===\n")

if __name__ == "__main__":
    logger = setup_logger("main_converter_test", "INFO")
    logger.info("ğŸ”„ ëª¨ë¸ ë³€í™˜ ëª¨ë“ˆ ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘ ğŸ”„")
    
    test_results = []
    
    # ê°œë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_results.append(("ModelConverter ì´ˆê¸°í™”", test_model_converter_initialization()))
    test_results.append(("Pickle ëª¨ë¸ ì €ì¥", test_pickle_model_saving()))
    test_results.append(("Core ML ë³€í™˜", test_coreml_conversion()))
    test_results.append(("ëª¨ë¸ ë³€í™˜ ê²€ì¦", test_model_validation()))
    test_results.append(("ê²€ì¦ í¬í•¨ ì „ì²´ ë³€í™˜", test_conversion_with_validation()))
    test_results.append(("ë©”íƒ€ë°ì´í„° ìƒì„±", test_metadata_creation()))
    test_results.append(("ë³€í™˜ ìš”ì•½ ì •ë³´", test_conversion_summary()))
    
    # ê²°ê³¼ ìš”ì•½
    logger.info("=" * 60)
    logger.info("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    
    passed = 0
    total = len(test_results)
    
    for test_name, result in test_results:
        status = "âœ… ì„±ê³µ" if result else "âŒ ì‹¤íŒ¨"
        logger.info(f"  {test_name}: {status}")
        if result:
            passed += 1
    
    logger.info("=" * 60)
    success_rate = passed / total * 100
    logger.info(f"ì „ì²´ í…ŒìŠ¤íŠ¸ ê²°ê³¼: {passed}/{total} í†µê³¼ ({success_rate:.1f}%)")
    
    if passed == total:
        logger.info("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        logger.info("âœ… ModelConverter í´ë˜ìŠ¤ê°€ ì˜¬ë°”ë¥´ê²Œ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤.")
        logger.info("âœ… Pickle ëª¨ë¸ ì €ì¥/ë¡œë”© ê¸°ëŠ¥ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
        if COREML_AVAILABLE:
            logger.info("âœ… Core ML ë³€í™˜ ë° ê²€ì¦ ê¸°ëŠ¥ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
        else:
            logger.info("â„¹ï¸ Core ML ê¸°ëŠ¥ì€ coremltoolsê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ í…ŒìŠ¤íŠ¸ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    else:
        logger.info("âš ï¸  ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    logger.info("ğŸ”„ ëª¨ë¸ ë³€í™˜ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ğŸ”„")