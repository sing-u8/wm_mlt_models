"""
ë°°í¬ ì¤€ë¹„ ìƒíƒœ ê²€ì¦ê¸°

pickle ë° Core ML ëª¨ë¸ì˜ ë°°í¬ ì¤€ë¹„ ìƒíƒœë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê²€ì¦í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys
import json
import pickle
import time
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
import tempfile
import hashlib

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.ml.model_converter import ModelConverter
from src.audio.feature_extraction import extract_features, FeatureVector
from src.utils.logger import LoggerMixin
from config import DEFAULT_CONFIG


@dataclass
class ModelInfo:
    """ëª¨ë¸ ì •ë³´"""
    file_path: str
    file_size_mb: float
    creation_time: str
    model_type: str  # 'pickle', 'coreml'
    is_valid: bool
    error_message: Optional[str] = None


@dataclass
class DeploymentCheck:
    """ë°°í¬ ê²€ì¦ í•­ëª©"""
    check_name: str
    status: str  # 'PASS', 'FAIL', 'WARNING'
    details: str
    recommendations: List[str] = None


@dataclass
class DeploymentReport:
    """ë°°í¬ ì¤€ë¹„ ë³´ê³ ì„œ"""
    timestamp: str
    pickle_models: List[ModelInfo]
    coreml_models: List[ModelInfo]
    deployment_checks: List[DeploymentCheck]
    overall_status: str
    deployment_score: float  # 0.0-1.0
    critical_issues: List[str]
    recommendations: List[str]
    model_performance_summary: Dict


class DeploymentReadinessChecker(LoggerMixin):
    """ë°°í¬ ì¤€ë¹„ ìƒíƒœ ê²€ì¦ê¸°"""
    
    def __init__(self, models_dir: str = "results/trained_models"):
        self.logger = self.get_logger()
        self.models_dir = Path(models_dir)
        
        # ê²€ì¦ ê¸°ì¤€
        self.max_model_size_mb = 100  # ìµœëŒ€ ëª¨ë¸ í¬ê¸°
        self.min_accuracy_threshold = 0.6  # ìµœì†Œ ì •í™•ë„
        self.required_model_types = ['svm', 'random_forest']  # í•„ìˆ˜ ëª¨ë¸ íƒ€ì…
    
    def run_comprehensive_deployment_check(self) -> DeploymentReport:
        """í¬ê´„ì  ë°°í¬ ì¤€ë¹„ ìƒíƒœ ê²€ì¦"""
        self.logger.info("=== ë°°í¬ ì¤€ë¹„ ìƒíƒœ ê²€ì¦ ì‹œì‘ ===")
        
        # ëª¨ë¸ íŒŒì¼ ê²€ìƒ‰ ë° ì •ë³´ ìˆ˜ì§‘
        pickle_models = self._discover_pickle_models()
        coreml_models = self._discover_coreml_models()
        
        # ë°°í¬ ê²€ì¦ ìˆ˜í–‰
        deployment_checks = []
        
        # 1. ëª¨ë¸ íŒŒì¼ ì¡´ì¬ì„± ê²€ì¦
        deployment_checks.extend(self._check_model_existence(pickle_models, coreml_models))
        
        # 2. ëª¨ë¸ ë¬´ê²°ì„± ê²€ì¦
        deployment_checks.extend(self._check_model_integrity(pickle_models, coreml_models))
        
        # 3. ëª¨ë¸ ì„±ëŠ¥ ê²€ì¦
        deployment_checks.extend(self._check_model_performance(pickle_models))
        
        # 4. ë°°í¬ í˜¸í™˜ì„± ê²€ì¦
        deployment_checks.extend(self._check_deployment_compatibility(pickle_models, coreml_models))
        
        # 5. ë³´ì•ˆ ê²€ì¦
        deployment_checks.extend(self._check_security_requirements(pickle_models, coreml_models))
        
        # 6. ë¬¸ì„œí™” ê²€ì¦
        deployment_checks.extend(self._check_documentation())
        
        # ì „ì²´ í‰ê°€ ë° ë³´ê³ ì„œ ìƒì„±
        overall_status, deployment_score, critical_issues, recommendations = self._evaluate_deployment_readiness(deployment_checks)
        
        # ì„±ëŠ¥ ìš”ì•½
        performance_summary = self._generate_performance_summary(pickle_models)
        
        report = DeploymentReport(
            timestamp=time.strftime('%Y-%m-%d %H:%M:%S'),
            pickle_models=pickle_models,
            coreml_models=coreml_models,
            deployment_checks=deployment_checks,
            overall_status=overall_status,
            deployment_score=deployment_score,
            critical_issues=critical_issues,
            recommendations=recommendations,
            model_performance_summary=performance_summary
        )
        
        self.logger.info(f"ë°°í¬ ì¤€ë¹„ ê²€ì¦ ì™„ë£Œ: {overall_status} (ì ìˆ˜: {deployment_score:.2f})")
        return report
    
    def _discover_pickle_models(self) -> List[ModelInfo]:
        """pickle ëª¨ë¸ ë°œê²¬ ë° ì •ë³´ ìˆ˜ì§‘"""
        pickle_models = []
        
        if not self.models_dir.exists():
            self.logger.warning(f"ëª¨ë¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {self.models_dir}")
            return pickle_models
        
        # .pkl íŒŒì¼ ê²€ìƒ‰
        for pkl_file in self.models_dir.glob("**/*.pkl"):
            try:
                stat = pkl_file.stat()
                
                model_info = ModelInfo(
                    file_path=str(pkl_file),
                    file_size_mb=stat.st_size / (1024 * 1024),
                    creation_time=time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(stat.st_mtime)),
                    model_type='pickle',
                    is_valid=False
                )
                
                # ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸
                try:
                    with open(pkl_file, 'rb') as f:
                        model = pickle.load(f)
                    
                    model_info.is_valid = True
                    self.logger.info(f"Pickle ëª¨ë¸ ë°œê²¬: {pkl_file.name} ({model_info.file_size_mb:.1f}MB)")
                    
                except Exception as e:
                    model_info.error_message = f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}"
                    self.logger.error(f"Pickle ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {pkl_file}: {e}")
                
                pickle_models.append(model_info)
                
            except Exception as e:
                self.logger.error(f"Pickle ëª¨ë¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨ {pkl_file}: {e}")
        
        return pickle_models
    
    def _discover_coreml_models(self) -> List[ModelInfo]:
        """Core ML ëª¨ë¸ ë°œê²¬ ë° ì •ë³´ ìˆ˜ì§‘"""
        coreml_models = []
        
        if not self.models_dir.exists():
            return coreml_models
        
        # .mlmodel íŒŒì¼ ê²€ìƒ‰
        for mlmodel_file in self.models_dir.glob("**/*.mlmodel"):
            try:
                stat = mlmodel_file.stat()
                
                model_info = ModelInfo(
                    file_path=str(mlmodel_file),
                    file_size_mb=stat.st_size / (1024 * 1024),
                    creation_time=time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(stat.st_mtime)),
                    model_type='coreml',
                    is_valid=False
                )
                
                # Core ML ëª¨ë¸ ê²€ì¦
                try:
                    # coremltoolsê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ê²€ì¦
                    import coremltools as ct
                    model = ct.models.MLModel(str(mlmodel_file))
                    
                    model_info.is_valid = True
                    self.logger.info(f"Core ML ëª¨ë¸ ë°œê²¬: {mlmodel_file.name} ({model_info.file_size_mb:.1f}MB)")
                    
                except ImportError:
                    model_info.error_message = "coremltools ì—†ìŒ (ì„ íƒì  ì˜ì¡´ì„±)"
                    model_info.is_valid = True  # ì„ íƒì ì´ë¯€ë¡œ ìœ íš¨í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼
                    self.logger.warning(f"Core ML ê²€ì¦ ê±´ë„ˆëœ€ (coremltools ì—†ìŒ): {mlmodel_file}")
                    
                except Exception as e:
                    model_info.error_message = f"Core ML ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}"
                    self.logger.error(f"Core ML ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {mlmodel_file}: {e}")
                
                coreml_models.append(model_info)
                
            except Exception as e:
                self.logger.error(f"Core ML ëª¨ë¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨ {mlmodel_file}: {e}")
        
        return coreml_models
    
    def _check_model_existence(self, pickle_models: List[ModelInfo], 
                             coreml_models: List[ModelInfo]) -> List[DeploymentCheck]:
        """ëª¨ë¸ íŒŒì¼ ì¡´ì¬ì„± ê²€ì¦"""
        checks = []
        
        # Pickle ëª¨ë¸ ì¡´ì¬ì„±
        if pickle_models:
            valid_pickle = sum(1 for m in pickle_models if m.is_valid)
            check = DeploymentCheck(
                check_name="pickle_models_existence",
                status="PASS" if valid_pickle > 0 else "FAIL",
                details=f"ìœ íš¨í•œ pickle ëª¨ë¸ {valid_pickle}ê°œ ë°œê²¬",
                recommendations=["ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ì €ì¥í•˜ì„¸ìš”"] if valid_pickle == 0 else []
            )
        else:
            check = DeploymentCheck(
                check_name="pickle_models_existence",
                status="FAIL",
                details="pickle ëª¨ë¸ì´ ë°œê²¬ë˜ì§€ ì•ŠìŒ",
                recommendations=["main.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨í•˜ì„¸ìš”"]
            )
        
        checks.append(check)
        
        # Core ML ëª¨ë¸ ì¡´ì¬ì„± (ì„ íƒì )
        if coreml_models:
            valid_coreml = sum(1 for m in coreml_models if m.is_valid)
            check = DeploymentCheck(
                check_name="coreml_models_existence",
                status="PASS" if valid_coreml > 0 else "WARNING",
                details=f"ìœ íš¨í•œ Core ML ëª¨ë¸ {valid_coreml}ê°œ ë°œê²¬",
                recommendations=["Core ML ë³€í™˜ì„ ìˆ˜í–‰í•˜ì„¸ìš”"] if valid_coreml == 0 else []
            )
        else:
            check = DeploymentCheck(
                check_name="coreml_models_existence",
                status="WARNING",
                details="Core ML ëª¨ë¸ì´ ë°œê²¬ë˜ì§€ ì•ŠìŒ (ì„ íƒì )",
                recommendations=["iOS/macOS ë°°í¬ë¥¼ ìœ„í•œ Core ML ë³€í™˜ì„ ê³ ë ¤í•˜ì„¸ìš”"]
            )
        
        checks.append(check)
        
        return checks
    
    def _check_model_integrity(self, pickle_models: List[ModelInfo], 
                             coreml_models: List[ModelInfo]) -> List[DeploymentCheck]:
        """ëª¨ë¸ ë¬´ê²°ì„± ê²€ì¦"""
        checks = []
        
        # Pickle ëª¨ë¸ ë¬´ê²°ì„±
        integrity_issues = []
        
        for model_info in pickle_models:
            if not model_info.is_valid:
                integrity_issues.append(f"{Path(model_info.file_path).name}: {model_info.error_message}")
            
            # ëª¨ë¸ í¬ê¸° ê²€ì¦
            if model_info.file_size_mb > self.max_model_size_mb:
                integrity_issues.append(f"{Path(model_info.file_path).name}: í¬ê¸°ê°€ ë„ˆë¬´ í¼ ({model_info.file_size_mb:.1f}MB > {self.max_model_size_mb}MB)")
        
        if integrity_issues:
            check = DeploymentCheck(
                check_name="model_integrity",
                status="FAIL",
                details=f"ë¬´ê²°ì„± ë¬¸ì œ {len(integrity_issues)}ê°œ ë°œê²¬",
                recommendations=[
                    "ì†ìƒëœ ëª¨ë¸ì„ ë‹¤ì‹œ í›ˆë ¨í•˜ì„¸ìš”",
                    "ëª¨ë¸ í¬ê¸°ë¥¼ ìµœì í™”í•˜ì„¸ìš”"
                ]
            )
        else:
            check = DeploymentCheck(
                check_name="model_integrity",
                status="PASS",
                details="ëª¨ë“  ëª¨ë¸ì´ ë¬´ê²°ì„± ê²€ì‚¬ë¥¼ í†µê³¼í•¨"
            )
        
        checks.append(check)
        
        return checks
    
    def _check_model_performance(self, pickle_models: List[ModelInfo]) -> List[DeploymentCheck]:
        """ëª¨ë¸ ì„±ëŠ¥ ê²€ì¦"""
        checks = []
        
        try:
            # ë©”íƒ€ë°ì´í„° íŒŒì¼ì—ì„œ ì„±ëŠ¥ ì •ë³´ ë¡œë“œ
            performance_data = self._load_model_performance_data()
            
            if not performance_data:
                check = DeploymentCheck(
                    check_name="model_performance",
                    status="WARNING",
                    details="ëª¨ë¸ ì„±ëŠ¥ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ",
                    recommendations=["ëª¨ë¸ì„ ë‹¤ì‹œ í›ˆë ¨í•˜ê³  í‰ê°€í•˜ì„¸ìš”"]
                )
                checks.append(check)
                return checks
            
            # ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦
            performance_issues = []
            best_accuracy = 0.0
            
            for model_name, metrics in performance_data.items():
                accuracy = metrics.get('accuracy', 0.0)
                best_accuracy = max(best_accuracy, accuracy)
                
                if accuracy < self.min_accuracy_threshold:
                    performance_issues.append(f"{model_name}: ì •í™•ë„ ë‚®ìŒ ({accuracy:.3f} < {self.min_accuracy_threshold})")
            
            if performance_issues:
                check = DeploymentCheck(
                    check_name="model_performance",
                    status="FAIL" if best_accuracy < self.min_accuracy_threshold else "WARNING",
                    details=f"ì„±ëŠ¥ ê¸°ì¤€ ë¯¸ë‹¬ ëª¨ë¸ {len(performance_issues)}ê°œ",
                    recommendations=[
                        "ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ìµœì í™”í•˜ì„¸ìš”",
                        "ë” ë§ì€ í›ˆë ¨ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”",
                        "íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ì„ ê°œì„ í•˜ì„¸ìš”"
                    ]
                )
            else:
                check = DeploymentCheck(
                    check_name="model_performance",
                    status="PASS",
                    details=f"ëª¨ë“  ëª¨ë¸ì´ ì„±ëŠ¥ ê¸°ì¤€ì„ ë§Œì¡±í•¨ (ìµœê³  ì •í™•ë„: {best_accuracy:.3f})"
                )
            
            checks.append(check)
            
        except Exception as e:
            check = DeploymentCheck(
                check_name="model_performance",
                status="WARNING",
                details=f"ì„±ëŠ¥ ê²€ì¦ ì‹¤íŒ¨: {str(e)}",
                recommendations=["ëª¨ë¸ í‰ê°€ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”"]
            )
            checks.append(check)
        
        return checks
    
    def _check_deployment_compatibility(self, pickle_models: List[ModelInfo], 
                                      coreml_models: List[ModelInfo]) -> List[DeploymentCheck]:
        """ë°°í¬ í˜¸í™˜ì„± ê²€ì¦"""
        checks = []
        
        # Python í™˜ê²½ í˜¸í™˜ì„± (pickle)
        python_compat_issues = []
        
        for model_info in pickle_models:
            if model_info.is_valid:
                try:
                    # ëª¨ë¸ ë¡œë”© ë° ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
                    with open(model_info.file_path, 'rb') as f:
                        model = pickle.load(f)
                    
                    # ë”ë¯¸ ë°ì´í„°ë¡œ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
                    dummy_features = np.random.rand(1, 30)  # 30ì°¨ì› íŠ¹ì§• ë²¡í„°
                    prediction = model.predict(dummy_features)
                    
                    # ì˜ˆì¸¡ ê²°ê³¼ ê²€ì¦
                    if not isinstance(prediction, np.ndarray) or len(prediction) == 0:
                        python_compat_issues.append(f"{Path(model_info.file_path).name}: ì˜ˆì¸¡ ê²°ê³¼ í˜•ì‹ ì˜¤ë¥˜")
                    
                except Exception as e:
                    python_compat_issues.append(f"{Path(model_info.file_path).name}: ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ({str(e)})")
        
        if python_compat_issues:
            check = DeploymentCheck(
                check_name="python_compatibility",
                status="FAIL",
                details=f"Python í˜¸í™˜ì„± ë¬¸ì œ {len(python_compat_issues)}ê°œ",
                recommendations=[
                    "ëª¨ë¸ì„ í˜„ì¬ Python í™˜ê²½ì—ì„œ ë‹¤ì‹œ í›ˆë ¨í•˜ì„¸ìš”",
                    "scikit-learn ë²„ì „ í˜¸í™˜ì„±ì„ í™•ì¸í•˜ì„¸ìš”"
                ]
            )
        else:
            check = DeploymentCheck(
                check_name="python_compatibility",
                status="PASS",
                details="Python í™˜ê²½ í˜¸í™˜ì„± ê²€ì¦ í†µê³¼"
            )
        
        checks.append(check)
        
        # Core ML í˜¸í™˜ì„± (ì„ íƒì )
        if coreml_models:
            coreml_issues = []
            
            for model_info in coreml_models:
                if model_info.is_valid:
                    try:
                        # Core ML ëª¨ë¸ ê²€ì¦ (coremltoolsê°€ ìˆëŠ” ê²½ìš°)
                        import coremltools as ct
                        model = ct.models.MLModel(model_info.file_path)
                        
                        # ì…ë ¥/ì¶œë ¥ ìŠ¤í™ ê²€ì¦
                        spec = model.get_spec()
                        if not spec.description.input or not spec.description.output:
                            coreml_issues.append(f"{Path(model_info.file_path).name}: ì…ì¶œë ¥ ìŠ¤í™ ëˆ„ë½")
                    
                    except ImportError:
                        # coremltools ì—†ìŒ - ê²½ê³ ë§Œ
                        pass
                    except Exception as e:
                        coreml_issues.append(f"{Path(model_info.file_path).name}: ê²€ì¦ ì‹¤íŒ¨ ({str(e)})")
            
            if coreml_issues:
                check = DeploymentCheck(
                    check_name="coreml_compatibility",
                    status="WARNING",
                    details=f"Core ML í˜¸í™˜ì„± ë¬¸ì œ {len(coreml_issues)}ê°œ",
                    recommendations=["Core ML ëª¨ë¸ì„ ë‹¤ì‹œ ë³€í™˜í•˜ì„¸ìš”"]
                )
            else:
                check = DeploymentCheck(
                    check_name="coreml_compatibility",
                    status="PASS",
                    details="Core ML í˜¸í™˜ì„± ê²€ì¦ í†µê³¼"
                )
            
            checks.append(check)
        
        return checks
    
    def _check_security_requirements(self, pickle_models: List[ModelInfo], 
                                   coreml_models: List[ModelInfo]) -> List[DeploymentCheck]:
        """ë³´ì•ˆ ìš”êµ¬ì‚¬í•­ ê²€ì¦"""
        checks = []
        
        security_issues = []
        
        # ëª¨ë¸ íŒŒì¼ ê¶Œí•œ ê²€ì¦
        for model_info in pickle_models + coreml_models:
            try:
                file_path = Path(model_info.file_path)
                stat = file_path.stat()
                
                # íŒŒì¼ ê¶Œí•œì´ ë„ˆë¬´ ê°œë°©ì ì¸ì§€ í™•ì¸
                if hasattr(stat, 'st_mode'):
                    mode = oct(stat.st_mode)[-3:]
                    if mode in ['777', '666']:  # ëª¨ë“  ì‚¬ìš©ìì—ê²Œ ì“°ê¸° ê¶Œí•œ
                        security_issues.append(f"{file_path.name}: ê¶Œí•œì´ ë„ˆë¬´ ê°œë°©ì ì„ ({mode})")
                
            except Exception as e:
                security_issues.append(f"{Path(model_info.file_path).name}: ê¶Œí•œ í™•ì¸ ì‹¤íŒ¨ ({str(e)})")
        
        # ëª¨ë¸ ì²´í¬ì„¬ ê²€ì¦ (ë¬´ê²°ì„±)
        checksum_file = self.models_dir / "model_checksums.json"
        if checksum_file.exists():
            try:
                with open(checksum_file, 'r') as f:
                    stored_checksums = json.load(f)
                
                for model_info in pickle_models:
                    if model_info.is_valid:
                        file_name = Path(model_info.file_path).name
                        
                        if file_name in stored_checksums:
                            # í˜„ì¬ ì²´í¬ì„¬ ê³„ì‚°
                            with open(model_info.file_path, 'rb') as f:
                                current_checksum = hashlib.sha256(f.read()).hexdigest()
                            
                            if current_checksum != stored_checksums[file_name]:
                                security_issues.append(f"{file_name}: ì²´í¬ì„¬ ë¶ˆì¼ì¹˜ (ë³€ì¡° ê°€ëŠ¥ì„±)")
                        else:
                            security_issues.append(f"{file_name}: ì²´í¬ì„¬ ì—†ìŒ")
            
            except Exception as e:
                security_issues.append(f"ì²´í¬ì„¬ ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
        
        if security_issues:
            check = DeploymentCheck(
                check_name="security_requirements",
                status="WARNING",
                details=f"ë³´ì•ˆ ì´ìŠˆ {len(security_issues)}ê°œ ë°œê²¬",
                recommendations=[
                    "ëª¨ë¸ íŒŒì¼ ê¶Œí•œì„ ì ì ˆíˆ ì„¤ì •í•˜ì„¸ìš”",
                    "ëª¨ë¸ ì²´í¬ì„¬ì„ ìƒì„±í•˜ê³  ê²€ì¦í•˜ì„¸ìš”",
                    "ëª¨ë¸ íŒŒì¼ì„ ì•ˆì „í•œ ìœ„ì¹˜ì— ì €ì¥í•˜ì„¸ìš”"
                ]
            )
        else:
            check = DeploymentCheck(
                check_name="security_requirements",
                status="PASS",
                details="ë³´ì•ˆ ìš”êµ¬ì‚¬í•­ ê²€ì¦ í†µê³¼"
            )
        
        checks.append(check)
        return checks
    
    def _check_documentation(self) -> List[DeploymentCheck]:
        """ë¬¸ì„œí™” ê²€ì¦"""
        checks = []
        
        documentation_issues = []
        
        # í•„ìˆ˜ ë¬¸ì„œ íŒŒì¼ë“¤
        required_docs = [
            ("README.md", "í”„ë¡œì íŠ¸ ì„¤ëª…ì„œ"),
            ("requirements.txt", "ì˜ì¡´ì„± ëª©ë¡"),
            ("docs/API_REFERENCE.md", "API ë¬¸ì„œ"),
            ("docs/MODEL_USAGE_EXAMPLES.md", "ëª¨ë¸ ì‚¬ìš© ì˜ˆì œ")
        ]
        
        for doc_path, description in required_docs:
            if not os.path.exists(doc_path):
                documentation_issues.append(f"{doc_path}: {description} ëˆ„ë½")
            else:
                # íŒŒì¼ í¬ê¸° í™•ì¸ (ìµœì†Œí•œì˜ ë‚´ìš© ìˆëŠ”ì§€)
                try:
                    size = os.path.getsize(doc_path)
                    if size < 100:  # 100ë°”ì´íŠ¸ ë¯¸ë§Œ
                        documentation_issues.append(f"{doc_path}: ë‚´ìš©ì´ ë¶€ì¡±í•¨ ({size} bytes)")
                except Exception:
                    documentation_issues.append(f"{doc_path}: ì ‘ê·¼ ë¶ˆê°€")
        
        # ëª¨ë¸ ë©”íƒ€ë°ì´í„° íŒŒì¼ í™•ì¸
        metadata_file = self.models_dir / "model_metadata.json"
        if not metadata_file.exists():
            documentation_issues.append("model_metadata.json: ëª¨ë¸ ë©”íƒ€ë°ì´í„° ëˆ„ë½")
        
        if documentation_issues:
            check = DeploymentCheck(
                check_name="documentation",
                status="WARNING",
                details=f"ë¬¸ì„œí™” ì´ìŠˆ {len(documentation_issues)}ê°œ",
                recommendations=[
                    "ëˆ„ë½ëœ ë¬¸ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”",
                    "ëª¨ë¸ ì‚¬ìš©ë²•ê³¼ API ë¬¸ì„œë¥¼ ì—…ë°ì´íŠ¸í•˜ì„¸ìš”",
                    "ë°°í¬ ê°€ì´ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”"
                ]
            )
        else:
            check = DeploymentCheck(
                check_name="documentation",
                status="PASS",
                details="ë¬¸ì„œí™” ê²€ì¦ í†µê³¼"
            )
        
        checks.append(check)
        return checks
    
    def _load_model_performance_data(self) -> Dict:
        """ëª¨ë¸ ì„±ëŠ¥ ë°ì´í„° ë¡œë“œ"""
        try:
            # í‰ê°€ ê²°ê³¼ íŒŒì¼ë“¤ ê²€ìƒ‰
            performance_files = [
                self.models_dir / "evaluation_results.json",
                self.models_dir / "model_performance.json",
                "results/evaluation_results.json"
            ]
            
            for perf_file in performance_files:
                if Path(perf_file).exists():
                    with open(perf_file, 'r') as f:
                        data = json.load(f)
                    return data
            
            return {}
            
        except Exception as e:
            self.logger.warning(f"ì„±ëŠ¥ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    def _evaluate_deployment_readiness(self, checks: List[DeploymentCheck]) -> Tuple[str, float, List[str], List[str]]:
        """ë°°í¬ ì¤€ë¹„ ìƒíƒœ ì¢…í•© í‰ê°€"""
        total_checks = len(checks)
        passed_checks = sum(1 for c in checks if c.status == 'PASS')
        failed_checks = sum(1 for c in checks if c.status == 'FAIL')
        warning_checks = sum(1 for c in checks if c.status == 'WARNING')
        
        # ë°°í¬ ì ìˆ˜ ê³„ì‚° (0.0-1.0)
        if total_checks == 0:
            deployment_score = 0.0
        else:
            deployment_score = (passed_checks + warning_checks * 0.5) / total_checks
        
        # ì „ì²´ ìƒíƒœ ê²°ì •
        if failed_checks == 0 and warning_checks <= 2:
            overall_status = "READY"
        elif failed_checks <= 1 and deployment_score >= 0.7:
            overall_status = "PARTIALLY_READY"
        else:
            overall_status = "NOT_READY"
        
        # ì¤‘ìš” ì´ìŠˆ ì¶”ì¶œ
        critical_issues = []
        for check in checks:
            if check.status == 'FAIL':
                critical_issues.append(f"{check.check_name}: {check.details}")
        
        # ê¶Œì¥ì‚¬í•­ ì¢…í•©
        recommendations = []
        for check in checks:
            if check.recommendations:
                recommendations.extend(check.recommendations)
        
        # ì¤‘ë³µ ì œê±°
        recommendations = list(set(recommendations))
        
        return overall_status, deployment_score, critical_issues, recommendations
    
    def _generate_performance_summary(self, pickle_models: List[ModelInfo]) -> Dict:
        """ì„±ëŠ¥ ìš”ì•½ ìƒì„±"""
        try:
            performance_data = self._load_model_performance_data()
            
            if not performance_data:
                return {"status": "ë°ì´í„° ì—†ìŒ"}
            
            summary = {
                "total_models": len(pickle_models),
                "valid_models": sum(1 for m in pickle_models if m.is_valid),
                "best_accuracy": 0.0,
                "average_accuracy": 0.0,
                "model_details": {}
            }
            
            accuracies = []
            
            for model_name, metrics in performance_data.items():
                accuracy = metrics.get('accuracy', 0.0)
                accuracies.append(accuracy)
                
                summary["model_details"][model_name] = {
                    "accuracy": accuracy,
                    "precision": metrics.get('precision', 0.0),
                    "recall": metrics.get('recall', 0.0),
                    "f1_score": metrics.get('f1_score', 0.0)
                }
            
            if accuracies:
                summary["best_accuracy"] = max(accuracies)
                summary["average_accuracy"] = sum(accuracies) / len(accuracies)
            
            return summary
            
        except Exception as e:
            return {"status": f"ì˜¤ë¥˜: {str(e)}"}
    
    def save_deployment_report(self, report: DeploymentReport, output_file: str = None):
        """ë°°í¬ ì¤€ë¹„ ë³´ê³ ì„œ ì €ì¥"""
        if output_file is None:
            output_file = f"deployment_readiness_report_{int(time.time())}.json"
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(asdict(report), f, indent=2, ensure_ascii=False)
            
            # í…ìŠ¤íŠ¸ ìš”ì•½ ë³´ê³ ì„œ
            summary_file = output_file.replace('.json', '_summary.txt')
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write("ìˆ˜ë°• ì†Œë¦¬ ë¶„ë¥˜ ì‹œìŠ¤í…œ ë°°í¬ ì¤€ë¹„ ë³´ê³ ì„œ\n")
                f.write("=" * 50 + "\n\n")
                f.write(f"ê²€ì¦ ì‹œê°„: {report.timestamp}\n")
                f.write(f"ì „ì²´ ìƒíƒœ: {report.overall_status}\n")
                f.write(f"ë°°í¬ ì¤€ë¹„ ì ìˆ˜: {report.deployment_score:.2f}/1.00\n\n")
                
                f.write(f"ëª¨ë¸ í˜„í™©:\n")
                f.write(f"  Pickle ëª¨ë¸: {len(report.pickle_models)}ê°œ\n")
                f.write(f"  Core ML ëª¨ë¸: {len(report.coreml_models)}ê°œ\n\n")
                
                # ì„±ëŠ¥ ìš”ì•½
                perf = report.model_performance_summary
                if perf.get("status") != "ë°ì´í„° ì—†ìŒ":
                    f.write(f"ì„±ëŠ¥ ìš”ì•½:\n")
                    f.write(f"  ìµœê³  ì •í™•ë„: {perf.get('best_accuracy', 0):.3f}\n")
                    f.write(f"  í‰ê·  ì •í™•ë„: {perf.get('average_accuracy', 0):.3f}\n\n")
                
                f.write(f"ê²€ì¦ ê²°ê³¼:\n")
                for check in report.deployment_checks:
                    status_symbol = "âœ…" if check.status == 'PASS' else "âŒ" if check.status == 'FAIL' else "âš ï¸"
                    f.write(f"  {status_symbol} {check.check_name}: {check.details}\n")
                
                if report.critical_issues:
                    f.write(f"\nğŸš¨ ì¤‘ìš” ì´ìŠˆ:\n")
                    for issue in report.critical_issues:
                        f.write(f"  â€¢ {issue}\n")
                
                if report.recommendations:
                    f.write(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:\n")
                    for rec in report.recommendations:
                        f.write(f"  â€¢ {rec}\n")
            
            self.logger.info(f"ë°°í¬ ì¤€ë¹„ ë³´ê³ ì„œ ì €ì¥: {output_file}")
            
        except Exception as e:
            self.logger.error(f"ë³´ê³ ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ë°°í¬ ì¤€ë¹„ ìƒíƒœ ê²€ì¦')
    parser.add_argument('--models-dir', default='results/trained_models', 
                       help='ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    parser.add_argument('--output', help='ë³´ê³ ì„œ ì¶œë ¥ íŒŒì¼ëª…')
    
    args = parser.parse_args()
    
    # ê²€ì¦ê¸° ìƒì„± ë° ì‹¤í–‰
    checker = DeploymentReadinessChecker(args.models_dir)
    report = checker.run_comprehensive_deployment_check()
    
    # ë³´ê³ ì„œ ì €ì¥
    output_file = args.output or f"deployment_report_{int(time.time())}.json"
    checker.save_deployment_report(report, output_file)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\n{'='*60}")
    print(f"ë°°í¬ ì¤€ë¹„ ìƒíƒœ ê²€ì¦ ê²°ê³¼")
    print(f"{'='*60}")
    print(f"ì „ì²´ ìƒíƒœ: {report.overall_status}")
    print(f"ë°°í¬ ì ìˆ˜: {report.deployment_score:.2f}/1.00")
    print(f"Pickle ëª¨ë¸: {len(report.pickle_models)}ê°œ")
    print(f"Core ML ëª¨ë¸: {len(report.coreml_models)}ê°œ")
    
    # ì„±ëŠ¥ ìš”ì•½
    perf = report.model_performance_summary
    if perf.get("status") != "ë°ì´í„° ì—†ìŒ":
        print(f"ìµœê³  ì •í™•ë„: {perf.get('best_accuracy', 0):.3f}")
    
    # ì¤‘ìš” ì´ìŠˆ
    if report.critical_issues:
        print(f"\nğŸš¨ ì¤‘ìš” ì´ìŠˆ:")
        for issue in report.critical_issues[:3]:  # ì²˜ìŒ 3ê°œë§Œ
            print(f"   â€¢ {issue}")
    
    # ìƒíƒœë³„ ê¶Œì¥ì‚¬í•­
    if report.overall_status == "READY":
        print(f"\nâœ… ë°°í¬ ì¤€ë¹„ ì™„ë£Œ!")
    elif report.overall_status == "PARTIALLY_READY":
        print(f"\nâš ï¸ ë¶€ë¶„ì ìœ¼ë¡œ ë°°í¬ ê°€ëŠ¥í•˜ë‚˜ ê°œì„  í•„ìš”")
    else:
        print(f"\nâŒ ë°°í¬ ì¤€ë¹„ ë¯¸ì™„ë£Œ - ë¬¸ì œ í•´ê²° í•„ìš”")
    
    print(f"\nìƒì„¸ ë³´ê³ ì„œ: {output_file}")
    
    # ì¢…ë£Œ ì½”ë“œ
    return 0 if report.overall_status in ["READY", "PARTIALLY_READY"] else 1


if __name__ == "__main__":
    sys.exit(main())