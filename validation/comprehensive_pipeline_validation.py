"""
í¬ê´„ì  íŒŒì´í”„ë¼ì¸ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸

ì „ì²´ ìˆ˜ë°• ì†Œë¦¬ ë¶„ë¥˜ ì‹œìŠ¤í…œì˜ ì™„ì „í•œ ê²€ì¦ì„ ìœ„í•œ ì¢…í•© í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸
"""

import os
import sys
import time
import tempfile
import shutil
import json
import traceback
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, asdict
import numpy as np
import logging

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.data.pipeline import DataPipeline
from src.ml.training import ModelTrainer  
from src.ml.evaluation import ModelEvaluator
from src.ml.model_converter import ModelConverter
from src.optimization.integrated_optimizer import IntegratedOptimizer
from src.utils.logger import LoggerMixin
from src.utils.performance_monitor import PerformanceMonitor
from src.utils.data_integrity import DataIntegrityChecker
from config import DEFAULT_CONFIG


@dataclass
class ValidationResult:
    """ê²€ì¦ ê²°ê³¼"""
    test_name: str
    status: str  # 'PASS', 'FAIL', 'SKIP'  
    execution_time: float
    details: Dict
    errors: List[str] = None
    warnings: List[str] = None


@dataclass
class ComprehensiveValidationReport:
    """ì¢…í•© ê²€ì¦ ë³´ê³ ì„œ"""
    validation_timestamp: str
    system_info: Dict
    total_tests: int
    passed_tests: int
    failed_tests: int
    skipped_tests: int
    total_execution_time: float
    test_results: List[ValidationResult]
    overall_status: str
    recommendations: List[str]


class ComprehensivePipelineValidator(LoggerMixin):
    """í¬ê´„ì  íŒŒì´í”„ë¼ì¸ ê²€ì¦ê¸°"""
    
    def __init__(self, validation_dir: str = None):
        self.logger = self.get_logger()
        
        # ê²€ì¦ ë””ë ‰í† ë¦¬ ì„¤ì •
        if validation_dir is None:
            self.validation_dir = Path("validation_results") / f"validation_{int(time.time())}"
        else:
            self.validation_dir = Path(validation_dir)
        
        self.validation_dir.mkdir(parents=True, exist_ok=True)
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°
        self.performance_monitor = PerformanceMonitor()
        
        # ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ê¸°
        self.integrity_checker = DataIntegrityChecker()
        
        # ê²€ì¦ ê²°ê³¼ ì €ì¥
        self.validation_results: List[ValidationResult] = []
        
        self.logger.info(f"ì¢…í•© ê²€ì¦ ì‹œì‘: {self.validation_dir}")
    
    def run_comprehensive_validation(self, 
                                   test_data_dirs: List[str] = None,
                                   skip_slow_tests: bool = False) -> ComprehensiveValidationReport:
        """
        í¬ê´„ì  íŒŒì´í”„ë¼ì¸ ê²€ì¦ ì‹¤í–‰
        
        Args:
            test_data_dirs: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë””ë ‰í† ë¦¬ ëª©ë¡
            skip_slow_tests: ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ëŠ” í…ŒìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°
            
        Returns:
            ì¢…í•© ê²€ì¦ ë³´ê³ ì„œ
        """
        self.logger.info("=== í¬ê´„ì  íŒŒì´í”„ë¼ì¸ ê²€ì¦ ì‹œì‘ ===")
        start_time = time.time()
        
        # ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘
        system_info = self._collect_system_info()
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
        test_datasets = self._prepare_test_datasets(test_data_dirs)
        
        # ì „ì²´ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self.performance_monitor.start_monitoring()
        
        try:
            # 1. ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê²€ì¦
            self._validate_data_pipeline(test_datasets)
            
            # 2. íŠ¹ì§• ì¶”ì¶œ ê²€ì¦
            self._validate_feature_extraction(test_datasets)
            
            # 3. ë°ì´í„° ì¦ê°• ê²€ì¦
            if not skip_slow_tests:
                self._validate_data_augmentation(test_datasets)
            else:
                self._skip_test("ë°ì´í„° ì¦ê°• ê²€ì¦", "ì‹œê°„ ì ˆì•½ì„ ìœ„í•´ ê±´ë„ˆëœ€")
            
            # 4. ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ ê²€ì¦
            self._validate_ml_pipeline(test_datasets)
            
            # 5. ëª¨ë¸ ë³€í™˜ ê²€ì¦
            self._validate_model_conversion()
            
            # 6. ìµœì í™” ì‹œìŠ¤í…œ ê²€ì¦
            if not skip_slow_tests:
                self._validate_optimization_system(test_datasets)
            else:
                self._skip_test("ìµœì í™” ì‹œìŠ¤í…œ ê²€ì¦", "ì‹œê°„ ì ˆì•½ì„ ìœ„í•´ ê±´ë„ˆëœ€")
            
            # 7. í†µí•© ì‹œìŠ¤í…œ ê²€ì¦
            self._validate_end_to_end_system(test_datasets)
            
            # 8. ì„±ëŠ¥ ë° ë¦¬ì†ŒìŠ¤ ê²€ì¦
            self._validate_performance_requirements()
            
        finally:
            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ
            perf_stats = self.performance_monitor.stop_monitoring()
            
        total_time = time.time() - start_time
        
        # ë³´ê³ ì„œ ìƒì„±
        report = self._generate_final_report(system_info, total_time, perf_stats)
        
        # ë³´ê³ ì„œ ì €ì¥
        self._save_validation_report(report)
        
        self.logger.info(f"=== ì¢…í•© ê²€ì¦ ì™„ë£Œ: {total_time:.2f}ì´ˆ ===")
        return report
    
    def _collect_system_info(self) -> Dict:
        """ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘"""
        try:
            import psutil
            import platform
            
            return {
                'platform': platform.platform(),
                'python_version': platform.python_version(),
                'cpu_count': psutil.cpu_count(),
                'memory_gb': psutil.virtual_memory().total / (1024**3),
                'disk_free_gb': psutil.disk_usage('.').free / (1024**3),
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }
        except Exception as e:
            self.logger.warning(f"ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def _prepare_test_datasets(self, test_data_dirs: List[str]) -> List[Dict]:
        """í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„"""
        test_datasets = []
        
        # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ
        default_paths = [
            "data/raw/train",
            "data/raw/validation", 
            "data/raw/test"
        ]
        
        # ì‹¤ì œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        for data_path in default_paths:
            if os.path.exists(data_path):
                dataset_info = {
                    'name': os.path.basename(data_path),
                    'path': data_path,
                    'type': 'real_data',
                    'file_count': self._count_audio_files(data_path)
                }
                if dataset_info['file_count'] > 0:
                    test_datasets.append(dataset_info)
        
        # ì¶”ê°€ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë””ë ‰í† ë¦¬
        if test_data_dirs:
            for test_dir in test_data_dirs:
                if os.path.exists(test_dir):
                    dataset_info = {
                        'name': f"custom_{os.path.basename(test_dir)}",
                        'path': test_dir,
                        'type': 'custom_data',
                        'file_count': self._count_audio_files(test_dir)
                    }
                    if dataset_info['file_count'] > 0:
                        test_datasets.append(dataset_info)
        
        # ì‹¤ì œ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ê°€ì§œ ë°ì´í„° ìƒì„±
        if not test_datasets:
            self.logger.warning("ì‹¤ì œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŒ, ê°€ì§œ ë°ì´í„° ìƒì„±")
            fake_dataset = self._create_fake_dataset()
            test_datasets.append(fake_dataset)
        
        self.logger.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ: {len(test_datasets)}ê°œ")
        return test_datasets
    
    def _count_audio_files(self, directory: str) -> int:
        """ë””ë ‰í† ë¦¬ì˜ ì˜¤ë””ì˜¤ íŒŒì¼ ìˆ˜ ê³„ì‚°"""
        count = 0
        audio_extensions = ['.wav', '.mp3', '.flac', '.aac']
        
        try:
            for root, dirs, files in os.walk(directory):
                for file in files:
                    if any(file.lower().endswith(ext) for ext in audio_extensions):
                        count += 1
        except Exception as e:
            self.logger.warning(f"íŒŒì¼ ì¹´ìš´íŠ¸ ì‹¤íŒ¨ {directory}: {e}")
        
        return count
    
    def _create_fake_dataset(self) -> Dict:
        """ê°€ì§œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±"""
        fake_data_dir = self.validation_dir / "fake_test_data"
        fake_data_dir.mkdir(exist_ok=True)
        
        # ê° í´ë˜ìŠ¤ë³„ ê°€ì§œ ì˜¤ë””ì˜¤ íŒŒì¼ ìƒì„±
        classes = ['watermelon_A', 'watermelon_B', 'watermelon_C'] 
        file_count = 0
        
        for class_name in classes:
            class_dir = fake_data_dir / class_name
            class_dir.mkdir(exist_ok=True)
            
            # ê° í´ë˜ìŠ¤ë‹¹ 5ê°œ íŒŒì¼ ìƒì„±
            for i in range(5):
                fake_audio_file = class_dir / f"{class_name}_test_{i:02d}.wav"
                self._create_fake_audio_file(fake_audio_file)
                file_count += 1
        
        return {
            'name': 'fake_test_data',
            'path': str(fake_data_dir),
            'type': 'generated_data',
            'file_count': file_count
        }
    
    def _create_fake_audio_file(self, file_path: Path):
        """ê°€ì§œ ì˜¤ë””ì˜¤ íŒŒì¼ ìƒì„±"""
        try:
            import soundfile as sf
            
            # 1ì´ˆ, 22050Hz ì‚¬ì¸íŒŒ ìƒì„±
            duration = 1.0
            sample_rate = 22050
            frequency = 440  # A4 ìŒ
            
            t = np.linspace(0, duration, int(sample_rate * duration))
            audio_data = 0.3 * np.sin(2 * np.pi * frequency * t)
            
            # ë…¸ì´ì¦ˆ ì¶”ê°€
            noise = np.random.normal(0, 0.05, len(audio_data))
            audio_data += noise
            
            sf.write(str(file_path), audio_data, sample_rate)
            
        except Exception as e:
            self.logger.warning(f"ê°€ì§œ ì˜¤ë””ì˜¤ íŒŒì¼ ìƒì„± ì‹¤íŒ¨ {file_path}: {e}")
    
    def _validate_data_pipeline(self, test_datasets: List[Dict]):
        """ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê²€ì¦"""
        test_name = "ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê²€ì¦"
        self.logger.info(f"=== {test_name} ì‹œì‘ ===")
        start_time = time.time()
        
        try:
            results = {}
            errors = []
            
            for dataset in test_datasets[:1]:  # ì²« ë²ˆì§¸ ë°ì´í„°ì…‹ë§Œ í…ŒìŠ¤íŠ¸
                dataset_name = dataset['name']
                self.logger.info(f"ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸: {dataset_name}")
                
                try:
                    # ë°ì´í„° íŒŒì´í”„ë¼ì¸ ìƒì„±
                    pipeline = DataPipeline(DEFAULT_CONFIG)
                    
                    # ë°ì´í„° ë¡œë”© í…ŒìŠ¤íŠ¸
                    if dataset['type'] == 'generated_data':
                        # ê°€ì§œ ë°ì´í„°ìš© ë¡œë”©
                        audio_files = []
                        for root, dirs, files in os.walk(dataset['path']):
                            for file in files:
                                if file.endswith('.wav'):
                                    audio_files.append(os.path.join(root, file))
                        
                        results[f'{dataset_name}_files_loaded'] = len(audio_files)
                    else:
                        # ì‹¤ì œ ë°ì´í„° ë¡œë”©
                        if 'train' in dataset['path']:
                            train_data = pipeline.load_train_data()
                            results[f'{dataset_name}_train_classes'] = len(train_data)
                        elif 'validation' in dataset['path']:
                            val_data = pipeline.load_validation_data()
                            results[f'{dataset_name}_validation_classes'] = len(val_data)
                        elif 'test' in dataset['path']:
                            test_data = pipeline.load_test_data()
                            results[f'{dataset_name}_test_classes'] = len(test_data)
                    
                    # ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬
                    integrity_result = self.integrity_checker.check_data_integrity(
                        dataset['path'], dataset['path'])
                    results[f'{dataset_name}_integrity'] = integrity_result.overall_score
                    
                except Exception as e:
                    error_msg = f"ë°ì´í„°ì…‹ {dataset_name} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
                    errors.append(error_msg)
                    self.logger.error(error_msg)
            
            execution_time = time.time() - start_time
            
            # ê²°ê³¼ í‰ê°€
            if errors:
                status = 'FAIL'
            elif results:
                status = 'PASS'
            else:
                status = 'SKIP'
            
            result = ValidationResult(
                test_name=test_name,
                status=status,
                execution_time=execution_time,
                details=results,
                errors=errors
            )
            
            self.validation_results.append(result)
            self.logger.info(f"{test_name} ì™„ë£Œ: {status} ({execution_time:.2f}ì´ˆ)")
            
        except Exception as e:
            self._handle_test_failure(test_name, e, start_time)
    
    def _validate_feature_extraction(self, test_datasets: List[Dict]):
        """íŠ¹ì§• ì¶”ì¶œ ê²€ì¦"""
        test_name = "íŠ¹ì§• ì¶”ì¶œ ê²€ì¦"
        self.logger.info(f"=== {test_name} ì‹œì‘ ===")
        start_time = time.time()
        
        try:
            from src.audio.feature_extraction import extract_features
            
            results = {}
            errors = []
            
            # ê° ë°ì´í„°ì…‹ì˜ ìƒ˜í”Œ íŒŒì¼ë¡œ íŠ¹ì§• ì¶”ì¶œ í…ŒìŠ¤íŠ¸
            for dataset in test_datasets:
                dataset_name = dataset['name']
                
                try:
                    # ìƒ˜í”Œ íŒŒì¼ ì°¾ê¸°
                    sample_files = []
                    for root, dirs, files in os.walk(dataset['path']):
                        for file in files:
                            if file.endswith('.wav') and len(sample_files) < 3:
                                sample_files.append(os.path.join(root, file))
                    
                    if not sample_files:
                        errors.append(f"ë°ì´í„°ì…‹ {dataset_name}ì—ì„œ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                        continue
                    
                    # íŠ¹ì§• ì¶”ì¶œ í…ŒìŠ¤íŠ¸
                    extracted_features = []
                    for audio_file in sample_files:
                        try:
                            features = extract_features(audio_file, DEFAULT_CONFIG)
                            if features is not None:
                                extracted_features.append(features)
                        except Exception as e:
                            errors.append(f"íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨ {audio_file}: {str(e)}")
                    
                    results[f'{dataset_name}_features_extracted'] = len(extracted_features)
                    
                    if extracted_features:
                        # íŠ¹ì§• ë²¡í„° ê²€ì¦
                        feature_array = extracted_features[0].to_array()
                        results[f'{dataset_name}_feature_dimension'] = len(feature_array)
                        results[f'{dataset_name}_feature_valid'] = not np.any(np.isnan(feature_array))
                
                except Exception as e:
                    error_msg = f"ë°ì´í„°ì…‹ {dataset_name} íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}"
                    errors.append(error_msg)
                    self.logger.error(error_msg)
            
            execution_time = time.time() - start_time
            
            # ê²°ê³¼ í‰ê°€
            status = 'FAIL' if errors else 'PASS'
            
            result = ValidationResult(
                test_name=test_name,
                status=status,
                execution_time=execution_time,
                details=results,
                errors=errors
            )
            
            self.validation_results.append(result)
            self.logger.info(f"{test_name} ì™„ë£Œ: {status} ({execution_time:.2f}ì´ˆ)")
            
        except Exception as e:
            self._handle_test_failure(test_name, e, start_time)
    
    def _validate_data_augmentation(self, test_datasets: List[Dict]):
        """ë°ì´í„° ì¦ê°• ê²€ì¦"""
        test_name = "ë°ì´í„° ì¦ê°• ê²€ì¦"
        self.logger.info(f"=== {test_name} ì‹œì‘ ===")
        start_time = time.time()
        
        try:
            from src.data.augmentation import BatchAugmentor
            
            results = {}
            errors = []
            warnings = []
            
            # ë…¸ì´ì¦ˆ íŒŒì¼ ì°¾ê¸°
            noise_files = []
            noise_paths = ["data/noise", "data/noise/environmental"]
            for noise_path in noise_paths:
                if os.path.exists(noise_path):
                    for root, dirs, files in os.walk(noise_path):
                        for file in files:
                            if file.endswith('.wav') and len(noise_files) < 3:
                                noise_files.append(os.path.join(root, file))
            
            if not noise_files:
                warnings.append("ë…¸ì´ì¦ˆ íŒŒì¼ì´ ì—†ì–´ ì¦ê°• í…ŒìŠ¤íŠ¸ë¥¼ ê°„ë‹¨íˆ ìˆ˜í–‰")
                # ê°€ì§œ ë…¸ì´ì¦ˆ íŒŒì¼ ìƒì„±
                fake_noise_dir = self.validation_dir / "fake_noise"
                fake_noise_dir.mkdir(exist_ok=True)
                fake_noise_file = fake_noise_dir / "test_noise.wav"
                self._create_fake_audio_file(fake_noise_file)
                noise_files = [str(fake_noise_file)]
            
            # ì¦ê°• í…ŒìŠ¤íŠ¸
            for dataset in test_datasets[:1]:  # ì²« ë²ˆì§¸ ë°ì´í„°ì…‹ë§Œ
                dataset_name = dataset['name']
                
                try:
                    # ìƒ˜í”Œ ì˜¤ë””ì˜¤ íŒŒì¼ ì°¾ê¸°
                    sample_files = []
                    for root, dirs, files in os.walk(dataset['path']):
                        for file in files:
                            if file.endswith('.wav') and len(sample_files) < 2:
                                sample_files.append(os.path.join(root, file))
                    
                    if sample_files and noise_files:
                        # ë°°ì¹˜ ì¦ê°•ê¸° ìƒì„±
                        augmentor = BatchAugmentor(DEFAULT_CONFIG)
                        
                        # ì„ì‹œ ì¶œë ¥ ë””ë ‰í† ë¦¬
                        aug_output_dir = self.validation_dir / f"augmented_{dataset_name}"
                        aug_output_dir.mkdir(exist_ok=True)
                        
                        # ì¦ê°• ìˆ˜í–‰
                        augmented_files = []
                        for audio_file in sample_files[:1]:  # í•˜ë‚˜ë§Œ í…ŒìŠ¤íŠ¸
                            for noise_file in noise_files[:1]:  # í•˜ë‚˜ë§Œ í…ŒìŠ¤íŠ¸
                                try:
                                    aug_result = augmentor.augment_noise(
                                        audio_file, noise_file, snr_level=10.0, 
                                        output_dir=str(aug_output_dir))
                                    if aug_result and aug_result.output_file:
                                        augmented_files.append(aug_result.output_file)
                                except Exception as e:
                                    errors.append(f"ì¦ê°• ì‹¤íŒ¨ {audio_file}: {str(e)}")
                        
                        results[f'{dataset_name}_augmented_files'] = len(augmented_files)
                
                except Exception as e:
                    error_msg = f"ë°ì´í„°ì…‹ {dataset_name} ì¦ê°• ì‹¤íŒ¨: {str(e)}"
                    errors.append(error_msg)
                    self.logger.error(error_msg)
            
            execution_time = time.time() - start_time
            
            # ê²°ê³¼ í‰ê°€
            if errors and not warnings:
                status = 'FAIL'
            elif results or warnings:
                status = 'PASS'
            else:
                status = 'SKIP'
            
            result = ValidationResult(
                test_name=test_name,
                status=status,
                execution_time=execution_time,
                details=results,
                errors=errors,
                warnings=warnings
            )
            
            self.validation_results.append(result)
            self.logger.info(f"{test_name} ì™„ë£Œ: {status} ({execution_time:.2f}ì´ˆ)")
            
        except Exception as e:
            self._handle_test_failure(test_name, e, start_time)
    
    def _validate_ml_pipeline(self, test_datasets: List[Dict]):
        """ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ ê²€ì¦"""
        test_name = "ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ ê²€ì¦"
        self.logger.info(f"=== {test_name} ì‹œì‘ ===")
        start_time = time.time()
        
        try:
            from src.audio.feature_extraction import extract_features
            from src.ml.training import ModelTrainer
            from src.ml.evaluation import ModelEvaluator
            
            results = {}
            errors = []
            
            # í›ˆë ¨ ë°ì´í„° ì¤€ë¹„
            X_train = []
            y_train = []
            
            for dataset in test_datasets:
                if dataset['file_count'] < 3:  # ë„ˆë¬´ ì‘ì€ ë°ì´í„°ì…‹ ê±´ë„ˆë›°ê¸°
                    continue
                
                dataset_name = dataset['name']
                class_id = 0  # ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ ëª¨ë“  íŒŒì¼ì„ í´ë˜ìŠ¤ 0ìœ¼ë¡œ
                
                try:
                    # ìƒ˜í”Œ íŒŒì¼ë“¤ë¡œ íŠ¹ì§• ì¶”ì¶œ
                    sample_count = 0
                    for root, dirs, files in os.walk(dataset['path']):
                        for file in files:
                            if file.endswith('.wav') and sample_count < 6:  # í´ë˜ìŠ¤ë‹¹ ìµœëŒ€ 6ê°œ
                                audio_file = os.path.join(root, file)
                                try:
                                    features = extract_features(audio_file, DEFAULT_CONFIG)
                                    if features is not None:
                                        X_train.append(features.to_array())
                                        y_train.append(class_id)
                                        sample_count += 1
                                except Exception as e:
                                    errors.append(f"íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨ {audio_file}: {str(e)}")
                        
                        # ì„œë¸Œë””ë ‰í† ë¦¬ë³„ë¡œ ë‹¤ë¥¸ í´ë˜ìŠ¤ í• ë‹¹
                        class_id += 1
                        if class_id >= 3:  # ìµœëŒ€ 3í´ë˜ìŠ¤
                            break
                
                except Exception as e:
                    error_msg = f"ë°ì´í„°ì…‹ {dataset_name} ML ì¤€ë¹„ ì‹¤íŒ¨: {str(e)}"
                    errors.append(error_msg)
                    self.logger.error(error_msg)
            
            if len(X_train) < 6:  # ìµœì†Œí•œì˜ í›ˆë ¨ ë°ì´í„° í•„ìš”
                errors.append(f"í›ˆë ¨ ë°ì´í„° ë¶€ì¡±: {len(X_train)}ê°œ (ìµœì†Œ 6ê°œ í•„ìš”)")
            else:
                try:
                    # ê°„ë‹¨í•œ í›ˆë ¨ ë° í‰ê°€
                    X_train = np.array(X_train)
                    y_train = np.array(y_train)
                    
                    results['training_samples'] = len(X_train)
                    results['feature_dimension'] = X_train.shape[1]
                    results['unique_classes'] = len(np.unique(y_train))
                    
                    # ê°„ë‹¨í•œ ëª¨ë¸ í›ˆë ¨ (ë¹ ë¥¸ ì„¤ì •)
                    trainer = ModelTrainer(DEFAULT_CONFIG)
                    
                    # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
                    split_idx = int(len(X_train) * 0.7)
                    X_train_split = X_train[:split_idx]
                    y_train_split = y_train[:split_idx]
                    X_test_split = X_train[split_idx:]
                    y_test_split = y_train[split_idx:]
                    
                    if len(X_test_split) > 0:
                        # ë¹ ë¥¸ í›ˆë ¨ (í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ìµœì†Œí™”)
                        training_result = trainer.train_with_cv(
                            X_train_split, y_train_split, cv_folds=2)  # 2-foldë¡œ ë¹ ë¥´ê²Œ
                        
                        results['training_success'] = training_result is not None
                        
                        if training_result:
                            # í‰ê°€
                            evaluator = ModelEvaluator(DEFAULT_CONFIG)
                            best_model = training_result.best_models.get('svm')  # SVMë§Œ í…ŒìŠ¤íŠ¸
                            
                            if best_model:
                                eval_result = evaluator.evaluate_model(
                                    best_model, X_test_split, y_test_split)
                                results['test_accuracy'] = eval_result.accuracy
                                results['evaluation_success'] = True
                            else:
                                errors.append("ìµœê³  ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    else:
                        warnings = ["í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ í‰ê°€ë¥¼ ê±´ë„ˆëœ€"]
                        results['evaluation_skipped'] = True
                
                except Exception as e:
                    error_msg = f"ML íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"
                    errors.append(error_msg)
                    self.logger.error(error_msg)
            
            execution_time = time.time() - start_time
            
            # ê²°ê³¼ í‰ê°€
            status = 'FAIL' if errors else 'PASS'
            
            result = ValidationResult(
                test_name=test_name,
                status=status,
                execution_time=execution_time,
                details=results,
                errors=errors
            )
            
            self.validation_results.append(result)
            self.logger.info(f"{test_name} ì™„ë£Œ: {status} ({execution_time:.2f}ì´ˆ)")
            
        except Exception as e:
            self._handle_test_failure(test_name, e, start_time)
    
    def _validate_model_conversion(self):
        """ëª¨ë¸ ë³€í™˜ ê²€ì¦"""
        test_name = "ëª¨ë¸ ë³€í™˜ ê²€ì¦"
        self.logger.info(f"=== {test_name} ì‹œì‘ ===")
        start_time = time.time()
        
        try:
            from src.ml.model_converter import ModelConverter
            from sklearn.ensemble import RandomForestClassifier
            
            results = {}
            errors = []
            
            try:
                # ê°„ë‹¨í•œ ëª¨ë¸ ìƒì„±
                X_dummy = np.random.rand(20, 30)  # 20 ìƒ˜í”Œ, 30 íŠ¹ì§•
                y_dummy = np.random.randint(0, 3, 20)  # 3 í´ë˜ìŠ¤
                
                model = RandomForestClassifier(n_estimators=10, random_state=42)
                model.fit(X_dummy, y_dummy)
                
                results['dummy_model_created'] = True
                
                # ëª¨ë¸ ë³€í™˜ê¸° ìƒì„±
                converter = ModelConverter(DEFAULT_CONFIG)
                
                # ì„ì‹œ ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
                model_dir = self.validation_dir / "test_models"
                model_dir.mkdir(exist_ok=True)
                
                # Pickle ì €ì¥ í…ŒìŠ¤íŠ¸
                pickle_path = model_dir / "test_model.pkl"
                pickle_result = converter.save_pickle_model(
                    model, str(pickle_path), 
                    feature_config={'n_features': 30},
                    metadata={'test': True})
                
                results['pickle_save_success'] = pickle_result.success
                results['pickle_file_exists'] = os.path.exists(pickle_path)
                
                # Core ML ë³€í™˜ í…ŒìŠ¤íŠ¸
                try:
                    coreml_path = model_dir / "test_model.mlmodel"
                    coreml_result = converter.convert_to_coreml(
                        model, str(coreml_path),
                        input_features=['feature_' + str(i) for i in range(30)],
                        class_labels=['watermelon_A', 'watermelon_B', 'watermelon_C'])
                    
                    results['coreml_conversion_success'] = coreml_result.success
                    results['coreml_file_exists'] = os.path.exists(coreml_path)
                    
                    # ì˜ˆì¸¡ ì¼ì¹˜ì„± ê²€ì¦
                    if coreml_result.success:
                        validation_result = converter.validate_model_conversion(
                            model, str(coreml_path), X_dummy[:5])  # 5ê°œ ìƒ˜í”Œë§Œ
                        results['prediction_consistency'] = validation_result
                
                except Exception as e:
                    # Core ML ë³€í™˜ì€ ì„ íƒì  ê¸°ëŠ¥ì´ë¯€ë¡œ ê²½ê³ ë¡œ ì²˜ë¦¬
                    self.logger.warning(f"Core ML ë³€í™˜ ì‹¤íŒ¨ (ì„ íƒì  ê¸°ëŠ¥): {e}")
                    results['coreml_conversion_skipped'] = True
            
            except Exception as e:
                error_msg = f"ëª¨ë¸ ë³€í™˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}"
                errors.append(error_msg)
                self.logger.error(error_msg)
            
            execution_time = time.time() - start_time
            
            # ê²°ê³¼ í‰ê°€ (Core ML ì‹¤íŒ¨ëŠ” ì „ì²´ ì‹¤íŒ¨ë¡œ ë³´ì§€ ì•ŠìŒ)
            critical_errors = [e for e in errors if 'coreml' not in e.lower()]
            status = 'FAIL' if critical_errors else 'PASS'
            
            result = ValidationResult(
                test_name=test_name,
                status=status,
                execution_time=execution_time,
                details=results,
                errors=errors
            )
            
            self.validation_results.append(result)
            self.logger.info(f"{test_name} ì™„ë£Œ: {status} ({execution_time:.2f}ì´ˆ)")
            
        except Exception as e:
            self._handle_test_failure(test_name, e, start_time)
    
    def _validate_optimization_system(self, test_datasets: List[Dict]):
        """ìµœì í™” ì‹œìŠ¤í…œ ê²€ì¦"""
        test_name = "ìµœì í™” ì‹œìŠ¤í…œ ê²€ì¦"
        self.logger.info(f"=== {test_name} ì‹œì‘ ===")
        start_time = time.time()
        
        try:
            results = {}
            errors = []
            
            try:
                # í†µí•© ìµœì í™”ê¸° ìƒì„±
                optimizer = IntegratedOptimizer()
                
                results['optimizer_created'] = True
                
                # ì‹œìŠ¤í…œ ë²¤ì¹˜ë§ˆí¬
                benchmark_result = optimizer.benchmark_system(num_test_files=5)
                
                if benchmark_result:
                    results['benchmark_completed'] = True
                    results['files_per_second'] = benchmark_result.get('feature_extraction', {}).get('files_per_second', 0)
                    results['memory_efficiency'] = benchmark_result.get('feature_extraction', {}).get('memory_efficiency', 0)
                    
                    # ì„±ëŠ¥ ì„ê³„ê°’ ê²€ì¦
                    files_per_sec = results['files_per_second']
                    if files_per_sec > 0.5:  # ìµœì†Œ ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­
                        results['performance_acceptable'] = True
                    else:
                        errors.append(f"ì„±ëŠ¥ì´ ê¸°ì¤€ ì´í•˜: {files_per_sec:.2f} files/sec < 0.5")
                else:
                    errors.append("ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì‹¤íŒ¨")
                
                # í•˜ë“œì›¨ì–´ ì„¤ì • í…ŒìŠ¤íŠ¸
                hw_config = optimizer.hardware_config.get_current_config()
                results['hardware_detected'] = hw_config is not None
                results['cpu_cores'] = hw_config.get('hardware_profile', {}).get('cpu_cores', 0)
                results['memory_gb'] = hw_config.get('hardware_profile', {}).get('memory_gb', 0)
            
            except Exception as e:
                error_msg = f"ìµœì í™” ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}"
                errors.append(error_msg)
                self.logger.error(error_msg)
            
            execution_time = time.time() - start_time
            
            # ê²°ê³¼ í‰ê°€
            status = 'FAIL' if errors else 'PASS'
            
            result = ValidationResult(
                test_name=test_name,
                status=status,
                execution_time=execution_time,
                details=results,
                errors=errors
            )
            
            self.validation_results.append(result)
            self.logger.info(f"{test_name} ì™„ë£Œ: {status} ({execution_time:.2f}ì´ˆ)")
            
        except Exception as e:
            self._handle_test_failure(test_name, e, start_time)
    
    def _validate_end_to_end_system(self, test_datasets: List[Dict]):
        """í†µí•© ì‹œìŠ¤í…œ ê²€ì¦"""
        test_name = "ì—”ë“œíˆ¬ì—”ë“œ ì‹œìŠ¤í…œ ê²€ì¦"
        self.logger.info(f"=== {test_name} ì‹œì‘ ===")
        start_time = time.time()
        
        try:
            results = {}
            errors = []
            
            try:
                # ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì„í¬íŠ¸
                from main import WatermelonClassificationPipeline
                
                # ì„ì‹œ ì¶œë ¥ ë””ë ‰í† ë¦¬
                e2e_output_dir = self.validation_dir / "e2e_test"
                e2e_output_dir.mkdir(exist_ok=True)
                
                # íŒŒì´í”„ë¼ì¸ ìƒì„± (ê°„ë‹¨í•œ ì„¤ì •)
                pipeline = WatermelonClassificationPipeline()
                
                results['pipeline_created'] = True
                
                # ì‹¤ì œ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
                has_real_data = any(d['type'] in ['real_data', 'custom_data'] 
                                  and d['file_count'] >= 9 for d in test_datasets)  # ìµœì†Œ 3í´ë˜ìŠ¤ * 3íŒŒì¼
                
                if has_real_data:
                    self.logger.info("ì‹¤ì œ ë°ì´í„°ë¡œ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
                    
                    # ê°„ë‹¨í•œ ì„¤ì •ìœ¼ë¡œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
                    pipeline_result = pipeline.run_complete_pipeline(
                        skip_augmentation=True,  # ì‹œê°„ ì ˆì•½
                        cv_folds=2,  # ë¹ ë¥¸ êµì°¨ ê²€ì¦
                        force_retrain=True,
                        dry_run=False
                    )
                    
                    results['pipeline_execution_success'] = pipeline_result is not None
                    
                    if pipeline_result:
                        results['models_trained'] = len(pipeline_result.get('trained_models', {}))
                        results['best_accuracy'] = pipeline_result.get('best_model_accuracy', 0)
                    
                else:
                    self.logger.info("ì‹¤ì œ ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ì „ì²´ íŒŒì´í”„ë¼ì¸ ê±´ë„ˆëœ€")
                    results['pipeline_skipped'] = "ë°ì´í„° ë¶€ì¡±"
                
                # êµ¬ì„±ìš”ì†Œë³„ ê°œë³„ í…ŒìŠ¤íŠ¸
                results['individual_components_tested'] = len([r for r in self.validation_results if r.status == 'PASS'])
            
            except Exception as e:
                error_msg = f"ì—”ë“œíˆ¬ì—”ë“œ ì‹œìŠ¤í…œ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"
                errors.append(error_msg)
                self.logger.error(error_msg)
            
            execution_time = time.time() - start_time
            
            # ê²°ê³¼ í‰ê°€
            status = 'FAIL' if errors else 'PASS'
            
            result = ValidationResult(
                test_name=test_name,
                status=status,
                execution_time=execution_time,
                details=results,
                errors=errors
            )
            
            self.validation_results.append(result)
            self.logger.info(f"{test_name} ì™„ë£Œ: {status} ({execution_time:.2f}ì´ˆ)")
            
        except Exception as e:
            self._handle_test_failure(test_name, e, start_time)
    
    def _validate_performance_requirements(self):
        """ì„±ëŠ¥ ë° ë¦¬ì†ŒìŠ¤ ìš”êµ¬ì‚¬í•­ ê²€ì¦"""
        test_name = "ì„±ëŠ¥ ë° ë¦¬ì†ŒìŠ¤ ìš”êµ¬ì‚¬í•­ ê²€ì¦"
        self.logger.info(f"=== {test_name} ì‹œì‘ ===")
        start_time = time.time()
        
        try:
            results = {}
            errors = []
            warnings = []
            
            try:
                import psutil
                
                # í˜„ì¬ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤
                cpu_count = psutil.cpu_count()
                memory_gb = psutil.virtual_memory().total / (1024**3)
                disk_free_gb = psutil.disk_usage('.').free / (1024**3)
                
                results['system_cpu_cores'] = cpu_count
                results['system_memory_gb'] = memory_gb
                results['system_disk_free_gb'] = disk_free_gb
                
                # ìµœì†Œ ìš”êµ¬ì‚¬í•­ ê²€ì¦
                min_requirements = {
                    'cpu_cores': 2,
                    'memory_gb': 2.0,
                    'disk_free_gb': 1.0
                }
                
                for req_name, min_value in min_requirements.items():
                    current_value = results[f'system_{req_name}']
                    if current_value >= min_value:
                        results[f'{req_name}_sufficient'] = True
                    else:
                        error_msg = f"{req_name} ë¶€ì¡±: {current_value} < {min_value}"
                        errors.append(error_msg)
                        results[f'{req_name}_sufficient'] = False
                
                # ê¶Œì¥ ìš”êµ¬ì‚¬í•­ ê²€ì¦
                recommended_requirements = {
                    'cpu_cores': 4,
                    'memory_gb': 8.0,
                    'disk_free_gb': 5.0
                }
                
                for req_name, rec_value in recommended_requirements.items():
                    current_value = results[f'system_{req_name}']
                    if current_value >= rec_value:
                        results[f'{req_name}_recommended'] = True
                    else:
                        warning_msg = f"{req_name} ê¶Œì¥ì‚¬í•­ ë¯¸ë‹¬: {current_value} < {rec_value}"
                        warnings.append(warning_msg)
                        results[f'{req_name}_recommended'] = False
                
                # ì„±ëŠ¥ ê²€ì¦ (ì´ì „ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë°˜)
                performance_tests = [r for r in self.validation_results 
                                   if 'ìµœì í™”' in r.test_name or 'íŠ¹ì§• ì¶”ì¶œ' in r.test_name]
                
                if performance_tests:
                    avg_execution_time = sum(t.execution_time for t in performance_tests) / len(performance_tests)
                    results['average_test_execution_time'] = avg_execution_time
                    
                    if avg_execution_time < 30:  # 30ì´ˆ ì´ë‚´
                        results['performance_acceptable'] = True
                    else:
                        warnings.append(f"ì„±ëŠ¥ì´ ëŠë¦¼: í‰ê·  {avg_execution_time:.1f}ì´ˆ")
                        results['performance_acceptable'] = False
            
            except Exception as e:
                error_msg = f"ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­ ê²€ì¦ ì‹¤íŒ¨: {str(e)}"
                errors.append(error_msg)
                self.logger.error(error_msg)
            
            execution_time = time.time() - start_time
            
            # ê²°ê³¼ í‰ê°€
            status = 'FAIL' if errors else 'PASS'
            
            result = ValidationResult(
                test_name=test_name,
                status=status,
                execution_time=execution_time,
                details=results,
                errors=errors,
                warnings=warnings
            )
            
            self.validation_results.append(result)
            self.logger.info(f"{test_name} ì™„ë£Œ: {status} ({execution_time:.2f}ì´ˆ)")
            
        except Exception as e:
            self._handle_test_failure(test_name, e, start_time)
    
    def _skip_test(self, test_name: str, reason: str):
        """í…ŒìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°"""
        result = ValidationResult(
            test_name=test_name,
            status='SKIP',
            execution_time=0.0,
            details={'skip_reason': reason}
        )
        self.validation_results.append(result)
        self.logger.info(f"{test_name} ê±´ë„ˆëœ€: {reason}")
    
    def _handle_test_failure(self, test_name: str, exception: Exception, start_time: float):
        """í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ì²˜ë¦¬"""
        execution_time = time.time() - start_time
        error_msg = f"ì˜ˆì™¸ ë°œìƒ: {str(exception)}"
        
        result = ValidationResult(
            test_name=test_name,
            status='FAIL',
            execution_time=execution_time,
            details={'exception': error_msg},
            errors=[error_msg]
        )
        
        self.validation_results.append(result)
        self.logger.error(f"{test_name} ì‹¤íŒ¨: {error_msg}")
        self.logger.debug(traceback.format_exc())
    
    def _generate_final_report(self, system_info: Dict, total_time: float, perf_stats: Dict) -> ComprehensiveValidationReport:
        """ìµœì¢… ê²€ì¦ ë³´ê³ ì„œ ìƒì„±"""
        passed_tests = sum(1 for r in self.validation_results if r.status == 'PASS')
        failed_tests = sum(1 for r in self.validation_results if r.status == 'FAIL')
        skipped_tests = sum(1 for r in self.validation_results if r.status == 'SKIP')
        total_tests = len(self.validation_results)
        
        # ì „ì²´ ìƒíƒœ ê²°ì •
        if failed_tests == 0:
            overall_status = 'PASS'
        elif passed_tests > failed_tests:
            overall_status = 'PARTIAL_PASS'
        else:
            overall_status = 'FAIL'
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = self._generate_recommendations()
        
        return ComprehensiveValidationReport(
            validation_timestamp=time.strftime('%Y-%m-%d %H:%M:%S'),
            system_info=system_info,
            total_tests=total_tests,
            passed_tests=passed_tests,
            failed_tests=failed_tests,
            skipped_tests=skipped_tests,
            total_execution_time=total_time,
            test_results=self.validation_results,
            overall_status=overall_status,
            recommendations=recommendations
        )
    
    def _generate_recommendations(self) -> List[str]:
        """ê²€ì¦ ê²°ê³¼ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        failed_tests = [r for r in self.validation_results if r.status == 'FAIL']
        
        # ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if any('ë°ì´í„° íŒŒì´í”„ë¼ì¸' in t.test_name for t in failed_tests):
            recommendations.append("ë°ì´í„° ë””ë ‰í† ë¦¬ êµ¬ì¡°ì™€ íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”")
        
        if any('íŠ¹ì§• ì¶”ì¶œ' in t.test_name for t in failed_tests):
            recommendations.append("ì˜¤ë””ì˜¤ íŒŒì¼ í˜•ì‹ê³¼ librosa ì„¤ì¹˜ë¥¼ í™•ì¸í•˜ì„¸ìš”")
        
        if any('ë¨¸ì‹ ëŸ¬ë‹' in t.test_name for t in failed_tests):
            recommendations.append("í›ˆë ¨ ë°ì´í„°ì˜ ì–‘ê³¼ í’ˆì§ˆì„ í™•ì¸í•˜ì„¸ìš”")
        
        if any('ì„±ëŠ¥' in t.test_name for t in failed_tests):
            recommendations.append("ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤(CPU, ë©”ëª¨ë¦¬)ë¥¼ ì—…ê·¸ë ˆì´ë“œí•˜ì„¸ìš”")
        
        # ì¼ë°˜ì ì¸ ê¶Œì¥ì‚¬í•­
        if not recommendations:
            recommendations.append("ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ í†µê³¼í–ˆìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
        
        return recommendations
    
    def _save_validation_report(self, report: ComprehensiveValidationReport):
        """ê²€ì¦ ë³´ê³ ì„œ ì €ì¥"""
        try:
            # JSON ë³´ê³ ì„œ
            report_file = self.validation_dir / "validation_report.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(asdict(report), f, indent=2, ensure_ascii=False, default=str)
            
            # í…ìŠ¤íŠ¸ ìš”ì•½ ë³´ê³ ì„œ
            summary_file = self.validation_dir / "validation_summary.txt"
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write(f"ìˆ˜ë°• ì†Œë¦¬ ë¶„ë¥˜ ì‹œìŠ¤í…œ ê²€ì¦ ë³´ê³ ì„œ\n")
                f.write(f"{'='*50}\n\n")
                f.write(f"ê²€ì¦ ì‹œê°„: {report.validation_timestamp}\n")
                f.write(f"ì „ì²´ ìƒíƒœ: {report.overall_status}\n")
                f.write(f"ì´ ì‹¤í–‰ ì‹œê°„: {report.total_execution_time:.2f}ì´ˆ\n\n")
                
                f.write(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼:\n")
                f.write(f"  í†µê³¼: {report.passed_tests}/{report.total_tests}\n")
                f.write(f"  ì‹¤íŒ¨: {report.failed_tests}/{report.total_tests}\n")
                f.write(f"  ê±´ë„ˆëœ€: {report.skipped_tests}/{report.total_tests}\n\n")
                
                f.write(f"ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼:\n")
                for result in report.test_results:
                    f.write(f"  [{result.status}] {result.test_name} ({result.execution_time:.2f}ì´ˆ)\n")
                    if result.errors:
                        for error in result.errors:
                            f.write(f"    ì˜¤ë¥˜: {error}\n")
                
                f.write(f"\nê¶Œì¥ì‚¬í•­:\n")
                for rec in report.recommendations:
                    f.write(f"  - {rec}\n")
            
            self.logger.info(f"ê²€ì¦ ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {self.validation_dir}")
            
        except Exception as e:
            self.logger.error(f"ë³´ê³ ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ìˆ˜ë°• ì†Œë¦¬ ë¶„ë¥˜ ì‹œìŠ¤í…œ í¬ê´„ì  ê²€ì¦')
    parser.add_argument('--test-data', nargs='*', help='ì¶”ê°€ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë””ë ‰í† ë¦¬')
    parser.add_argument('--skip-slow', action='store_true', help='ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ëŠ” í…ŒìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--output-dir', help='ê²€ì¦ ê²°ê³¼ ì¶œë ¥ ë””ë ‰í† ë¦¬')
    
    args = parser.parse_args()
    
    # ê²€ì¦ê¸° ìƒì„±
    validator = ComprehensivePipelineValidator(args.output_dir)
    
    # í¬ê´„ì  ê²€ì¦ ì‹¤í–‰
    report = validator.run_comprehensive_validation(
        test_data_dirs=args.test_data,
        skip_slow_tests=args.skip_slow
    )
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\n{'='*60}")
    print(f"ìˆ˜ë°• ì†Œë¦¬ ë¶„ë¥˜ ì‹œìŠ¤í…œ ê²€ì¦ ê²°ê³¼")
    print(f"{'='*60}")
    print(f"ì „ì²´ ìƒíƒœ: {report.overall_status}")
    print(f"ì´ ì‹¤í–‰ ì‹œê°„: {report.total_execution_time:.2f}ì´ˆ")
    print(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼: {report.passed_tests} í†µê³¼, {report.failed_tests} ì‹¤íŒ¨, {report.skipped_tests} ê±´ë„ˆëœ€")
    
    print(f"\nê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    for result in report.test_results:
        status_symbol = "âœ…" if result.status == 'PASS' else "âŒ" if result.status == 'FAIL' else "â­ï¸"
        print(f"  {status_symbol} {result.test_name} ({result.execution_time:.2f}ì´ˆ)")
    
    if report.recommendations:
        print(f"\nê¶Œì¥ì‚¬í•­:")
        for rec in report.recommendations:
            print(f"  ğŸ’¡ {rec}")
    
    print(f"\nìƒì„¸ ë³´ê³ ì„œ: {validator.validation_dir}")
    
    # ì¢…ë£Œ ì½”ë“œ
    return 0 if report.overall_status in ['PASS', 'PARTIAL_PASS'] else 1


if __name__ == "__main__":
    sys.exit(main())