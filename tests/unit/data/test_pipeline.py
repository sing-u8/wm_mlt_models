#!/usr/bin/env python3
"""
ë°ì´í„° íŒŒì´í”„ë¼ì¸ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ë°ì´í„° ëˆ„ì¶œ ë°©ì§€ ë° ë¬´ê²°ì„± ê²€ì¦ í¬í•¨
"""

import sys
import os
import tempfile
import shutil
import numpy as np
from pathlib import Path
sys.path.append('.')

from src.data.pipeline import DataPipeline, AudioFile, DatasetSplit
from src.utils.logger import setup_logger
from config import DEFAULT_CONFIG

def test_data_loading():
    """ë°ì´í„° ë¡œë”© ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    
    logger = setup_logger("data_loading_test", "INFO")
    logger.info("=== ë°ì´í„° ë¡œë”© í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    pipeline = DataPipeline()
    
    # ê° ë¶„í• ë³„ ë°ì´í„° ë¡œë”© í…ŒìŠ¤íŠ¸
    try:
        # í›ˆë ¨ ë°ì´í„° ë¡œë”©
        train_data = pipeline.load_train_data()
        logger.info(f"í›ˆë ¨ ë°ì´í„° ë¡œë”©: {len(train_data)}ê°œ í´ë˜ìŠ¤")
        
        for class_name, files in train_data.items():
            logger.info(f"  {class_name}: {len(files)}ê°œ íŒŒì¼")
            if files:
                # ì²« ë²ˆì§¸ íŒŒì¼ ì •ë³´ í™•ì¸
                first_file = files[0]
                logger.info(f"    ì˜ˆì‹œ: {os.path.basename(first_file.file_path)} "
                           f"(split: {first_file.split}, augmented: {first_file.is_augmented})")
        
        # ê²€ì¦ ë°ì´í„° ë¡œë”©
        validation_data = pipeline.load_validation_data()
        logger.info(f"ê²€ì¦ ë°ì´í„° ë¡œë”©: {len(validation_data)}ê°œ í´ë˜ìŠ¤")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”©
        test_data = pipeline.load_test_data()
        logger.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”©: {len(test_data)}ê°œ í´ë˜ìŠ¤")
        
        # ì†ŒìŒ íŒŒì¼ ë¡œë”©
        noise_files = pipeline.load_noise_files()
        logger.info(f"ì†ŒìŒ íŒŒì¼ ë¡œë”©: {len(noise_files)}ê°œ íŒŒì¼")
        
        success = True
        
    except Exception as e:
        logger.error(f"ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")
        success = False
    
    logger.info("=== ë°ì´í„° ë¡œë”© í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===\n")
    return success

def test_complete_data_loading():
    """ì „ì²´ ë°ì´í„° ë¡œë”© ë° DatasetSplit í…ŒìŠ¤íŠ¸"""
    
    logger = setup_logger("complete_loading_test", "INFO")
    logger.info("=== ì „ì²´ ë°ì´í„° ë¡œë”© í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    pipeline = DataPipeline()
    
    try:
        # ì „ì²´ ë°ì´í„° ë¡œë”©
        dataset_split = pipeline.load_all_data()
        
        logger.info(f"DatasetSplit ìƒì„± ì™„ë£Œ:")
        logger.info(f"  í›ˆë ¨: {dataset_split.total_train}ê°œ íŒŒì¼")
        logger.info(f"  ê²€ì¦: {dataset_split.total_validation}ê°œ íŒŒì¼")
        logger.info(f"  í…ŒìŠ¤íŠ¸: {dataset_split.total_test}ê°œ íŒŒì¼")
        logger.info(f"  ì†ŒìŒ: {len(dataset_split.noise_files)}ê°œ íŒŒì¼")
        
        # í´ë˜ìŠ¤ë³„ ë¶„í¬ í™•ì¸
        logger.info("í´ë˜ìŠ¤ë³„ ë¶„í¬:")
        for class_name in DEFAULT_CONFIG.class_names:
            train_count = len(dataset_split.train_files.get(class_name, []))
            val_count = len(dataset_split.validation_files.get(class_name, []))
            test_count = len(dataset_split.test_files.get(class_name, []))
            
            logger.info(f"  {class_name}: í›ˆë ¨ {train_count}, ê²€ì¦ {val_count}, í…ŒìŠ¤íŠ¸ {test_count}")
        
        success = True
        
    except Exception as e:
        logger.error(f"ì „ì²´ ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")
        success = False
    
    logger.info("=== ì „ì²´ ë°ì´í„° ë¡œë”© í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===\n")
    return success

def test_data_integrity_validation():
    """ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ í…ŒìŠ¤íŠ¸"""
    
    logger = setup_logger("integrity_test", "INFO")
    logger.info("=== ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    pipeline = DataPipeline()
    
    try:
        # ë°ì´í„° ë¡œë”©
        dataset_split = pipeline.load_all_data()
        
        # ë¬´ê²°ì„± ê²€ì¦
        is_valid = pipeline.validate_data_integrity()
        
        if is_valid:
            logger.info("âœ… ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ í†µê³¼")
        else:
            logger.warning("âš ï¸ ë°ì´í„° ë¬´ê²°ì„± ë¬¸ì œ ë°œê²¬")
        
        success = True
        
    except Exception as e:
        logger.error(f"ë¬´ê²°ì„± ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
        success = False
    
    logger.info("=== ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===\n")
    return success

def test_training_data_augmentation():
    """í›ˆë ¨ ë°ì´í„° ì¦ê°• í…ŒìŠ¤íŠ¸"""
    
    logger = setup_logger("augmentation_test", "INFO")
    logger.info("=== í›ˆë ¨ ë°ì´í„° ì¦ê°• í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    pipeline = DataPipeline()
    
    try:
        # ë°ì´í„° ë¡œë”©
        dataset_split = pipeline.load_all_data()
        
        # ì›ë³¸ í›ˆë ¨ ë°ì´í„° ê°œìˆ˜
        original_train_count = dataset_split.total_train
        logger.info(f"ì›ë³¸ í›ˆë ¨ ë°ì´í„°: {original_train_count}ê°œ íŒŒì¼")
        
        # ì¦ê°• ìˆ˜í–‰
        augmented_train = pipeline.augment_training_data()
        
        # ì¦ê°• ê²°ê³¼ í™•ì¸
        augmented_count = sum(len(files) for files in augmented_train.values())
        logger.info(f"ì¦ê°• í›„ í›ˆë ¨ ë°ì´í„°: {augmented_count}ê°œ íŒŒì¼")
        logger.info(f"ì¦ê°• ë¹„ìœ¨: {augmented_count / original_train_count:.1f}x")
        
        # í´ë˜ìŠ¤ë³„ ì¦ê°• ê²°ê³¼
        for class_name, files in augmented_train.items():
            original_files = [f for f in files if not f.is_augmented]
            augmented_files = [f for f in files if f.is_augmented]
            
            logger.info(f"  {class_name}: ì›ë³¸ {len(original_files)}ê°œ + ì¦ê°• {len(augmented_files)}ê°œ "
                       f"= ì´ {len(files)}ê°œ")
        
        # ì¦ê°• íŒŒì¼ ê²€ì¦: í›ˆë ¨ ì„¸íŠ¸ì—ë§Œ ìˆëŠ”ì§€ í™•ì¸
        validation_augmented = sum(1 for files in dataset_split.validation_files.values() 
                                 for f in files if f.is_augmented)
        test_augmented = sum(1 for files in dataset_split.test_files.values() 
                           for f in files if f.is_augmented)
        
        if validation_augmented > 0 or test_augmented > 0:
            logger.error(f"ë°ì´í„° ëˆ„ì¶œ ë°œê²¬: ê²€ì¦ {validation_augmented}ê°œ, í…ŒìŠ¤íŠ¸ {test_augmented}ê°œ ì¦ê°• íŒŒì¼")
            success = False
        else:
            logger.info("âœ… ì¦ê°•ì´ í›ˆë ¨ ì„¸íŠ¸ì—ë§Œ ì ìš©ë¨ (ë°ì´í„° ëˆ„ì¶œ ì—†ìŒ)")
            success = True
        
    except Exception as e:
        logger.error(f"í›ˆë ¨ ë°ì´í„° ì¦ê°• ì¤‘ ì˜¤ë¥˜: {e}")
        success = False
    
    logger.info("=== í›ˆë ¨ ë°ì´í„° ì¦ê°• í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===\n")
    return success

def test_feature_extraction():
    """íŠ¹ì§• ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
    
    logger = setup_logger("feature_extraction_test", "INFO")
    logger.info("=== íŠ¹ì§• ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    pipeline = DataPipeline()
    
    try:
        # ë°ì´í„° ë¡œë”© ë° ì¦ê°• (ì‘ì€ ìƒ˜í”Œë¡œ)
        dataset_split = pipeline.load_all_data()
        
        # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì‘ì€ ë°ì´í„°ì…‹ ìƒì„±
        test_train_files = {}
        for class_name, files in dataset_split.train_files.items():
            # ê° í´ë˜ìŠ¤ì—ì„œ ì²˜ìŒ 2ê°œ íŒŒì¼ë§Œ ì‚¬ìš©
            test_train_files[class_name] = files[:2]
        
        # ì„ì‹œë¡œ ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ ì„¤ì •
        pipeline._dataset_split.train_files = test_train_files
        pipeline._augmented_train_files = test_train_files
        
        # íŠ¹ì§• ì¶”ì¶œ
        logger.info("íŠ¹ì§• ì¶”ì¶œ ì‹œì‘ (ì‘ì€ ìƒ˜í”Œ)")
        X_train, y_train, X_val, y_val, X_test, y_test = pipeline.extract_all_features()
        
        logger.info(f"íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ:")
        logger.info(f"  í›ˆë ¨: {X_train.shape} features, {y_train.shape} labels")
        logger.info(f"  ê²€ì¦: {X_val.shape} features, {y_val.shape} labels")
        logger.info(f"  í…ŒìŠ¤íŠ¸: {X_test.shape} features, {y_test.shape} labels")
        
        # íŠ¹ì§• ë²¡í„° í¬ê¸° ê²€ì¦
        expected_feature_size = 30  # design.md ëª…ì„¸
        if X_train.shape[1] == expected_feature_size:
            logger.info(f"âœ… íŠ¹ì§• ë²¡í„° í¬ê¸° ì˜¬ë°”ë¦„: {expected_feature_size}ì°¨ì›")
        else:
            logger.error(f"âŒ íŠ¹ì§• ë²¡í„° í¬ê¸° ì˜¤ë¥˜: ì˜ˆìƒ {expected_feature_size}, ì‹¤ì œ {X_train.shape[1]}")
        
        # ë¼ë²¨ ê²€ì¦
        unique_labels = np.unique(np.concatenate([y_train, y_val, y_test]))
        expected_labels = list(range(len(DEFAULT_CONFIG.class_names)))
        
        if set(unique_labels) == set(expected_labels):
            logger.info(f"âœ… ë¼ë²¨ ì˜¬ë°”ë¦„: {unique_labels}")
        else:
            logger.error(f"âŒ ë¼ë²¨ ì˜¤ë¥˜: ì˜ˆìƒ {expected_labels}, ì‹¤ì œ {unique_labels}")
        
        success = X_train.size > 0 and X_val.size > 0 and X_test.size > 0
        
    except Exception as e:
        logger.error(f"íŠ¹ì§• ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        success = False
    
    logger.info("=== íŠ¹ì§• ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===\n")
    return success

def test_complete_pipeline():
    """ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    
    logger = setup_logger("complete_pipeline_test", "INFO")
    logger.info("=== ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    pipeline = DataPipeline()
    
    try:
        # ì¦ê°• ì—†ì´ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
        logger.info("ì¦ê°• ì—†ì´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
        features = pipeline.run_complete_pipeline(skip_augmentation=True)
        
        X_train, y_train, X_val, y_val, X_test, y_test = features
        
        logger.info(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ:")
        logger.info(f"  í›ˆë ¨: {X_train.shape}")
        logger.info(f"  ê²€ì¦: {X_val.shape}")
        logger.info(f"  í…ŒìŠ¤íŠ¸: {X_test.shape}")
        
        # íŒŒì´í”„ë¼ì¸ ìš”ì•½ ì •ë³´
        summary = pipeline.get_pipeline_summary()
        logger.info(f"íŒŒì´í”„ë¼ì¸ ìš”ì•½: {summary}")
        
        success = True
        
    except Exception as e:
        logger.error(f"ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        success = False
    
    logger.info("=== ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===\n")
    return success

def test_data_leakage_prevention():
    """ë°ì´í„° ëˆ„ì¶œ ë°©ì§€ ì „ìš© í…ŒìŠ¤íŠ¸"""
    
    logger = setup_logger("data_leakage_test", "INFO")
    logger.info("=== ë°ì´í„° ëˆ„ì¶œ ë°©ì§€ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    pipeline = DataPipeline()
    
    try:
        # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ì¦ê°• í¬í•¨)
        dataset_split = pipeline.load_all_data()
        augmented_train = pipeline.augment_training_data()
        
        # 1. ì›ë³¸ íŒŒì¼ ì¤‘ë³µ ê²€ì‚¬
        all_original_files = set()
        duplicates_found = []
        
        for split_name, split_files in [
            ("train", dataset_split.train_files),
            ("validation", dataset_split.validation_files),
            ("test", dataset_split.test_files)
        ]:
            for class_name, files in split_files.items():
                for audio_file in files:
                    if not audio_file.is_augmented:
                        file_key = os.path.basename(audio_file.file_path)
                        if file_key in all_original_files:
                            duplicates_found.append(file_key)
                        else:
                            all_original_files.add(file_key)
        
        # 2. ì¦ê°• íŒŒì¼ì´ í›ˆë ¨ ì„¸íŠ¸ì—ë§Œ ìˆëŠ”ì§€ ê²€ì‚¬
        augmented_in_validation = []
        augmented_in_test = []
        
        for class_name, files in dataset_split.validation_files.items():
            for audio_file in files:
                if audio_file.is_augmented:
                    augmented_in_validation.append(audio_file.file_path)
        
        for class_name, files in dataset_split.test_files.items():
            for audio_file in files:
                if audio_file.is_augmented:
                    augmented_in_test.append(audio_file.file_path)
        
        # 3. ê²°ê³¼ ë³´ê³ 
        issues = []
        
        if duplicates_found:
            issues.append(f"ì¤‘ë³µ ì›ë³¸ íŒŒì¼: {duplicates_found}")
        
        if augmented_in_validation:
            issues.append(f"ê²€ì¦ ì„¸íŠ¸ì— ì¦ê°• íŒŒì¼: {len(augmented_in_validation)}ê°œ")
        
        if augmented_in_test:
            issues.append(f"í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— ì¦ê°• íŒŒì¼: {len(augmented_in_test)}ê°œ")
        
        if issues:
            logger.error("ë°ì´í„° ëˆ„ì¶œ ë¬¸ì œ ë°œê²¬:")
            for issue in issues:
                logger.error(f"  - {issue}")
            success = False
        else:
            logger.info("âœ… ë°ì´í„° ëˆ„ì¶œ ë°©ì§€ ê²€ì¦ í†µê³¼")
            logger.info("  - ì›ë³¸ íŒŒì¼ ì¤‘ë³µ ì—†ìŒ")
            logger.info("  - ì¦ê°•ì´ í›ˆë ¨ ì„¸íŠ¸ì—ë§Œ ì ìš©ë¨")
            logger.info("  - ê²€ì¦/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¬´ê²°ì„± ìœ ì§€")
            success = True
        
    except Exception as e:
        logger.error(f"ë°ì´í„° ëˆ„ì¶œ ë°©ì§€ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
        success = False
    
    logger.info("=== ë°ì´í„° ëˆ„ì¶œ ë°©ì§€ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===\n")
    return success

if __name__ == "__main__":
    logger = setup_logger("main_pipeline_test", "INFO")
    logger.info("ğŸ‰ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ëª¨ë“ˆ ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘ ğŸ‰")
    
    test_results = []
    
    # ê°œë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_results.append(("ë°ì´í„° ë¡œë”©", test_data_loading()))
    test_results.append(("ì „ì²´ ë°ì´í„° ë¡œë”©", test_complete_data_loading()))
    test_results.append(("ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦", test_data_integrity_validation()))
    test_results.append(("í›ˆë ¨ ë°ì´í„° ì¦ê°•", test_training_data_augmentation()))
    test_results.append(("íŠ¹ì§• ì¶”ì¶œ", test_feature_extraction()))
    test_results.append(("ì™„ì „í•œ íŒŒì´í”„ë¼ì¸", test_complete_pipeline()))
    test_results.append(("ë°ì´í„° ëˆ„ì¶œ ë°©ì§€", test_data_leakage_prevention()))
    
    # ê²°ê³¼ ìš”ì•½
    logger.info("=" * 60)
    logger.info("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    
    passed = 0
    total = len(test_results)
    
    for test_name, result in test_results:
        status = "âœ… ì„±ê³µ" if result else "âŒ ì‹¤íŒ¨"
        logger.info(f"  {test_name}: {status}")
        if result:
            passed += 1
    
    logger.info("=" * 60)
    success_rate = passed / total * 100
    logger.info(f"ì „ì²´ í…ŒìŠ¤íŠ¸ ê²°ê³¼: {passed}/{total} í†µê³¼ ({success_rate:.1f}%)")
    
    if passed == total:
        logger.info("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        logger.info("âœ… ë°ì´í„° íŒŒì´í”„ë¼ì¸ì´ ì˜¬ë°”ë¥´ê²Œ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤.")
        logger.info("âœ… ë°ì´í„° ëˆ„ì¶œ ë°©ì§€ê°€ ì œëŒ€ë¡œ ì‘ë™í•©ë‹ˆë‹¤.")
    else:
        logger.info("âš ï¸  ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    logger.info("ğŸ‰ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ğŸ‰")