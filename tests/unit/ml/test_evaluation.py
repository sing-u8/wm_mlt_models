#!/usr/bin/env python3
"""
ëª¨ë¸ í‰ê°€ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ModelEvaluator í´ë˜ìŠ¤ ë° í‰ê°€ ë©”íŠ¸ë¦­ ê¸°ëŠ¥ ê²€ì¦
"""

import sys
import os
import tempfile
import shutil
import numpy as np
from pathlib import Path
import matplotlib
matplotlib.use('Agg')  # GUI ì—†ëŠ” í™˜ê²½ì—ì„œ matplotlib ì‚¬ìš©
import matplotlib.pyplot as plt
sys.path.append('.')

from src.ml.evaluation import (
    ModelEvaluator, ClassificationMetrics, ModelComparison, EvaluationReport
)
from src.ml.training import ModelTrainer
from src.utils.logger import setup_logger
from config import DEFAULT_CONFIG

def create_test_models_and_data():
    """í…ŒìŠ¤íŠ¸ìš© ëª¨ë¸ê³¼ ë°ì´í„° ìƒì„±"""
    np.random.seed(42)
    
    # 30ì°¨ì› íŠ¹ì§• ë²¡í„° (design.md ëª…ì„¸)
    n_samples = 150
    n_features = 30
    n_classes = 3
    
    # ê° í´ë˜ìŠ¤ë³„ë¡œ 50ê°œ ìƒ˜í”Œ ìƒì„±
    X = []
    y = []
    
    for class_idx in range(n_classes):
        # í´ë˜ìŠ¤ë³„ë¡œ ë‹¤ë¥¸ íŠ¹ì„±ì„ ê°€ì§„ ë°ì´í„° ìƒì„±
        class_center = np.random.randn(n_features) * 2
        class_samples = np.random.randn(50, n_features) + class_center
        
        X.append(class_samples)
        y.extend([class_idx] * 50)
    
    X = np.vstack(X)
    y = np.array(y)
    
    # ë°ì´í„° ì…”í”Œ
    indices = np.random.permutation(len(X))
    X = X[indices]
    y = y[indices]
    
    # train/test ë¶„í• 
    train_size = int(0.8 * len(X))
    
    X_train = X[:train_size]
    y_train = y[:train_size]
    X_test = X[train_size:]
    y_test = y[train_size:]
    
    # ê°„ë‹¨í•œ ëª¨ë¸ í›ˆë ¨
    trainer = ModelTrainer()
    
    # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì¶•ì†Œ
    trainer.models["svm"].param_grid = {"C": [1], "gamma": ["scale"]}
    trainer.models["random_forest"].param_grid = {"n_estimators": [50], "max_depth": [10]}
    
    # ëª¨ë¸ í›ˆë ¨
    all_results = trainer.train_with_cv(X_train, y_train, cv_folds=3)
    
    # í›ˆë ¨ëœ ëª¨ë¸ ë°˜í™˜
    models = {
        "svm": trainer.trained_models["svm"],
        "random_forest": trainer.trained_models["random_forest"]
    }
    
    return models, X_test, y_test

def test_model_evaluator_initialization():
    """ModelEvaluator ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
    
    logger = setup_logger("evaluator_init_test", "INFO")
    logger.info("=== ModelEvaluator ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    try:
        # ê¸°ë³¸ êµ¬ì„±ìœ¼ë¡œ ì´ˆê¸°í™”
        evaluator = ModelEvaluator()
        
        # êµ¬ì„± í™•ì¸
        if evaluator.config is not None:
            logger.info("âœ… êµ¬ì„± ê°ì²´ ë¡œë“œë¨")
        else:
            logger.error("âŒ êµ¬ì„± ê°ì²´ ë¡œë“œ ì‹¤íŒ¨")
            return False
        
        # ê²°ê³¼ ì €ì¥ì†Œ ì´ˆê¸°í™” í™•ì¸
        if hasattr(evaluator, 'evaluation_results'):
            logger.info("âœ… í‰ê°€ ê²°ê³¼ ì €ì¥ì†Œ ì´ˆê¸°í™”ë¨")
        else:
            logger.error("âŒ í‰ê°€ ê²°ê³¼ ì €ì¥ì†Œ ì´ˆê¸°í™” ì‹¤íŒ¨")
            return False
        
        logger.info("âœ… ModelEvaluator ì´ˆê¸°í™” ì„±ê³µ")
        return True
        
    except Exception as e:
        logger.error(f"ModelEvaluator ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False
    
    finally:
        logger.info("=== ModelEvaluator ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===\n")

def test_single_model_evaluation():
    """ë‹¨ì¼ ëª¨ë¸ í‰ê°€ í…ŒìŠ¤íŠ¸"""
    
    logger = setup_logger("single_eval_test", "INFO")
    logger.info("=== ë‹¨ì¼ ëª¨ë¸ í‰ê°€ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    try:
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë° ëª¨ë¸ ìƒì„±
        models, X_test, y_test = create_test_models_and_data()
        logger.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape} í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ")
        
        # ModelEvaluator ì´ˆê¸°í™”
        evaluator = ModelEvaluator()
        
        # SVM ëª¨ë¸ í‰ê°€
        logger.info("SVM ëª¨ë¸ í‰ê°€ ì‹œì‘...")
        svm_metrics = evaluator.evaluate_model(
            models["svm"], X_test, y_test, "svm"
        )
        
        # ê²°ê³¼ ê²€ì¦
        if not isinstance(svm_metrics, ClassificationMetrics):
            logger.error("âŒ SVM í‰ê°€ ê²°ê³¼ í˜•ì‹ ì˜¤ë¥˜")
            return False
        
        # í•„ìˆ˜ ë©”íŠ¸ë¦­ í™•ì¸
        required_attributes = [
            'accuracy', 'precision_macro', 'recall_macro', 'f1_macro',
            'class_precision', 'class_recall', 'class_f1', 'confusion_matrix'
        ]
        
        for attr in required_attributes:
            if not hasattr(svm_metrics, attr):
                logger.error(f"âŒ SVM í‰ê°€ ê²°ê³¼ì—ì„œ {attr} ëˆ„ë½")
                return False
        
        logger.info(f"âœ… SVM í‰ê°€ ì™„ë£Œ:")
        logger.info(f"  ì •í™•ë„: {svm_metrics.accuracy:.4f}")
        logger.info(f"  F1 (macro): {svm_metrics.f1_macro:.4f}")
        logger.info(f"  í˜¼ë™ í–‰ë ¬ í˜•íƒœ: {svm_metrics.confusion_matrix.shape}")
        
        # ë©”íŠ¸ë¦­ ê°’ ë²”ìœ„ ê²€ì¦
        if not (0 <= svm_metrics.accuracy <= 1):
            logger.error(f"âŒ ì •í™•ë„ ê°’ ë²”ìœ„ ì˜¤ë¥˜: {svm_metrics.accuracy}")
            return False
        
        if not (0 <= svm_metrics.f1_macro <= 1):
            logger.error(f"âŒ F1-score ê°’ ë²”ìœ„ ì˜¤ë¥˜: {svm_metrics.f1_macro}")
            return False
        
        # í´ë˜ìŠ¤ë³„ ë©”íŠ¸ë¦­ í™•ì¸
        expected_classes = set(DEFAULT_CONFIG.class_names)
        actual_classes = set(svm_metrics.class_precision.keys())
        if expected_classes != actual_classes:
            logger.error(f"âŒ í´ë˜ìŠ¤ë³„ ë©”íŠ¸ë¦­ í´ë˜ìŠ¤ ì˜¤ë¥˜: {actual_classes}")
            return False
        
        logger.info("âœ… ë‹¨ì¼ ëª¨ë¸ í‰ê°€ ì„±ê³µ")
        return True
        
    except Exception as e:
        logger.error(f"ë‹¨ì¼ ëª¨ë¸ í‰ê°€ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        logger.info("=== ë‹¨ì¼ ëª¨ë¸ í‰ê°€ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===\n")

def test_model_comparison():
    """ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸"""
    
    logger = setup_logger("model_comparison_test", "INFO")
    logger.info("=== ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    try:
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë° ëª¨ë¸ ìƒì„±
        models, X_test, y_test = create_test_models_and_data()
        
        # ModelEvaluator ì´ˆê¸°í™”
        evaluator = ModelEvaluator()
        
        # ëª¨ë¸ ë¹„êµ
        logger.info("SVM vs Random Forest ë¹„êµ ì‹œì‘...")
        comparison = evaluator.compare_models(
            models["svm"], models["random_forest"],
            X_test, y_test,
            "svm", "random_forest",
            cv_folds=3  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 3-fold
        )
        
        # ê²°ê³¼ ê²€ì¦
        if not isinstance(comparison, ModelComparison):
            logger.error("âŒ ëª¨ë¸ ë¹„êµ ê²°ê³¼ í˜•ì‹ ì˜¤ë¥˜")
            return False
        
        # í•„ìˆ˜ ì†ì„± í™•ì¸
        required_attributes = [
            'model1_name', 'model2_name', 'model1_metrics', 'model2_metrics',
            'accuracy_diff', 'f1_macro_diff', 'better_model'
        ]
        
        for attr in required_attributes:
            if not hasattr(comparison, attr):
                logger.error(f"âŒ ëª¨ë¸ ë¹„êµ ê²°ê³¼ì—ì„œ {attr} ëˆ„ë½")
                return False
        
        logger.info(f"âœ… ëª¨ë¸ ë¹„êµ ì™„ë£Œ:")
        logger.info(f"  ëª¨ë¸ 1: {comparison.model1_name}")
        logger.info(f"  ëª¨ë¸ 2: {comparison.model2_name}")
        logger.info(f"  ì •í™•ë„ ì°¨ì´: {comparison.accuracy_diff:+.4f}")
        logger.info(f"  F1 ì°¨ì´: {comparison.f1_macro_diff:+.4f}")
        logger.info(f"  ë” ì¢‹ì€ ëª¨ë¸: {comparison.better_model}")
        
        # í†µê³„ì  ìœ ì˜ì„± í…ŒìŠ¤íŠ¸ ê²°ê³¼ í™•ì¸
        if 'pvalue' in comparison.accuracy_ttest:
            logger.info(f"  ì •í™•ë„ t-test p-value: {comparison.accuracy_ttest['pvalue']:.4f}")
        
        # ë” ì¢‹ì€ ëª¨ë¸ì´ ì˜¬ë°”ë¥´ê²Œ ì„ ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if comparison.better_model not in [comparison.model1_name, comparison.model2_name]:
            logger.error(f"âŒ ë” ì¢‹ì€ ëª¨ë¸ ì„ ì • ì˜¤ë¥˜: {comparison.better_model}")
            return False
        
        logger.info("âœ… ëª¨ë¸ ë¹„êµ ì„±ê³µ")
        return True
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ ë¹„êµ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally: 
        logger.info("=== ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===\n")

def test_confusion_matrix_visualization():
    """í˜¼ë™ í–‰ë ¬ ì‹œê°í™” í…ŒìŠ¤íŠ¸"""
    
    logger = setup_logger("confusion_viz_test", "INFO")
    logger.info("=== í˜¼ë™ í–‰ë ¬ ì‹œê°í™” í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    try:
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë° ëª¨ë¸ ìƒì„±
        models, X_test, y_test = create_test_models_and_data()
        
        # ModelEvaluator ì´ˆê¸°í™”
        evaluator = ModelEvaluator()
        
        # ëª¨ë¸ í‰ê°€
        svm_metrics = evaluator.evaluate_model(
            models["svm"], X_test, y_test, "svm"
        )
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ì— ì‹œê°í™” ì €ì¥
        with tempfile.TemporaryDirectory() as temp_dir:
            logger.info(f"ì„ì‹œ ë””ë ‰í† ë¦¬ì— ì‹œê°í™” ì €ì¥: {temp_dir}")
            
            # í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
            save_path = os.path.join(temp_dir, "test_confusion_matrix.png")
            saved_path = evaluator.plot_confusion_matrix(
                svm_metrics, "svm", save_path=save_path
            )
            
            # íŒŒì¼ ìƒì„± í™•ì¸
            if not os.path.exists(saved_path):
                logger.error(f"âŒ í˜¼ë™ í–‰ë ¬ ì‹œê°í™” íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•ŠìŒ: {saved_path}")
                return False
            
            file_size = os.path.getsize(saved_path)
            if file_size < 1000:  # 1KB ë¯¸ë§Œì´ë©´ ì˜¤ë¥˜ë¡œ ê°„ì£¼
                logger.error(f"âŒ í˜¼ë™ í–‰ë ¬ ì‹œê°í™” íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìŒ: {file_size} bytes")
                return False
            
            logger.info(f"âœ… í˜¼ë™ í–‰ë ¬ ì‹œê°í™” ìƒì„± ì™„ë£Œ: {file_size} bytes")
        
        logger.info("âœ… í˜¼ë™ í–‰ë ¬ ì‹œê°í™” í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        return True
        
    except Exception as e:
        logger.error(f"í˜¼ë™ í–‰ë ¬ ì‹œê°í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc() 
        return False
    
    finally:
        logger.info("=== í˜¼ë™ í–‰ë ¬ ì‹œê°í™” í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===\n")

def test_model_comparison_visualization():
    """ëª¨ë¸ ë¹„êµ ì‹œê°í™” í…ŒìŠ¤íŠ¸"""
    
    logger = setup_logger("comparison_viz_test", "INFO")
    logger.info("=== ëª¨ë¸ ë¹„êµ ì‹œê°í™” í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    try:
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë° ëª¨ë¸ ìƒì„±
        models, X_test, y_test = create_test_models_and_data()
        
        # ModelEvaluator ì´ˆê¸°í™”
        evaluator = ModelEvaluator()
        
        # ëª¨ë¸ ë¹„êµ
        comparison = evaluator.compare_models(
            models["svm"], models["random_forest"],
            X_test, y_test,
            "svm", "random_forest",
            cv_folds=3
        )
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ì— ì‹œê°í™” ì €ì¥
        with tempfile.TemporaryDirectory() as temp_dir:
            logger.info(f"ì„ì‹œ ë””ë ‰í† ë¦¬ì— ì‹œê°í™” ì €ì¥: {temp_dir}")
            
            # ëª¨ë¸ ë¹„êµ ì‹œê°í™”
            save_path = os.path.join(temp_dir, "test_model_comparison.png")
            saved_path = evaluator.plot_model_comparison(comparison, save_path=save_path)
            
            # íŒŒì¼ ìƒì„± í™•ì¸
            if not os.path.exists(saved_path):
                logger.error(f"âŒ ëª¨ë¸ ë¹„êµ ì‹œê°í™” íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•ŠìŒ: {saved_path}")
                return False
            
            file_size = os.path.getsize(saved_path)
            if file_size < 5000:  # 5KB ë¯¸ë§Œì´ë©´ ì˜¤ë¥˜ë¡œ ê°„ì£¼ (ë” ë³µì¡í•œ ì°¨íŠ¸)
                logger.error(f"âŒ ëª¨ë¸ ë¹„êµ ì‹œê°í™” íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìŒ: {file_size} bytes")
                return False
            
            logger.info(f"âœ… ëª¨ë¸ ë¹„êµ ì‹œê°í™” ìƒì„± ì™„ë£Œ: {file_size} bytes")
        
        logger.info("âœ… ëª¨ë¸ ë¹„êµ ì‹œê°í™” í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        return True
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ ë¹„êµ ì‹œê°í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        logger.info("=== ëª¨ë¸ ë¹„êµ ì‹œê°í™” í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===\n")

def test_evaluation_report_generation():
    """ì¢…í•© í‰ê°€ ë³´ê³ ì„œ ìƒì„± í…ŒìŠ¤íŠ¸"""
    
    logger = setup_logger("report_gen_test", "INFO")
    logger.info("=== ì¢…í•© í‰ê°€ ë³´ê³ ì„œ ìƒì„± í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    try:
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë° ëª¨ë¸ ìƒì„±
        models, X_test, y_test = create_test_models_and_data()
        
        # ModelEvaluator ì´ˆê¸°í™”
        evaluator = ModelEvaluator()
        
        # ë°ì´í„°ì…‹ ì •ë³´
        dataset_info = {
            'test_samples': len(y_test),
            'n_features': X_test.shape[1],
            'n_classes': len(np.unique(y_test)),
            'description': 'Test dataset for evaluation'
        }
        
        # ì¢…í•© í‰ê°€ ë³´ê³ ì„œ ìƒì„±
        logger.info("ì¢…í•© í‰ê°€ ë³´ê³ ì„œ ìƒì„± ì‹œì‘...")
        report = evaluator.generate_evaluation_report(
            models, X_test, y_test, 
            dataset_info=dataset_info,
            save_report=False  # ë©”ëª¨ë¦¬ì—ë§Œ ë³´ê´€
        )
        
        # ë³´ê³ ì„œ ê²€ì¦
        if not isinstance(report, EvaluationReport):
            logger.error("âŒ í‰ê°€ ë³´ê³ ì„œ í˜•ì‹ ì˜¤ë¥˜")
            return False
        
        # í•„ìˆ˜ ì†ì„± í™•ì¸
        required_attributes = [
            'evaluation_id', 'created_at', 'dataset_info', 'model_metrics'
        ]
        
        for attr in required_attributes:
            if not hasattr(report, attr):
                logger.error(f"âŒ í‰ê°€ ë³´ê³ ì„œì—ì„œ {attr} ëˆ„ë½")
                return False
        
        # ëª¨ë¸ ë©”íŠ¸ë¦­ í™•ì¸
        if len(report.model_metrics) != 2:
            logger.error(f"âŒ ëª¨ë¸ ë©”íŠ¸ë¦­ ìˆ˜ ì˜¤ë¥˜: {len(report.model_metrics)}")
            return False
        
        expected_models = {"svm", "random_forest"}
        actual_models = set(report.model_metrics.keys())
        if expected_models != actual_models:
            logger.error(f"âŒ ëª¨ë¸ ë©”íŠ¸ë¦­ ëª¨ë¸ëª… ì˜¤ë¥˜: {actual_models}")
            return False
        
        # ëª¨ë¸ ë¹„êµ í™•ì¸
        if report.model_comparison is None:
            logger.error("âŒ ëª¨ë¸ ë¹„êµ ê²°ê³¼ê°€ ì—†ìŒ")
            return False
        
        logger.info(f"âœ… ì¢…í•© í‰ê°€ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ:")
        logger.info(f"  í‰ê°€ ID: {report.evaluation_id}")
        logger.info(f"  ëª¨ë¸ ìˆ˜: {len(report.model_metrics)}")
        logger.info(f"  ë°ì´í„°ì…‹ ìƒ˜í”Œ ìˆ˜: {report.dataset_info['test_samples']}")
        logger.info(f"  ëª¨ë¸ ë¹„êµ í¬í•¨: {report.model_comparison is not None}")
        
        # ë³´ê³ ì„œ ìš”ì•½ í…ŒìŠ¤íŠ¸
        summary = evaluator.get_evaluation_summary(report)
        
        if 'best_model' not in summary:
            logger.error("âŒ ë³´ê³ ì„œ ìš”ì•½ì— ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì •ë³´ ëˆ„ë½")
            return False
        
        logger.info(f"  ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {summary['best_model']['name']} "
                   f"(F1: {summary['best_model']['f1_macro']:.4f})")
        
        logger.info("âœ… ì¢…í•© í‰ê°€ ë³´ê³ ì„œ ìƒì„± ì„±ê³µ")
        return True
        
    except Exception as e:
        logger.error(f"ì¢…í•© í‰ê°€ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        logger.info("=== ì¢…í•© í‰ê°€ ë³´ê³ ì„œ ìƒì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===\n")

def test_report_save_and_load():
    """ë³´ê³ ì„œ ì €ì¥ ë° ë¡œë”© í…ŒìŠ¤íŠ¸"""
    
    logger = setup_logger("report_save_load_test", "INFO")
    logger.info("=== ë³´ê³ ì„œ ì €ì¥ ë° ë¡œë”© í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    try:
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë° ëª¨ë¸ ìƒì„±
        models, X_test, y_test = create_test_models_and_data()
        
        # ModelEvaluator ì´ˆê¸°í™”
        evaluator = ModelEvaluator()
        
        # ì¢…í•© í‰ê°€ ë³´ê³ ì„œ ìƒì„±
        report = evaluator.generate_evaluation_report(
            models, X_test, y_test, save_report=False
        )
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ì— ë³´ê³ ì„œ ì €ì¥
        with tempfile.TemporaryDirectory() as temp_dir:
            logger.info(f"ì„ì‹œ ë””ë ‰í† ë¦¬ì— ë³´ê³ ì„œ ì €ì¥: {temp_dir}")
            
            # ì„ì‹œ êµ¬ì„±ìœ¼ë¡œ ì €ì¥ ê²½ë¡œ ë³€ê²½
            original_output_dir = evaluator.config.model_output_dir
            evaluator.config.model_output_dir = temp_dir
            
            try:
                # ë³´ê³ ì„œ ì €ì¥
                saved_path = evaluator.save_evaluation_report(report)
                
                # íŒŒì¼ ìƒì„± í™•ì¸
                if not os.path.exists(saved_path):
                    logger.error(f"âŒ ë³´ê³ ì„œ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•ŠìŒ: {saved_path}")
                    return False
                
                file_size = os.path.getsize(saved_path)
                if file_size < 100:  # 100 bytes ë¯¸ë§Œì´ë©´ ì˜¤ë¥˜ë¡œ ê°„ì£¼
                    logger.error(f"âŒ ë³´ê³ ì„œ íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìŒ: {file_size} bytes")
                    return False
                
                logger.info(f"âœ… ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {file_size} bytes")
                
                # ë³´ê³ ì„œ ë¡œë”©
                loaded_report = evaluator.load_evaluation_report(saved_path)
                
                # ë¡œë”©ëœ ë³´ê³ ì„œ ê²€ì¦
                if loaded_report is None:
                    logger.error("âŒ ë³´ê³ ì„œ ë¡œë”© ì‹¤íŒ¨")
                    return False
                
                # ê¸°ë³¸ ì •ë³´ ë¹„êµ
                if loaded_report['evaluation_id'] != report.evaluation_id:
                    logger.error("âŒ ë¡œë”©ëœ ë³´ê³ ì„œ ID ë¶ˆì¼ì¹˜")
                    return False
                
                logger.info(f"âœ… ë³´ê³ ì„œ ë¡œë”© ì™„ë£Œ: {loaded_report['evaluation_id']}")
                
            finally:
                # ì›ë˜ êµ¬ì„± ë³µì›
                evaluator.config.model_output_dir = original_output_dir
        
        logger.info("âœ… ë³´ê³ ì„œ ì €ì¥ ë° ë¡œë”© ì„±ê³µ")
        return True
        
    except Exception as e:
        logger.error(f"ë³´ê³ ì„œ ì €ì¥/ë¡œë”© ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        logger.info("=== ë³´ê³ ì„œ ì €ì¥ ë° ë¡œë”© í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===\n")

def test_evaluation_summary():
    """í‰ê°€ ìš”ì•½ ì •ë³´ í…ŒìŠ¤íŠ¸"""
    
    logger = setup_logger("eval_summary_test", "INFO")
    logger.info("=== í‰ê°€ ìš”ì•½ ì •ë³´ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    try:
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë° ëª¨ë¸ ìƒì„±
        models, X_test, y_test = create_test_models_and_data()
        
        # ModelEvaluator ì´ˆê¸°í™”
        evaluator = ModelEvaluator()
        
        # ì¢…í•© í‰ê°€ ë³´ê³ ì„œ ìƒì„±
        report = evaluator.generate_evaluation_report(
            models, X_test, y_test, save_report=False
        )
        
        # í‰ê°€ ìš”ì•½ ìƒì„±
        summary = evaluator.get_evaluation_summary(report)
        
        # ìš”ì•½ ì •ë³´ ê²€ì¦
        required_keys = ['evaluation_id', 'total_models', 'model_performance', 'best_model']
        for key in required_keys:
            if key not in summary:
                logger.error(f"âŒ í‰ê°€ ìš”ì•½ì—ì„œ {key} ëˆ„ë½")
                return False
        
        # ëª¨ë¸ë³„ ì„±ëŠ¥ ì •ë³´ í™•ì¸
        if len(summary['model_performance']) != 2:
            logger.error(f"âŒ ëª¨ë¸ ì„±ëŠ¥ ì •ë³´ ìˆ˜ ì˜¤ë¥˜: {len(summary['model_performance'])}")
            return False
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì •ë³´ í™•ì¸
        best_model = summary['best_model']
        if 'name' not in best_model or 'f1_macro' not in best_model:
            logger.error("âŒ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì •ë³´ ë¶ˆì™„ì „")
            return False
        
        logger.info(f"âœ… í‰ê°€ ìš”ì•½ ì •ë³´:")
        logger.info(f"  í‰ê°€ ID: {summary['evaluation_id']}")
        logger.info(f"  ì´ ëª¨ë¸ ìˆ˜: {summary['total_models']}")
        logger.info(f"  ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model['name']} (F1: {best_model['f1_macro']:.4f})")
        
        # ëª¨ë¸ ë¹„êµ ì •ë³´ í™•ì¸
        if 'model_comparison' in summary:
            comparison = summary['model_comparison']
            logger.info(f"  ëª¨ë¸ ë¹„êµ ê²°ê³¼: {comparison['better_model']} ìŠ¹ë¦¬")
            logger.info(f"  í†µê³„ì  ìœ ì˜ì„±: {comparison['statistically_significant']}")
        
        logger.info("âœ… í‰ê°€ ìš”ì•½ ì •ë³´ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        return True
        
    except Exception as e:
        logger.error(f"í‰ê°€ ìš”ì•½ ì •ë³´ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        logger.info("=== í‰ê°€ ìš”ì•½ ì •ë³´ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===\n")

if __name__ == "__main__":
    logger = setup_logger("main_evaluation_test", "INFO")
    logger.info("ğŸ“Š ëª¨ë¸ í‰ê°€ ëª¨ë“ˆ ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘ ğŸ“Š")
    
    test_results = []
    
    # ê°œë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_results.append(("ModelEvaluator ì´ˆê¸°í™”", test_model_evaluator_initialization()))
    test_results.append(("ë‹¨ì¼ ëª¨ë¸ í‰ê°€", test_single_model_evaluation()))
    test_results.append(("ëª¨ë¸ ë¹„êµ", test_model_comparison()))
    test_results.append(("í˜¼ë™ í–‰ë ¬ ì‹œê°í™”", test_confusion_matrix_visualization()))
    test_results.append(("ëª¨ë¸ ë¹„êµ ì‹œê°í™”", test_model_comparison_visualization()))
    test_results.append(("ì¢…í•© í‰ê°€ ë³´ê³ ì„œ ìƒì„±", test_evaluation_report_generation()))
    test_results.append(("ë³´ê³ ì„œ ì €ì¥/ë¡œë”©", test_report_save_and_load()))
    test_results.append(("í‰ê°€ ìš”ì•½ ì •ë³´", test_evaluation_summary()))
    
    # ê²°ê³¼ ìš”ì•½
    logger.info("=" * 60)
    logger.info("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    
    passed = 0
    total = len(test_results)
    
    for test_name, result in test_results:
        status = "âœ… ì„±ê³µ" if result else "âŒ ì‹¤íŒ¨"
        logger.info(f"  {test_name}: {status}")
        if result:
            passed += 1
    
    logger.info("=" * 60)
    success_rate = passed / total * 100
    logger.info(f"ì „ì²´ í…ŒìŠ¤íŠ¸ ê²°ê³¼: {passed}/{total} í†µê³¼ ({success_rate:.1f}%)")
    
    if passed == total:
        logger.info("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        logger.info("âœ… ModelEvaluator í´ë˜ìŠ¤ê°€ ì˜¬ë°”ë¥´ê²Œ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤.")
        logger.info("âœ… ëª¨ë¸ í‰ê°€ ë° ë¹„êµ ê¸°ëŠ¥ì´ ì œëŒ€ë¡œ ì‘ë™í•©ë‹ˆë‹¤.")
        logger.info("âœ… ì‹œê°í™” ë° ë³´ê³ ì„œ ìƒì„± ê¸°ëŠ¥ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
    else:
        logger.info("âš ï¸  ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    logger.info("ğŸ“Š ëª¨ë¸ í‰ê°€ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ğŸ“Š")