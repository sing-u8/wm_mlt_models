#!/usr/bin/env python3
"""
ìˆ˜ë°• ì†Œë¦¬ ë¶„ë¥˜ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ì „ì²´ íŒŒì´í”„ë¼ì¸ì˜ ì—”ë“œíˆ¬ì—”ë“œ í†µí•© í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import sys
import os
import tempfile
import shutil
import numpy as np
from pathlib import Path
import json
sys.path.append('.')

from main import WatermelonClassificationPipeline, PipelineCheckpoint
from src.utils.logger import setup_logger
from config import DEFAULT_CONFIG

def create_test_audio_data():
    """í…ŒìŠ¤íŠ¸ìš© ê°€ì§œ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    logger = setup_logger("test_data_creation", "INFO")
    logger.info("í…ŒìŠ¤íŠ¸ìš© ì˜¤ë””ì˜¤ ë°ì´í„° ìƒì„± ì¤‘...")
    
    # ì„ì‹œ ë°ì´í„° ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
    temp_data_dir = Path("test_data_temp")
    
    # ê¸°ì¡´ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬
    if temp_data_dir.exists():
        shutil.rmtree(temp_data_dir)
    
    # ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
    for split in ['train', 'validation', 'test']:
        for class_name in DEFAULT_CONFIG.class_names:
            class_dir = temp_data_dir / 'raw' / split / class_name
            class_dir.mkdir(parents=True, exist_ok=True)
    
    # ì†ŒìŒ ë””ë ‰í† ë¦¬ ìƒì„±
    noise_dir = temp_data_dir / 'noise' / 'environmental' / 'retail' / 'homeplus'
    noise_dir.mkdir(parents=True, exist_ok=True)
    
    # ê°€ì§œ ì˜¤ë””ì˜¤ íŒŒì¼ ìƒì„± (ì‹¤ì œë¡œëŠ” numpy ë°°ì—´ì„ ì €ì¥)
    np.random.seed(42)
    
    # ê° ë¶„í• ë³„ë¡œ íŒŒì¼ ìƒì„±
    file_counts = {'train': 15, 'validation': 6, 'test': 9}  # í´ë˜ìŠ¤ë‹¹ íŒŒì¼ ìˆ˜
    
    for split, count in file_counts.items():
        for class_idx, class_name in enumerate(DEFAULT_CONFIG.class_names):
            for i in range(count):
                # í´ë˜ìŠ¤ë³„ë¡œ ë‹¤ë¥¸ íŠ¹ì„±ì„ ê°€ì§„ ê°€ì§œ ì˜¤ë””ì˜¤ ìƒì„±
                duration = 2.0  # 2ì´ˆ
                sample_rate = DEFAULT_CONFIG.sample_rate
                n_samples = int(duration * sample_rate)
                
                # í´ë˜ìŠ¤ë³„ë¡œ ë‹¤ë¥¸ ì£¼íŒŒìˆ˜ íŠ¹ì„±
                base_freq = 100 + class_idx * 50  # 100Hz, 150Hz, 200Hz
                t = np.linspace(0, duration, n_samples)
                
                # ì‚¬ì¸íŒŒ + ë…¸ì´ì¦ˆë¡œ ê°€ì§œ ì˜¤ë””ì˜¤ ìƒì„±
                audio = (np.sin(2 * np.pi * base_freq * t) * 0.5 + 
                        np.random.randn(n_samples) * 0.1)
                
                # íŒŒì¼ ì €ì¥
                file_path = temp_data_dir / 'raw' / split / class_name / f"{class_name}_{i:03d}.npy"
                np.save(file_path, audio.astype(np.float32))
    
    # ê°€ì§œ ì†ŒìŒ íŒŒì¼ ìƒì„±
    for i in range(5):
        noise_audio = np.random.randn(int(2.0 * DEFAULT_CONFIG.sample_rate)) * 0.2
        noise_path = noise_dir / f"noise_{i:03d}.npy"
        np.save(noise_path, noise_audio.astype(np.float32))
    
    logger.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì™„ë£Œ: {temp_data_dir}")
    return temp_data_dir

def test_pipeline_initialization():
    """íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
    logger = setup_logger("pipeline_init_test", "INFO")
    logger.info("=== íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    try:
        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:
            pipeline = WatermelonClassificationPipeline(
                checkpoint_dir=temp_checkpoint_dir
            )
            
            # êµ¬ì„±ìš”ì†Œ ì´ˆê¸°í™” í™•ì¸
            assert pipeline.data_pipeline is not None, "DataPipeline ì´ˆê¸°í™” ì‹¤íŒ¨"
            assert pipeline.model_trainer is not None, "ModelTrainer ì´ˆê¸°í™” ì‹¤íŒ¨"
            assert pipeline.model_evaluator is not None, "ModelEvaluator ì´ˆê¸°í™” ì‹¤íŒ¨"
            assert pipeline.model_converter is not None, "ModelConverter ì´ˆê¸°í™” ì‹¤íŒ¨"
            assert pipeline.checkpoint_manager is not None, "CheckpointManager ì´ˆê¸°í™” ì‹¤íŒ¨"
            
            # ìƒíƒœ í™•ì¸
            status = pipeline.get_pipeline_status()
            assert status['pipeline_initialized'] == True, "íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ìƒíƒœ ì˜¤ë¥˜"
            assert status['checkpoint_available'] == False, "ë¹ˆ ì²´í¬í¬ì¸íŠ¸ ìƒíƒœ ì˜¤ë¥˜"
            
            components = status['components_ready']
            for component, ready in components.items():
                assert ready == True, f"{component} êµ¬ì„±ìš”ì†Œ ì¤€ë¹„ ì‹¤íŒ¨"
        
        logger.info("âœ… íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        logger.info("=== íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===\n")

def test_checkpoint_functionality():
    """ì²´í¬í¬ì¸íŠ¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    logger = setup_logger("checkpoint_test", "INFO")
    logger.info("=== ì²´í¬í¬ì¸íŠ¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    try:
        with tempfile.TemporaryDirectory() as temp_dir:
            checkpoint_manager = PipelineCheckpoint(temp_dir)
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í…ŒìŠ¤íŠ¸
            test_data = {
                'step': 'data_loading',
                'samples': 100,
                'features': 30
            }
            
            checkpoint_manager.save_checkpoint('data_loading', test_data, 10.5)
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
            loaded_checkpoint = checkpoint_manager.load_checkpoint()
            
            assert loaded_checkpoint is not None, "ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨"
            assert loaded_checkpoint['step'] == 'data_loading', "ì²´í¬í¬ì¸íŠ¸ ë‹¨ê³„ ë¶ˆì¼ì¹˜"
            assert loaded_checkpoint['data']['samples'] == 100, "ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ë¶ˆì¼ì¹˜"
            assert loaded_checkpoint['execution_time'] == 10.5, "ì²´í¬í¬ì¸íŠ¸ ì‹¤í–‰ì‹œê°„ ë¶ˆì¼ì¹˜"
            
            # ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ í…ŒìŠ¤íŠ¸
            checkpoint_manager.clear_checkpoint()
            cleared_checkpoint = checkpoint_manager.load_checkpoint()
            assert cleared_checkpoint is None, "ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ ì‹¤íŒ¨"
        
        logger.info("âœ… ì²´í¬í¬ì¸íŠ¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        logger.info("=== ì²´í¬í¬ì¸íŠ¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===\n")

def test_data_integrity_checks():
    """ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ í…ŒìŠ¤íŠ¸"""
    logger = setup_logger("data_integrity_test", "INFO")
    logger.info("=== ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    try:
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        test_data_dir = create_test_audio_data()
        
        # ì›ë³¸ êµ¬ì„±ì„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ìˆ˜ì •
        test_config = DEFAULT_CONFIG
        test_config.data_base_dir = str(test_data_dir)
        
        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:
            pipeline = WatermelonClassificationPipeline(
                config=test_config,
                checkpoint_dir=temp_checkpoint_dir
            )
            
            # 1ë‹¨ê³„: ë°ì´í„° ë¡œë”© í…ŒìŠ¤íŠ¸
            logger.info("ë°ì´í„° ë¡œë”© ë¬´ê²°ì„± ê²€ì‚¬ ì¤‘...")
            X_train, y_train, X_val, y_val, X_test, y_test = pipeline.step_1_load_data(
                skip_augmentation=True  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì¦ê°• ê±´ë„ˆë›°ê¸°
            )
            
            # ë°ì´í„° í˜•íƒœ ê²€ì¦
            assert len(X_train) > 0, "í›ˆë ¨ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŒ"
            assert len(X_val) > 0, "ê²€ì¦ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŒ"
            assert len(X_test) > 0, "í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŒ"
            
            assert len(X_train) == len(y_train), "í›ˆë ¨ ë°ì´í„° íŠ¹ì§•-ë ˆì´ë¸” ê¸¸ì´ ë¶ˆì¼ì¹˜"
            assert len(X_val) == len(y_val), "ê²€ì¦ ë°ì´í„° íŠ¹ì§•-ë ˆì´ë¸” ê¸¸ì´ ë¶ˆì¼ì¹˜"
            assert len(X_test) == len(y_test), "í…ŒìŠ¤íŠ¸ ë°ì´í„° íŠ¹ì§•-ë ˆì´ë¸” ê¸¸ì´ ë¶ˆì¼ì¹˜"
            
            # íŠ¹ì§• ë²¡í„° ì°¨ì› ê²€ì¦ (design.md ëª…ì„¸: 30ì°¨ì›)
            if len(X_train) > 0:
                assert X_train.shape[1] == 30, f"íŠ¹ì§• ë²¡í„° ì°¨ì› ì˜¤ë¥˜: {X_train.shape[1]} != 30"
            if len(X_val) > 0:
                assert X_val.shape[1] == 30, f"ê²€ì¦ íŠ¹ì§• ë²¡í„° ì°¨ì› ì˜¤ë¥˜: {X_val.shape[1]} != 30"
            if len(X_test) > 0:
                assert X_test.shape[1] == 30, f"í…ŒìŠ¤íŠ¸ íŠ¹ì§• ë²¡í„° ì°¨ì› ì˜¤ë¥˜: {X_test.shape[1]} != 30"
            
            # ë ˆì´ë¸” ë²”ìœ„ ê²€ì¦
            all_labels = np.concatenate([y_train, y_val, y_test])
            unique_labels = np.unique(all_labels)
            assert len(unique_labels) <= len(DEFAULT_CONFIG.class_names), "ì˜ˆìƒë³´ë‹¤ ë§ì€ í´ë˜ìŠ¤"
            assert np.all(unique_labels >= 0), "ìŒìˆ˜ ë ˆì´ë¸” ë°œê²¬"
            assert np.all(unique_labels < len(DEFAULT_CONFIG.class_names)), "ë²”ìœ„ ì´ˆê³¼ ë ˆì´ë¸” ë°œê²¬"
            
            # NaN/Infinity ê²€ì¦
            for dataset_name, (X, y) in [('train', (X_train, y_train)), 
                                        ('val', (X_val, y_val)), 
                                        ('test', (X_test, y_test))]:
                if len(X) > 0:
                    assert not np.any(np.isnan(X)), f"{dataset_name} íŠ¹ì§•ì— NaN ë°œê²¬"
                    assert not np.any(np.isinf(X)), f"{dataset_name} íŠ¹ì§•ì— Infinity ë°œê²¬"
                    assert not np.any(np.isnan(y)), f"{dataset_name} ë ˆì´ë¸”ì— NaN ë°œê²¬"
            
            logger.info("âœ… ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ í†µê³¼")
            logger.info(f"  í›ˆë ¨: {len(X_train)}ìƒ˜í”Œ, ê²€ì¦: {len(X_val)}ìƒ˜í”Œ, í…ŒìŠ¤íŠ¸: {len(X_test)}ìƒ˜í”Œ")
            logger.info(f"  íŠ¹ì§• ì°¨ì›: {X_train.shape[1] if len(X_train) > 0 else 'N/A'}")
            logger.info(f"  í´ë˜ìŠ¤ ìˆ˜: {len(unique_labels)}")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬
        if test_data_dir.exists():
            shutil.rmtree(test_data_dir)
        
        logger.info("âœ… ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬
        if 'test_data_dir' in locals() and test_data_dir.exists():
            shutil.rmtree(test_data_dir)
        
        return False
    
    finally:
        logger.info("=== ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===\n")

def test_pipeline_steps_integration():
    """íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ í†µí•© í…ŒìŠ¤íŠ¸"""
    logger = setup_logger("pipeline_steps_test", "INFO")
    logger.info("=== íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    try:
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        test_data_dir = create_test_audio_data()
        
        # ì›ë³¸ êµ¬ì„±ì„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ìˆ˜ì •
        test_config = DEFAULT_CONFIG
        test_config.data_base_dir = str(test_data_dir)
        
        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:
            pipeline = WatermelonClassificationPipeline(
                config=test_config,
                checkpoint_dir=temp_checkpoint_dir
            )
            
            # 1ë‹¨ê³„: ë°ì´í„° ë¡œë”©
            logger.info("1ë‹¨ê³„: ë°ì´í„° ë¡œë”© í…ŒìŠ¤íŠ¸ ì¤‘...")
            X_train, y_train, X_val, y_val, X_test, y_test = pipeline.step_1_load_data(
                skip_augmentation=True
            )
            
            assert len(X_train) > 0, "1ë‹¨ê³„: í›ˆë ¨ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨"
            step1_checkpoint = pipeline.checkpoint_manager.load_checkpoint()
            assert step1_checkpoint['step'] == 'data_loading', "1ë‹¨ê³„: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨"
            
            # 2ë‹¨ê³„: ëª¨ë¸ í›ˆë ¨ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ íŒŒë¼ë¯¸í„° ì¶•ì†Œ)
            logger.info("2ë‹¨ê³„: ëª¨ë¸ í›ˆë ¨ í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            # í›ˆë ¨ íŒŒë¼ë¯¸í„° ì¶•ì†Œ
            original_svm_params = pipeline.model_trainer.models["svm"].param_grid
            original_rf_params = pipeline.model_trainer.models["random_forest"].param_grid
            
            pipeline.model_trainer.models["svm"].param_grid = {"C": [1], "gamma": ["scale"]}
            pipeline.model_trainer.models["random_forest"].param_grid = {"n_estimators": [10], "max_depth": [3]}
            
            training_results = pipeline.step_2_train_models(X_train, y_train, cv_folds=3)
            
            assert len(training_results) > 0, "2ë‹¨ê³„: ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨"
            assert 'svm' in training_results, "2ë‹¨ê³„: SVM í›ˆë ¨ ì‹¤íŒ¨"
            assert 'random_forest' in training_results, "2ë‹¨ê³„: Random Forest í›ˆë ¨ ì‹¤íŒ¨"
            
            step2_checkpoint = pipeline.checkpoint_manager.load_checkpoint()
            assert step2_checkpoint['step'] == 'model_training', "2ë‹¨ê³„: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨"
            
            # 3ë‹¨ê³„: ëª¨ë¸ í‰ê°€
            logger.info("3ë‹¨ê³„: ëª¨ë¸ í‰ê°€ í…ŒìŠ¤íŠ¸ ì¤‘...")
            evaluation_results = pipeline.step_3_evaluate_models(X_test, y_test)
            
            assert len(evaluation_results) > 0, "3ë‹¨ê³„: ëª¨ë¸ í‰ê°€ ì‹¤íŒ¨"
            
            for model_name in ['svm', 'random_forest']:
                assert model_name in evaluation_results, f"3ë‹¨ê³„: {model_name} í‰ê°€ ê²°ê³¼ ëˆ„ë½"
                
                eval_result = evaluation_results[model_name]
                assert hasattr(eval_result, 'accuracy'), f"3ë‹¨ê³„: {model_name} ë©”íŠ¸ë¦­ ëˆ„ë½"
                
                metrics = eval_result
                assert hasattr(metrics, 'accuracy'), f"3ë‹¨ê³„: {model_name} ì •í™•ë„ ëˆ„ë½"
                assert 0 <= metrics.accuracy <= 1, f"3ë‹¨ê³„: {model_name} ì •í™•ë„ ë²”ìœ„ ì˜¤ë¥˜"
            
            step3_checkpoint = pipeline.checkpoint_manager.load_checkpoint()
            assert step3_checkpoint['step'] == 'model_evaluation', "3ë‹¨ê³„: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨"
            
            # 4ë‹¨ê³„: ëª¨ë¸ ì €ì¥ ë° ë³€í™˜ (Core ML ê±´ë„ˆë›°ê¸°)
            logger.info("4ë‹¨ê³„: ëª¨ë¸ ì €ì¥ í…ŒìŠ¤íŠ¸ ì¤‘...")
            conversion_results = pipeline.step_4_save_and_convert_models(convert_to_coreml=False)
            
            assert 'conversion_summary' in conversion_results, "4ë‹¨ê³„: ë³€í™˜ ìš”ì•½ ëˆ„ë½"
            
            step4_checkpoint = pipeline.checkpoint_manager.load_checkpoint()
            assert step4_checkpoint['step'] == 'model_conversion', "4ë‹¨ê³„: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨"
            
            # íŒŒë¼ë¯¸í„° ë³µì›
            pipeline.model_trainer.models["svm"].param_grid = original_svm_params
            pipeline.model_trainer.models["random_forest"].param_grid = original_rf_params
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬
        if test_data_dir.exists():
            shutil.rmtree(test_data_dir)
        
        logger.info("âœ… íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ í†µí•© í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬
        if 'test_data_dir' in locals() and test_data_dir.exists():
            shutil.rmtree(test_data_dir)
        
        return False
    
    finally:
        logger.info("=== íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===\n")

def test_complete_pipeline_run():
    """ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í…ŒìŠ¤íŠ¸"""
    logger = setup_logger("complete_pipeline_test", "INFO")
    logger.info("=== ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    try:
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        test_data_dir = create_test_audio_data()
        
        # ì›ë³¸ êµ¬ì„±ì„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ìˆ˜ì •
        test_config = DEFAULT_CONFIG
        test_config.data_base_dir = str(test_data_dir)
        
        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:
            pipeline = WatermelonClassificationPipeline(
                config=test_config,
                checkpoint_dir=temp_checkpoint_dir
            )
            
            # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ íŒŒë¼ë¯¸í„° ì¡°ì •
            pipeline.model_trainer.models["svm"].param_grid = {"C": [1], "gamma": ["scale"]}
            pipeline.model_trainer.models["random_forest"].param_grid = {"n_estimators": [10], "max_depth": [3]}
            
            # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            logger.info("ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘...")
            results = pipeline.run_complete_pipeline(
                skip_augmentation=True,
                cv_folds=3,
                convert_to_coreml=False,  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ Core ML ê±´ë„ˆë›°ê¸°
                resume_from_checkpoint=False
            )
            
            # ê²°ê³¼ ê²€ì¦
            assert 'execution_summary' in results, "ì‹¤í–‰ ìš”ì•½ ëˆ„ë½"
            assert results['execution_summary']['success'] == True, "íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨"
            
            execution_summary = results['execution_summary']
            assert 'total_time' in execution_summary, "ì´ ì‹¤í–‰ ì‹œê°„ ëˆ„ë½"
            assert execution_summary['total_time'] > 0, "ì‹¤í–‰ ì‹œê°„ ì˜¤ë¥˜"
            
            assert 'step_times' in execution_summary, "ë‹¨ê³„ë³„ ì‹œê°„ ëˆ„ë½"
            step_times = execution_summary['step_times']
            
            expected_steps = ['data_loading', 'model_training', 'model_evaluation', 'model_conversion']
            for step in expected_steps:
                assert step in step_times, f"{step} ë‹¨ê³„ ì‹œê°„ ëˆ„ë½"
                assert step_times[step] > 0, f"{step} ë‹¨ê³„ ì‹œê°„ ì˜¤ë¥˜"
            
            # ê° ë‹¨ê³„ ê²°ê³¼ ê²€ì¦
            assert 'data_loading' in results, "ë°ì´í„° ë¡œë”© ê²°ê³¼ ëˆ„ë½"
            assert 'model_training' in results, "ëª¨ë¸ í›ˆë ¨ ê²°ê³¼ ëˆ„ë½"
            assert 'model_evaluation' in results, "ëª¨ë¸ í‰ê°€ ê²°ê³¼ ëˆ„ë½"
            assert 'model_conversion' in results, "ëª¨ë¸ ë³€í™˜ ê²°ê³¼ ëˆ„ë½"
            
            # ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬ í™•ì¸
            final_checkpoint = pipeline.checkpoint_manager.load_checkpoint()
            assert final_checkpoint is None, "íŒŒì´í”„ë¼ì¸ ì™„ë£Œ í›„ ì²´í¬í¬ì¸íŠ¸ ë¯¸ì •ë¦¬"
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬
        if test_data_dir.exists():
            shutil.rmtree(test_data_dir)
        
        logger.info("âœ… ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        logger.info(f"  ì´ ì‹¤í–‰ ì‹œê°„: {execution_summary['total_time']:.2f}ì´ˆ")
        logger.info(f"  ë‹¨ê³„ë³„ ì‹œê°„: {step_times}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬
        if 'test_data_dir' in locals() and test_data_dir.exists():
            shutil.rmtree(test_data_dir)
        
        return False
    
    finally:
        logger.info("=== ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===\n")

def test_resume_functionality():
    """ì¬ì‹œì‘ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    logger = setup_logger("resume_test", "INFO")
    logger.info("=== ì¬ì‹œì‘ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    try:
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        test_data_dir = create_test_audio_data()
        
        # ì›ë³¸ êµ¬ì„±ì„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ìˆ˜ì •
        test_config = DEFAULT_CONFIG
        test_config.data_base_dir = str(test_data_dir)
        
        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:
            # ì²« ë²ˆì§¸ íŒŒì´í”„ë¼ì¸: 1ë‹¨ê³„ë§Œ ì‹¤í–‰
            pipeline1 = WatermelonClassificationPipeline(
                config=test_config,
                checkpoint_dir=temp_checkpoint_dir
            )
            
            logger.info("1ë‹¨ê³„ë§Œ ì‹¤í–‰í•˜ì—¬ ì²´í¬í¬ì¸íŠ¸ ìƒì„±...")
            X_train, y_train, X_val, y_val, X_test, y_test = pipeline1.step_1_load_data(
                skip_augmentation=True
            )
            
            # ì²´í¬í¬ì¸íŠ¸ ì¡´ì¬ í™•ì¸
            checkpoint = pipeline1.checkpoint_manager.load_checkpoint()
            assert checkpoint is not None, "ì²´í¬í¬ì¸íŠ¸ ìƒì„± ì‹¤íŒ¨"
            assert checkpoint['step'] == 'data_loading', "ì²´í¬í¬ì¸íŠ¸ ë‹¨ê³„ ì˜¤ë¥˜"
            
            # ë‘ ë²ˆì§¸ íŒŒì´í”„ë¼ì¸: ì¬ì‹œì‘ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
            pipeline2 = WatermelonClassificationPipeline(
                config=test_config,
                checkpoint_dir=temp_checkpoint_dir
            )
            
            # ìƒíƒœ í™•ì¸
            status = pipeline2.get_pipeline_status()
            assert status['checkpoint_available'] == True, "ì²´í¬í¬ì¸íŠ¸ ì¸ì‹ ì‹¤íŒ¨"
            assert status['last_completed_step'] == 'data_loading', "ë§ˆì§€ë§‰ ë‹¨ê³„ ì¸ì‹ ì‹¤íŒ¨"
            
            logger.info("ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ì‹œì‘ ê¸°ëŠ¥ í™•ì¸ ì™„ë£Œ")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬
        if test_data_dir.exists():
            shutil.rmtree(test_data_dir)
        
        logger.info("âœ… ì¬ì‹œì‘ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ì¬ì‹œì‘ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬
        if 'test_data_dir' in locals() and test_data_dir.exists():
            shutil.rmtree(test_data_dir)
        
        return False
    
    finally:
        logger.info("=== ì¬ì‹œì‘ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===\n")

def test_performance_benchmarking():
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ í…ŒìŠ¤íŠ¸"""
    logger = setup_logger("performance_test", "INFO")
    logger.info("=== ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    try:
        import psutil
        import time
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        test_data_dir = create_test_audio_data()
        
        # ì›ë³¸ êµ¬ì„±ì„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ìˆ˜ì •
        test_config = DEFAULT_CONFIG
        test_config.data_base_dir = str(test_data_dir)
        
        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:
            pipeline = WatermelonClassificationPipeline(
                config=test_config,
                checkpoint_dir=temp_checkpoint_dir
            )
            
            # ì„±ëŠ¥ ì¸¡ì • ì‹œì‘
            process = psutil.Process()
            start_memory = process.memory_info().rss / 1024 / 1024  # MB
            start_time = time.time()
            
            logger.info(f"ì‹œì‘ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {start_memory:.2f} MB")
            
            # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ íŒŒë¼ë¯¸í„° ì¡°ì •
            pipeline.model_trainer.models["svm"].param_grid = {"C": [1], "gamma": ["scale"]}
            pipeline.model_trainer.models["random_forest"].param_grid = {"n_estimators": [5], "max_depth": [3]}
            
            # 1ë‹¨ê³„ ì„±ëŠ¥ ì¸¡ì •
            step_start_time = time.time()
            X_train, y_train, X_val, y_val, X_test, y_test = pipeline.step_1_load_data(
                skip_augmentation=True
            )
            step1_time = time.time() - step_start_time
            step1_memory = process.memory_info().rss / 1024 / 1024
            
            logger.info(f"1ë‹¨ê³„ ì™„ë£Œ: {step1_time:.2f}ì´ˆ, ë©”ëª¨ë¦¬: {step1_memory:.2f} MB")
            
            # 2ë‹¨ê³„ ì„±ëŠ¥ ì¸¡ì •
            step_start_time = time.time()
            training_results = pipeline.step_2_train_models(X_train, y_train, cv_folds=3)
            step2_time = time.time() - step_start_time
            step2_memory = process.memory_info().rss / 1024 / 1024
            
            logger.info(f"2ë‹¨ê³„ ì™„ë£Œ: {step2_time:.2f}ì´ˆ, ë©”ëª¨ë¦¬: {step2_memory:.2f} MB")
            
            # ì „ì²´ ì„±ëŠ¥ ì¸¡ì •
            end_time = time.time()
            end_memory = process.memory_info().rss / 1024 / 1024
            total_time = end_time - start_time
            memory_increase = end_memory - start_memory
            
            # ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦
            assert total_time < 300, f"ì „ì²´ ì‹¤í–‰ ì‹œê°„ ì´ˆê³¼: {total_time:.2f}ì´ˆ > 300ì´ˆ"  # 5ë¶„ ì´ë‚´
            assert memory_increase < 500, f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì´ˆê³¼: {memory_increase:.2f}MB > 500MB"  # 500MB ì´ë‚´
            
            # ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±
            performance_report = {
                'total_execution_time': total_time,
                'memory_usage': {
                    'start': start_memory,
                    'end': end_memory,
                    'increase': memory_increase,
                    'peak': step2_memory
                },
                'step_performance': {
                    'data_loading': {'time': step1_time, 'memory': step1_memory},
                    'model_training': {'time': step2_time, 'memory': step2_memory}
                },
                'performance_criteria': {
                    'time_limit_met': total_time < 300,
                    'memory_limit_met': memory_increase < 500
                }
            }
            
            logger.info("ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ê²°ê³¼:")
            logger.info(f"  ì´ ì‹¤í–‰ ì‹œê°„: {total_time:.2f}ì´ˆ")
            logger.info(f"  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€: {memory_increase:.2f} MB")
            logger.info(f"  ìµœëŒ€ ë©”ëª¨ë¦¬: {step2_memory:.2f} MB")
            logger.info(f"  ì‹œê°„ ê¸°ì¤€ í†µê³¼: {performance_report['performance_criteria']['time_limit_met']}")
            logger.info(f"  ë©”ëª¨ë¦¬ ê¸°ì¤€ í†µê³¼: {performance_report['performance_criteria']['memory_limit_met']}")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬
        if test_data_dir.exists():
            shutil.rmtree(test_data_dir)
        
        logger.info("âœ… ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        return True
        
    except ImportError:
        logger.warning("âš ï¸ psutil ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        logger.warning("ì„¤ì¹˜í•˜ë ¤ë©´: pip install psutil")
        return True  # psutilì´ ì—†ëŠ” ê²ƒì€ ì˜¤ë¥˜ê°€ ì•„ë‹˜
        
    except Exception as e:
        logger.error(f"âŒ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬
        if 'test_data_dir' in locals() and test_data_dir.exists():
            shutil.rmtree(test_data_dir)
        
        return False
    
    finally:
        logger.info("=== ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===\n")

if __name__ == "__main__":
    logger = setup_logger("main_integration_test", "INFO")
    logger.info("ğŸ”„ ìˆ˜ë°• ì†Œë¦¬ ë¶„ë¥˜ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘ ğŸ”„")
    
    test_results = []
    
    # ê°œë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_results.append(("íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”", test_pipeline_initialization()))
    test_results.append(("ì²´í¬í¬ì¸íŠ¸ ê¸°ëŠ¥", test_checkpoint_functionality()))
    test_results.append(("ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬", test_data_integrity_checks()))
    test_results.append(("íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ í†µí•©", test_pipeline_steps_integration()))
    test_results.append(("ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰", test_complete_pipeline_run()))
    test_results.append(("ì¬ì‹œì‘ ê¸°ëŠ¥", test_resume_functionality()))
    test_results.append(("ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹", test_performance_benchmarking()))
    
    # ê²°ê³¼ ìš”ì•½
    logger.info("=" * 60)
    logger.info("í†µí•© í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    
    passed = 0
    total = len(test_results)
    
    for test_name, result in test_results:
        status = "âœ… ì„±ê³µ" if result else "âŒ ì‹¤íŒ¨"
        logger.info(f"  {test_name}: {status}")
        if result:
            passed += 1
    
    logger.info("=" * 60)
    success_rate = passed / total * 100
    logger.info(f"ì „ì²´ í†µí•© í…ŒìŠ¤íŠ¸ ê²°ê³¼: {passed}/{total} í†µê³¼ ({success_rate:.1f}%)")
    
    if passed == total:
        logger.info("ğŸ‰ ëª¨ë“  í†µí•© í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        logger.info("âœ… ìˆ˜ë°• ì†Œë¦¬ ë¶„ë¥˜ íŒŒì´í”„ë¼ì¸ì´ ì˜¬ë°”ë¥´ê²Œ í†µí•©ë˜ì—ˆìŠµë‹ˆë‹¤.")
        logger.info("âœ… ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ê°€ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
        logger.info("âœ… ì²´í¬í¬ì¸íŠ¸ ë° ì¬ì‹œì‘ ê¸°ëŠ¥ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
        logger.info("âœ… ì„±ëŠ¥ ê¸°ì¤€ì„ ì¶©ì¡±í•©ë‹ˆë‹¤.")
    else:
        logger.info("âš ï¸  ì¼ë¶€ í†µí•© í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    logger.info("ğŸ”„ ìˆ˜ë°• ì†Œë¦¬ ë¶„ë¥˜ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ ğŸ”„")